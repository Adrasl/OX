% Definimos el estilo del documento
\documentclass[12pt,A4,spanish]{book}

% Definimos los márgenes de la página
\usepackage[lmargin=2.5cm,rmargin=1.5cm,tmargin=3.0cm,bmargin=3.0cm]{geometry}

% Utilizamos el paquete para utilizar español
\usepackage[spanish]{babel}

% Utilizamos el paquete para gestionar acentos
\usepackage[latin1]{inputenc}

%Utilizamos el paquete para inluir imágenes jpg
\usepackage{graphicx}

%Para evitar problemas de imágenes flotantes
\usepackage{float}

%Utilizamos el paquete para incorporar graficos postcript
%\usepackage[dvips,final]{epsfig}

%utilizamos paquete para introducir cuadros de código
%\usepackage[utf8]{inputenc}
%\usepackage[T1]{fontenc}
\usepackage{listings}
%\usepackage{times}
\usepackage{color}

\definecolor{gray97}{gray}{.97}
\definecolor{gray75}{gray}{.75}
\definecolor{gray45}{gray}{.45}
 

\lstset{ frame=Ltb,
     framerule=0pt,
     aboveskip=0.5cm,
     framextopmargin=3pt,
     framexbottommargin=3pt,
     framexleftmargin=0.4cm,
     framesep=0pt,
     rulesep=.4pt,
     backgroundcolor=\color{gray97},
     rulesepcolor=\color{black},
     %
     stringstyle=\ttfamily,
     showstringspaces = false,
     basicstyle=\small\ttfamily,
     commentstyle=\color{gray45},
     keywordstyle=\bfseries,
     %
     numbers=left,
     numbersep=15pt,
     numberstyle=\tiny,
     numberfirstline = false,
     breaklines=true,
   }
 
% minimizar fragmentado de listados
\lstnewenvironment{listing}[1][]
   {\lstset{#1}\pagebreak[0]}{\pagebreak[0]}
 
\lstdefinestyle{consola}
   {basicstyle=\scriptsize\bf\ttfamily,
    backgroundcolor=\color{gray75},
   }
 
\lstdefinestyle{C++}
   {language=C++,
   } 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\setcounter{tocdepth}{3}

%Empieza el documento
\begin{document}

% Definimos titulo, autor, fecha, generamos titulo e indice de contenidos
\title{ARTE DIGITAL: UNA PROPUESTA DE INSTALACIÓN BASADA EN TÉCNICAS DE PERCEPCIÓN Y VIDA ARTIFICIAL MEDIANTE LA CREACIÓN DE UN SDK PARA LA CREACIÓN DE INSTALACIONES}
\author{Antonio José Sánchez López}
\date{Escuela de Ingeniería Informática.\\
Universidad de Las Palmas de G.C.}

\thispagestyle{empty}
\maketitle

%Definimos página posterior a la portada 
\thispagestyle{empty}
\noindent Escuela de Ingeniería Informática. Universidad de Las Palmas de G.C.

% Definimos página con datos sobre el proyecto 
\newpage
\thispagestyle{empty}
\section*{Proyecto fin de carrera}

\bigskip
\noindent {\bf Título: } Arte Digital: Una Propuesta de Instalación basada en Percepción y Vida Artificial mediante la creación de un SDK para la creación de Instalaciones\\
{\bf Apellidos y nombre del alumno: } Sánchez López, Antonio José\\
{\bf Fecha : } Octubre 2011 \\

\vspace{2cm}
\noindent{\bf Tutor: } Castrillón Santana, Modesto \\

% Definimos página posterior 
\newpage
\thispagestyle{empty}
\noindent Escuela de Ingeniería Informática. Universidad de Las Palmas de G.C.
\newpage

% Definimos una  pagina para los agradecimientos
\newpage
\thispagestyle{empty}
\section*{Agradecimientos}
 Aquí ponemos los agradecimientos

% Definimos página posterior 
\newpage
\thispagestyle{empty}
\noindent Escuela de Ingeniería Informática. Universidad de Las Palmas de G.C.
\newpage
\setcounter{page}{1}
\tableofcontents

% Definimos página posterior 
\newpage
\thispagestyle{empty}
\noindent Escuela de Ingeniería Informática. Universidad de Las Palmas de G.C.
\newpage

% Empezamos capitulos


\chapter{Introducci\'on}

 La introducción es lo primero que se lee, pero habitualmente lo último que se escribe. Pues su redacción
 depende de cómo se hayan escrito todas las otras secciones. Normalmente la introducción incluye una
 descripción muy general del proyecto y termina con un desglose del contenido de la memoria.

A lo largo de la historia, diversas disciplinas han hecho uso de la representación bidimensional para mostrar propuestas creativas. El ordenador es en la actualidad una herramienta de enorme potencial para el arte visual \cite{Spalter99}, tanto en el marco de la imagen estática, como en el contexto donde el factor tiempo se introduce en el proceso expresivo.
\\

Las imágenes son fácilmente comprendidas por los humanos, motivo por el cual es un ámbito válido de trabajo creativo. Por otro lado, esa diversidad posible en una imagen ocupa también a multitud de científicos del campo de la Visión por Computador en su búsqueda de técnicas para detectar y reconocer objetos en ese espacio de representación: la imagen. Espacio donde un humano reconoce objetos conocidos con gran facilidad.
\\

Desde el punto de vista de la imagen estática, una imagen es una matriz de píxeles que representan un punto en un espacio de muy alta dimensionalidad. La tecnología digital en este contexto, presenta la singularidad de la no existencia de un original único, el arte digital permite disponer del original en cualquier parte, éste es copiable hasta la saciedad sin pérdida. Adicionalmente, nuevos ámbitos tecnológicos han ido abriendo capacidades y posibilidades expresivas. Las tecnologías del vídeo digital y la animación introducen el factor tiempo en el proceso expresivo, la mutabilidad, la fugacidad y la narrativa temporal. La introducción de la interactividad a través del uso de las tecnologías de visión por computador aporta un nuevo canal expresivo y unas posibilidades para la generación de sensaciones a través de los conceptos de obra viva e interactiva, tal y como ya describiera Krueger en su concepto de Realidad Artificial \cite{Krueger85}. 
\\

La obra se puede convertir así en única y cambiante, reactiva a la interacción con su entorno en cada momento. Recupera el concepto de exclusividad, siendo además posible registrar la vida de la instalación. Este enfoque se relaciona de forma clara con el concepto de instalación manejado en el mundo artístico una obra es instalación si dialoga con el espacio que la circunda \cite{Iges99}. 
\\

La motivación de este proyecto es investigar el uso de capacidades actuales de Visión por Computador y Vida Artificial para su integración en instalaciones artísticas. Hay que destacar que nuestra experiencia se relaciona fundamentalmente con el mundo tecnológico, por tanto, no es nuestro objetivo presentar una obra de creación, sino mostrar las posibilidades interactivas que la Inteligencia Artificial puede introducir en el arte, yendo más allá de aplicaciones básicas en visión de segmentación de figura y fondo en contextos restringidos como en Messa di voce \cite{Levin04}, y abordando el contexto de detección de personas y vida artificial. 
\\

La exploración puede plantear y profundizar en nuevas posibilidades de interfaces y formas de interacción hombre-máquina.
\\

Abordar cuando tenga paciencia para repasar documentación y referencias e inspiración para escribir bonito... 
\chapter{Estado actual del tema}
 Descripción del estado actual del tema con referencia a trabajos anteriores en el caso de proyectos que
 sean continuación o relacionados con otros proyectos.
\\
 
Abordar cuando tenga paciencia para repasar documentación y referencias e inspiración para escribir bonito... 

\chapter{Metodología}
 Metodología a utilizar para el desarrollo del proyecto, herramientas de análisis, etc..
\\ 
Las Metodologías de Desarrollo Ágiles son un marco de trabajo conceptual de la ingeniería del software que propone realizar iteraciones de las distintas fases a lo largo del desarrollo. Cada metodología introduce sus propias definiciones, pero en conjunto reflejan siguientes fases: Planificación, Análisis de Requerimientos, Diseño, Codificación, Revisión y Documentación. Una iteración no agrega demasiada funcionalidad en relación al producto final, pero permite tener una demo de la misma al final de cada iteración, momento en el que se evalúa de nuevo las prioridades del proyecto. Así se consigue un producto que podrá probarse desde las primeras semanas al que ir añadiendo funcionalidades \cite{RCol09}. 
\\

La fortaleza del desarrollo ágil se centra en minimizar los riesgos desarrollando software en lapsos cortos y en su capacidad de respuesta al cambio, enfatizando el software funcional como objetivo y promoviendo las comunicaciones eficientes. \cite{KenB01}
\\

En nuestro caso concreto se ha decidido utilizar como metodología ágil el Proceso Unificado Esencial (Essential Unified Process - EssUP), creado por Invar Jacobson \cite{IJacobson09}. EssUp plantea un conjunto de nueve prácticas ligeras y compatibles, que pueden adoptarse de forma individual o combinadas según las necesidades de un proyecto en cuestión. Estas prácticas son: Definición de Casos de Uso, Definición de Iteración, Definición de Arquitectura, Definición de Componente, Definición de Modelo, Definición de Producto, Definición de Proceso, Definición de Equipo y Definición del Proceso Unificado.

%\begin{itemize}
%\item Definición de Casos de Uso
%\item Definición de Iteración
%\item Definición de Arquitectura
%\item Definición de Componente
%\item Definición de Modelo
%\item Definición de Producto
%\item Definición de Proceso
%\item Definición de Equipo
%\item Definición del Proceso Unificado
%\end{itemize}

Para el desarrollo de la aplicación se hará uso de los apartados necesarios en cada fase del proyecto. Invar Jacobson también ofrece software enfocado al seguimiento de estas tareas de forma gratuita. Estas herramientas son:

\begin{itemize}
\item EssWork: Herramienta para el desarrollo de software orientado a la práctica que combina tareas para un trabajo flexible, con coherencia y efectivo. 
\item Essential Modeler: Herramienta visual que permite crear diagramas de casos de uso UML y modelos de clases.
\end{itemize}

Finalmente, para el caso del Diseño de la Base de Datos cabe destacar que se usará el modelo de Diagramas de Entidad-Interrelación, que se continuará con un proceso de transformación mediante Diseño Relacional para obtener finalmente la Base de Datos.

\chapter{Recursos necesarios}
 Se detallan los recursos hardware, software u otros necesarios para el desarrollo del proyecto, incluyendo desde las aplicaciones necesarias así como librerías a utilizar y requerimientos hardware.
\\
\subsection{Software}

\begin{itemize}

\item Aplicaciones:
\begin{itemize}
\item TexMaker: Editor de LaTex.
\item EssWork: Editor de modelos de Casos de Uso.
\item Essential Modeler: Para el seguimiento y orientación de la metodología EssUp.
\item Dia: Para modelado de diagramas UML y Entidad-Interrelación.
\item Visual Studio 2008 Express Edition: Entorno de desarrollo.
\item Blender: Programa de edición 3D.
\item Gimp: Programa de edición 2D.
\item HeidiSQL: Front-end de MySQL.
\end{itemize}

\item Librerías:
\begin{itemize}
\item De Visión por Computador: OpenCV.
\item Motor gráfico: Se estudiarán UDK, CryEngine 3 ESDK, Crystal Space, OpenSceneGraph, Ogre, Irrlicht y Panda3D.
\item Interfaz gráfica de usuario: se estudiarán Qt y wxWidgets.
\item Para captura de audio y reproducción 3D se estudiará las capacidades del motor gráfico o de juego a usar, así como Fmod, OpenAL, SDL Mixer, Clunk, Irrklang y SFML.
\item Para composición de audio: Se estudiarán Chuck y Marsyas.
\item De sistema: Boost y SFML.
\item BBDD: MySQL, PostGreSQL.
\item De documentación: Doxygen.
\end{itemize}

\item Recursos:
\begin{itemize}
\item Imágenes: GCTextures.
\item Sonidos: wiki.laptop.org,go,Soundsamples; Jamendo.
\end{itemize}

\end{itemize}

\subsection{Hardware}
\begin{itemize}

\item Primera aproximación:
\begin{itemize}
\item Entorno controlado: fondo estático, buena iluminación, sin ruido ambiente.
\item 1 PC con: 1 Monitor/Proyector, 1 cámara, micrófono y altavoces sonido envolvente.
\end{itemize}

\item Segunda aproximación:
\begin{itemize}
\item Entorno controlado: fondo estático, buena iluminación, sin ruido ambiente, mobiliario.
\item 1 PC con: 1 Monitor/Proyector, 2 cámaras, micrófono y altavoces sonido envolvente.
\end{itemize}

\item-Tercera aproximación:
\begin{itemize}
\item Entorno controlado: fondo estático, buena iluminación, sin ruido ambiente, mobiliario.
\item 1 PC con: 2 Monitores/Proyectores, 2 cámaras, micrófono y altavoces sonido envolvente.
\end{itemize}

\item Cuarta aproximación:
\begin{itemize}
\item Entorno controlado: fondo estático, buena iluminación, sin ruido ambiente, mobiliario.
\item Varios PCs con: 2 Monitores/Proyectores, 2 cámaras, micrófono y altavoces sonido envolvente.
\end{itemize}

\end{itemize}


\chapter{Plan de trabajo y temporización}
A continuación se muestra el plan de trabajo para el presente proyecto desglosado en etapas, con una estimación en cada etapa del tiempo de ejecución. Siguiendo un proceso iterativo, donde se construirán versiones cada vez más completas, se definen las siguientes cinco etapas:

\begin{itemize}
\item Etapa 1: Acercamiento
\item Etapa 2: Primera Demo: Bases de la interfaz entre módulos, Aplicación principal, GUI, Percepción y Producción
\item Etapa 3: Segunda Demo: Usuario, Entorno, Configuración y Persistencia
\item Etapa 4: Tercera Demo: Detección, Navegación e Interacción Básica
\item Etapa 5: Cuarta Demo: Interacción Avanzada y Creación de Contenidos
\end{itemize}

\begin{figure} [h]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/planificacion.png}
\end{center}
\caption{ \label{Temporizacion} Plan de trabajo y temporización}
\end{figure}



\chapter{Etapa 1: Acercamiento}

En esta fase se llevará a cabo la primera aproximación a la solución que se va adoptar. Para ello se hará un estudio global de las capacidades de las que se desea dotar a la aplicación, que pretende ser completo aunque no en profundidad. También se hará una búsqueda y estudio de las herramientas disponibles que puedan ofrecer las funcionalidades necesarias para la construcción del proyecto.

En las siguientes fases/iteraciones se abordarán y trabajará en profundidad un subconjunto de cada apartado con el objetivo de conseguir demos intermedias que sean funcionales.

\section{Análisis}

\subsection{Documentación y herramientas}
  Existe un conjunto de librerías bastante amplio que puede ser de utilidad en el desarrollo del proyecto. A continuación se hace una revisión de las mismas con el fin de elegir las que mejor se adapten a las necesidades del proyecto. Finalmente se elegirán las más adecuadas.
\\

(RECORDAR AÑADIR LA FEATURE-LIST DE LOS DISTINTOS MOTORES - copy paste en forma de tablas bonitas)

\subsubsection {UDK: Unreal Development Kit}
  A noviembre de 2009, el motor gráfico Unreal3, desarrollado por Epic Games, es considerado uno de los más potentes y de mayor calidad junto con el motor CryEngine de Crytek. A mediados del mismo mes Epic Games decide liberar UDK, su kit de desarrollo para proyectos sin ánimo de lucro, además de ofrecer licencias más asequibles para proyectos comerciales. Inicialmente el coste de una licencia para Unreal3 oscilaba alrededor los \$700.000.  Esta nueva situación hace posible tomarlo en consideración para el desarrollo de la aplicación que tratamos, por lo que se procede a su estudio. Es interesante comentar que Epic Games ya trabaja en la cuarta de versión de su motor, Unreal4, que planea estar disponible a partir de 2012~-2018.
\\

Algunas de las compañías que han usado Unreal 3 son: Atari, Activision, Capcom, Disney, Konami, Koei, 2K Games, Midway, THQ, Ubisoft, Sega, Sony, Electronic Arts, Square Enix y 3D Realms. Entre los ejemplos más destacados se encuentran las series BioShock o Gears of War, entre otros juegos como Medal of Honor: Airbone y Unreal Tournament 3.
\\

Sin duda, los resultados que pueden verse en las imágenes corresponden a proyectos de grandes compañías que disponen de una importante cantidad de recursos. Como muestra visual más razonable se encuentra uno de los juegos de ejemplo que puede encontrarse en la página oficial del UDK, Whizzle.
\\

Al ser un kit de desarrollo completo su lista de capacidades es muy extensa. En la siguiente tabla se muestra un resumen de su lista de capacidades de render:
\\

Finalmente, se exponen algunas de las características clave para la toma de decisión:
\\
\begin{itemize}
\item UDK es un kit de desarrollo software que ofrece un entorno completo para el desarrollo de videojuegos o aplicaciones similares. 
\item Está implementado con C++.
\item Aunque el motor Unreal3 es multiplataforma, UDK sólo está disponible para los Sistemas Operativos Windows por ahora.
\item El desarrollo de una aplicación en UDK puede realizarse en gran medida de forma visual a través de las opciones de la interfaz del entorno o escribiendo código en el lenguaje propio UnrealScript, similar a C++.
\item El código escrito en UnrealScript se ejecuta sobre una máquina virtual propia.
\item La máquina virtual de UnrealScript simula ejecución multihilos. Destacan que permiten la gestión de grandes cantidades de hilos, cantidad que hecho de otra forma, con el sistema de hilos nativo de Windows, podría provocar problemas.
\item La ejecución de código UnrealScript es mucho más lento que C++. Detallan que el código en C++ es 20x más rápido.
\item No se tiene acceso al código fuente del motor.
\item La interacción con otro tipo de código en tiempo de ejecución debe hacerse a través de ficheros (.ini) que pueda leer UnrealScript, modificando o implementando controladores de periféricos, o a través de conexiones TCP/UDP. Este punto dificulta en gran medida la interacción con otras librerías necesarias para el desarrollo del proyecto.
\item Es gratis para uso no comercial. En proyectos comerciales es necesario abonar una licencia y un porcentaje de las ventas en concepto de royalties que puede llegar al 25\% para ingresos superiores a 5000 euros.
\end{itemize}
Las capacidades del motor gráfico son sobresalientes. Sin embargo, el esquema de desarrollo con UDK no se ajusta adecuadamente al del  resto de la aplicación.

\subsubsection{CryENGINE 3 Educational SDK}

Como era de esperar, poco después de la salida del Unreal Development Kit por parte de Epic Games, otro de los grandes motores gráficos comerciales lanzó su propia alternativa. Se trata en este caso del motor CryENGINE 3, de Crytek, con su propio kit de desarrollo con licencia gratuita. Sin embargo, hay que destacar que los términos de la misma son mucho menos flexibles que la de Epic Games y enfocado sólo hacia un sector. El SDK sólo está disponible para instituciones académicas por parte de jefes de curso, exclusivamente para proyectos de naturaleza interna y con fines no comerciales. Se excluye específicamente el caso de estudiantes individuales o proyectos de grupo. Además, los términos de la licencia en sí no son públicos y sólo son accesibles para personal académico previa solicitud. 
\\

Por ahora el único juego que parece usar este motor es Crysis 2, actualmente en desarrollo por Crytek. 
\\

Aunque una alternativa más limitada, en términos de licencia de uso, y en contradicción con algunos puntos de la naturaleza del proyecto, como es el uso de software gratuito y en la medida de lo posible libre, podría ser una alternativa a tener en cuenta y se procede a su análisis. 
\\

En la siguiente tabla se muestra un resumen de su lista de capacidades de render:
\\

Finalmente, se exponen algunas de las características clave para la toma de decisión:

\begin{itemize}
\item Licencia de uso muy restrictiva. Sólo para proyectos académicos no comerciales y no puede ser solicitada por estudiantes individualmente. Continuidad del proyecto también limitada.
\end{itemize}

\subsubsection{Crystal Space}
Crystal Space es un entorno de desarrollo comúnmente usado con motor de juego. Está escrito en C++ usando un modelo Orientado a Objetos y soporta las principales plataformas. El SDK se distribuye bajo licencia LGPL, por lo que puede utilizarse de forma gratuita para cualquier tipo de desarrollo. Durante mucho tiempo se le ha considerado como uno de los motores libres más completos y populares, ya que no sólo es un motor de render sino un motor de juego, que incluye distintos módulos que facilitan el desarrollo.
\\

Algunos ejemplos de uso son PlaneShift, El Hierro Virtual (ULPGC) y el reciente Yo Frankie! (Apricot Open Game Project).

\begin{itemize}
\item Relación estrecha con Blender, entorno de diseño 3D y de desarrollo de videojuegos con el que se dispone de cierta experiencia.
\item Complejidad alta y relativamente abandonado, pasando largos períodos de tiempo entre revisiones.
\item Nueva versión del SDK publicada el 25 de Enero de 2010, que incluye algunas mejoras y pocas funcionalidades añadidas. Entre ellas destaca el uso de un plugin basado en OpenAL para ofrecer sonido 3D.
\item Resultados visualmente pobres y de bajo rendimiento incluso en proyectos desarrollados por equipos con experiencia.
\end{itemize}

(completar)

\subsubsection{Ogre, Object-Oriented Graphics Rendering Engine}
Ogre es considerado uno de los motores gráficos open-source multiplataforma más populares y de más éxito. Está escrito en C++ y está dirigido al mismo lenguaje, aunque existen wrappers para Python, Java y .NET. 
\\

A diferencia de otros, no está planteado como un motor de juego, sólo como motor gráfico. Aunque se le puede añadir con facilidad otras librerías. La filosofía de Ogre es que el desarrollador pueda añadir Ogre como un módulo más en su aplicación, así como los módulos que necesite, en lugar de tener que adaptar la aplicación para que encaje correctamente con una solución que pretenda abarcar más de lo que se necesita y provocando incomodidades e incompatibilidades.
\\

\begin{itemize}
\item Altamente modular, se le pueden añadir librerías y plugins con facilidad. 
\item Esto mismo hace que las capacidades sean más limitadas que las de los motores y SDKs.
\item Dispone de un equipo de desarrollo dinámico y de una comunidad muy activa.
\item Nueva versión publicada el 31 de Diciembre de 2009, con una lista amplia de mejoras.
\item Utiliza la licencia del MIT-X11, por lo que Ogre es gratuito, libre y puede usarse para cualquier tipo de desarrollo.
\end{itemize}
(completar)

\subsubsection{OSG: OpenSceneGraph}
OSG se presenta como una librería multiplataforma para el desarrollo de aplicaciones que requieran de visualización 3D con alto rendimiento, como visualización científica, realidad virtual o comúnmente juegos. 
\\

Es una librería ampliamente usada, con cierta tendencia hacia la visualización científica y geográfica, con una lista amplia de capacidades. Algunos ejemplos de uso son TerrainView, Pok3D, Nasa'a Blue Marble y Capaware!/Geviemer (ULPGC)

\begin{itemize}
\item Altamente modular y potente, con multitud de extensiones. 
\item Dispone de un equipo de desarrollo dinámico, con revisiones frecuentes y de una comunidad activa.
\item Se dispone de cierta experiencia con el entorno.
\item El diseño de la API puede ser confuso y en cierta medida deficiente en términos de facilidad de uso.
\item Documentación pobre.
\item Utiliza licencia LGPL, por lo que es gratuita, libre y puede usarse para cualquier tipo de desarrollo.
\end{itemize}
(completar)

\subsubsection{Irrlicht: Lightning Fast Realtime 3D Engine}
Motor gráfico 3D multiplataforma de código abierto y de alto rendimiento. Está escrito en C++, pero también esta disponible para lenguajes .NET, así como otros lenguajes como Java, Perl, Ruby, Python o Lua, mediante bindings. 

\begin{itemize}
\item Altamente modular, potente y con multitud de extensiones. 
\item Dispone de un equipo de desarrollo dinámico, con revisiones frecuentes y de una comunidad activa.
\item Utiliza la licencia zlib/libpng, por lo que es gratuito, libre y puede usarse para cualquier tipo de desarrollo.
\item Mediante IrrEDIT se peude construir escenarios con facilidad que peuden usarse dentro de Irrlycht.
\item Sin embargo, otras librerías como IrrKlang, para sonido, se distribuyen bajo licencias más restrictivas.
\item Se define a la altura de los motores comerciales pero tiene ciertas carencias.
\end{itemize}
(completar)

\subsubsection{Panda3D: Free 3D Game Engine}
Originalmente Panda3D, acrónimo de Platform Agnostic Networked Display Architecture, fue creado por Disney como parte de una de sus atracciones y posteriormente liberado, en 2002, con la intención de facilitar el trabajo con universidades y projectos de investigación en Realidad Virtual. 
\\

Acogido por el Carnegie Mellon Entertainment Technology Center, fue mejorado y preparado para su uso público. Panda3D se define como un motor de juego y un entorno de de trabajo para render 3D y desarrollo de juegos, libre, gratuito y multiplataforma, que puede usarse para cualquier tipo de desarrollo. Incluye motor gráfico, audio, gestión de entrada/salida y detección de colisiones entre otras capacidades.

\begin{itemize}
\item Dispone de un equipo de desarrollo dinámico, con revisiones frecuentes y de una comunidad activa. Aunque mayormente para Python.
\item Sin embargo, existe un menor dinamismo y soporte para C++ que para Python.
\item Tiene un diseño orientado a mejoras futuras e incluye funcionalidades avanzadas cercanas a los motores comerciales que aún no incluyen otros motores libres.
\item Diseño de la API atractivo y sencillo de usar.
\item Énfasis en la documentación de la librería, en distintas modalidades: manuales, referencias, y plantillas conceptuales, que facilitan en gran medida la comprensión y el uso de la librería. 
\item Se añade a la documentación tradicional diversos video tutoriales creados tanto por la comunidad, así como clases impartidas sobre Panda3D por David Rose, del Instituto de Realidad Virtual de Walt Disney.
\item Ampliamente usada por alumnos del Carnegie Mellon y en proyectos de equipos pequeños con resultados visuales atractivos y con buen rendimiento. 
\item Se distribuye bajo una licencia basada en BSD que permite su uso libre y gratuito para todo tipo de proyectos.
\end{itemize}
(completar)

\subsubsection{Doxygen: Source code documentation generator tool}
Generador de documentación multiplataforma para C++, entre otros lenguajes. Permite generar documentación online para su visualización en un navegador web (HTML), así como LaTex, MS-Word, PostCript, PDF con hipervínculos entre otros. Además puede usarse para extraer la estructura de código de fuentes no documentados, y generar grafos de dependencias, diagramas de clases y otros esquemas.
\\

Doxygen se distribuye bajo licencia GPL. De cualquier forma, los documentos producidos mediante Doxigen son trabajo derivado de los datos usados en su producción, por lo que no se ven afectados por la licencia.
\\

No se estudian más casos por considerarse la mejor(¿única?) opción y tener cierta experiencia con ella, lo que es una ventaja determinante.
\\

\subsubsection{Boost C++ Libraries }
Boost es un conjunto de librerías de código abierto multiplataforma con la intención de extender las capacidades del lenguaje de programación C++. Varios fundadores de Boost forman parte del Comité ISO de Estándares de C++, por lo que algunas de estas librerias terminan por introducirse en la siguiente versión estándar de C++. Utiliza una licencia propia, la Boost Software License, que permite su uso en cualquier tipo de proyectos, comerciales o no. 
\\

Algunas de las librerías de mayor uso son las que facilitan las operaciones con el sistema, I/O y gestión de hilos, entre muchas otras.


\subsubsection{Qt: Cross-platform Application and UI Framework}

Librería multiplataforma de Trolltech para desarrollar interfaces gráficas de usuario, así como para desarrollar aplicaciones de consola y servidores. Utiliza como lenguaje C++, aunque también está disponible para otros lenguajes a través de binding (Python, C\#, Ruby, Java, Ada y Php, entre otros).
\\

Qt es ampliamente usado y considerado como una de las opciones multiplataforma más completa y estable. Algunos ejemplos de uso son principalmente KDE, a partir del cual logró un considerable éxito y expansión, además de otras aplicaciones como Google Earth o Skype.
\\

Tras polémicas en sus inicios por publicitarse como código libre sin serlo, actualmente la biblioteca es gratuita y libre tomando en consideración las condiciones de las opciones LGPL 2.1 y GPL 3.0 para el proyecto. También existe otra opción de pago destinada para software comercial que no quiera cumplir con las condiciones anteriores. 


\subsubsection{wxWidgets: Cross-platform GUI Library}
Se trata de una librería para C++ para el desarrollo de interfaces gráficas de usuario. Es una de las pocas opciones realmente multiplataforma, gratuita y open source disponibles. Tiene la capacidad para ofrecer interfaces de aspecto nativo dependiendo de la máquina sobre la que se ejecuta el código. 
\\

wxWidgets, aunque con sus desventajas y limitaciones, es una libería ampliamente usada que ha creado comunidad. Algunos de sus usarios más conocidos son: AOL(AOL Comunicator), California Institute of Technology (Gambit), Carnegie Mellon University (Audacity),   Grisoft Inc. (AVG Antivirus), NASA (NASGRO), National Human Genome Research Institute - USA
(ComboScreen), TomTom (TomTom HOME), Xerox (VIPP) o la propia Universidad de Las Palmas de Gran Canaria (Capaware!) entre otros.
\\

Se exponen algunas de las características clave para la toma de decisión sobre su uso:
\begin{itemize}
\item Librería multiplataforma gratuita y open source para desarrollar interfaces gráficas de usuario para aplicaciones de escritorio.
\item No todas las opciones que ofrece la librería funciona en las distintas plataformas, por lo que hay que tener especial cuidado si que quiere que el proyecto sea, en potencia, multiplataforma. Esto limita las opciones de la librería que pueden usarse y la calidad visual del resultado.
\item Existe documentación, foros y una comunidad bastante amplia y activa.
\item Hay que tener en cuenta que durante el funcionamiento de la aplicación no existe una interfaz gráfica destinada al usuario en términos tradicionales. Es decir, no existen ventanas, botones, menús ni indicadores destinados a que el usuario las maneje. Por lo tanto no es un apartado crítico del proyecto. Sin embargo, se desea una interfaz gráfica que permita preparar la instalación en su situación final, además de gestionar ventanas y para mostrar las opciones de configuración de la aplicación. Las capacidades de wxWidgets resultan suficientes para estas necesidades.
\item Se dispone de amplia experiencia previa con la librería en otros proyectos.
\end{itemize}

Se decide optar por esta librería por cubrir las necesidades básicas del proyecto, por ser gratuita y libre, y por tener una extensa experiencia con la misma en otros proyectos, lo que es determinante para considerarla la mejor opción.

\subsubsection{OpenCV: Open Source Computer Vision}
OpenCV es una librería multiplataforma de funciones de visión por computador en tiempo real. Está desarrollada inicialmente por Intel, siendo gratuita tanto para uso comercial como investigación bajo licencia BSD. Surgió en 1999 como una iniciativa de Intel para mejorar aplicaciones intensivas en CPU, formando parte de una serie de proyectos que incluían ray tracing en tiempo real y pantallas de representación 3D. Actualmente acaba de salir la versión 2.0 que incluye amplias mejoras a la interfaz con C++, mejor prototipado, nuevas funciones y mejoras de rendimiento, especialmente en sistemas multicore. 
\\

Algunas de sus aplicaciones son HCI (Human-Computer Interaction), Identificación, Segmentación y Reconocimiento de objetos, Reconocimiento de Caras, Reconocimiento de Gestos, Motion tracking, Ego Motion, Motion Understanding, SFM(Structure from Motion), Calibración estéreo y multi-cámara, Percepción de Profundidad y Robótica móvil.
\\

La librería es utilizada en el proyecto principalmente para las tareas de reconocimiento de objetos, caras, gestos y movimiento, lo que formaría parte del Módulo de Reconocimiento. Además, también es utilizada para el proceso de imágenes con fines estéticos, adoptando los resultados de los algoritmos de reconocimiento como modificadores de las imágenes en si, formando parte del Módulo de Producción.

\subsubsection{Encara2:}
Escribir escribir...

\subsubsection{Fmod: Music \& Sound Effects System}
Librería de audio propietaria y multiplataforma de Firelight Technologies que soporta un amplio abanico de formatos de audio. También tiene capacidad para reproducir sonido 3D en sistemas de sonido envolventes.
\\

La biblioteca es ampliamente utilizada en juegos y varios motores gráficos incluyen soporte para la misma. Algunos ejemplos son: BioShock, Call of Duty 4, Crysis, Far Cry, la saga Guitar Hero, Heavenly Sword, Hellgate: London, Metroid Prime 3, Second Life o World of Warcraft, entre otros.
\\

Fmod está disponible siguiendo distintos esquemas. Sin embargo, no es de código abierto y sólo es gratuito para desarrollo de aplicaciones no comerciales.

\subsubsection{OpenAL: Cross-platform 3D Audio API}
API de audio multiplataforma desarrollada por Creative Labs destinada la reproducción de audio posicional y multicanal en 3D. Se ideó para su uso extenso en videojuegos siguiendo las mismas convenciones que OpenGL, consiguiendo convertirse en un estándar aceptado. 
\\

La biblioteca es ampliamente usada en juegos y varios motores incluyen soporte para la misma. Algunos ejemplos relativamente recientes son: Doom3, Quake4, Unreal2, Unreal Tournament 3 o Hitman2. 
\\

Sin embargo, en los últimos años el mantenimiento de la misma se ha descuidado y aparecen errores, especialmente en sistemas de 64 bits. Son comunes las nuevas variaciones de OpenAL que le dan continuidad, como OpenAL Soft, OpenAL++ o distintas librerías a modo de wrapper que usan OpenAL por debajo, como SFML.
\\

De cualquier forma, OpenAL parece ser la única opción multiplataforma gratuita y libre para cualquier tipo de desarrollo que provea de posicionamiento 3D de audio en sistemas de sonido envolventes.

\subsubsection{SDL\_Mixer: Simple DirectMedia Layer Mixer}
Partiendo de la SDL, librería multiplataforma que permite el acceso de bajo nivel a dispositivos de audio, periféricos y hardware 3D, SDL\_Mixer se centra en el primer apartado facilitando la mezcla de sonido multicanal, así como la carga de samples y de música de distintos formatos.
\\

Sin embargo, aunque permite el acceso a bajo nivel, no aporta mayores funcionalidades por sí misma.
Bla bla bla...
\\
(completar)

\subsubsection{Clunk: Open Source 3D-Sound library}
Biblioteca de creación reciente de código abierto para C++ que pretende dar soporte para la generación de sonido 3D, binaural, en tiempo real. Propone una API bastante manejable y sencilla de usar, con un modelo orientado a la gestión de objetos. Sin embargo aún se encuentra en fase de testeo antes de lanzar su primera release. 
\\

Sin embargo, está preparado para generar sonido binaural, el cual tiene su mejor efecto en auriculares y no tanto en altavoces. Por otro lado del modelo de escucha sólo tiene en consideración posición, velocidad y dirección de orientación, sin información de verticalidad; por lo que no es posible definir realmente su estado en un entorno 3D, sólo en un plano.

\subsubsection{Irrklang: High level 3D audio engine/API}
API de alto nivel para sonido 2D/3D multiplataforma enfocado hacia C++ y lenguajes.NET. Da soporte para un amplio abanico de formatos de sonido y provee de una API muy sencilla de usar. Se puede encontrar bajo distintas licencias pero sólo es gratuito para desarrollos no comerciales.
\\

Al igual que OpenAL y FMOD éste sí permite definir completamente las propiedades necesarias para orientar en 3D tanto los sonidos como la escucha. Sin embargo, la limitaciones de licencia son menos interesantes que las de una opción más abierta.
\\
(completar)

\subsubsection{SFML: Simple and Fast Multimedia Library}
SFML se ofrece como una API open-source multimedia que provee mecanismos para la gestión del sistema, de gráficos, interfaz gráfica de usuario, sonido, periféricos y de red. La librería es multiplataforma y se encuentra disponible para varios lenguajes como C, C++,.NET, Python o Ruby, entre otros. 
\\

Sigue un diseño Orientado a Objetos y define un interfaz fácil de usar y de integrar en otros proyectos. Se compone de diferentes paquetes que pueden ser usados en conjunto o individualmente. Es de especial interés el paquete de sonido, que funciona a modo de wrapper de OpenAL, arreglando algunos de los bugs que tiene y simplificando su uso. 
\\

Se distribuye bajo licencia zlib/png, de la Open Source Initiative, por lo que es gratuita y abierta para todo tipo de proyectos.
\\

Tiene varios defectos. Por un lado, en el paquete de sonido la documentación es errónea, y puede llevar a confusiones importantes. Además la definición de la API, aunque más simple, limita la funcionalidad real de OpenAL hasta el punto de que no puede orientarse correctamente la escucha en 3D, por lo que sólo puede usarse en 2D o 3D en el plano horizontal. Sin embargo, pequeñas modificaciones en la libería permitirían recuperar esa funcionalidad sin problemas.

\subsubsection{Chuck: Strongly-timed, Concurrent, and On-the-fly Audio Programming Language}\label{SUBSUBSEC_CHUCK}

Chuck es un lenguaje de programación para el análisis, síntesis, composición y producción de audio. La librería es multiplataforma y presenta un modelo de programación concurrente basado en tiempo, altamente preciso (strongly-timed) y con la habilidad de añadir y moficar código en tiempo de ejecución. Chuck soporta dispositivos MIDI, OSC, HID y audio multicanal.
\\

(completar, scripts, etc y estudiar más a fondo junto con Tapestrea, en comparación con Marsyas)

\subsubsection{Marsyas: Music Analysis, Retrieval and Synthesis for Audio Signals}\label{SUBSUBSEC_MARSYAS}
Librería y framework para C++ para el procesado de audio con especial énfasis en aplicaciones de extracción de información en música, creada principalmente por George Tzanetakis y desarrollada de forma abierta. Está enfocado hacia el prototipado rápido y la experimentación en el análisis y síntesis de audio, procurando un alto rendimiento. 

Bla bla bla...

(completar... y estudiar más a fondo en comparación con Chuck)

\subsubsection{PostgreSQL}
Potente Bases de Datos objeto-relacional open source. Es considerada la BBDD open source más avanzada y potente, con más de 15 años de desarrollo. Está disponible para múltiples sistemas operativos Linux, UNIX y Windows y dispone de interfaces con múltiples lenguages, entre ellos C++. 
\\
bla bla bla
\\
Para eliminar el usuario (especialmente en win 7) usar comando NET USER postgres /DELETE

\subsubsection{OODBMS: Object Oriented Data Base Management Systems}
Se han estudiado múltiples variantes de Bases de Datos Orientadas a Objetos. Sin embargo, no ha sido posible encontrar ninguna que se adapte a los requisitos software del proyecto. Se resumen detalles que implican su descarte:
\begin{itemize}
\item EyeDB: Disponible para C++, potente y estable. Sólo está disponible para Linux.
\item Metakit: Base de datos embebida limita su uso y crecimiento futuro del proyecto.
\item db4o: Embebida, sólo para Java o c\#.
\item Orient: Versión para Windows sólo embebida. Mala documentación.
\item NeoDatis: Sólo para Java o C\#.
\item Perst: Embebida.
\item Odaba: Proyecto antiguo, API confusa.
\item Matisse: Licencia gratuitia restrictiva, sólo para prueba de la librería.
\item Jade6: Licencia restrictiva, sólo para prueba de la librería.
\item Bifröst: Abandonado.
\item Cerebrum: ??
\item Frontier: Abandonado.
\item Oviedo3: Abandonado.
\item Thor: Proyecto del MIT, no disponible aún.
\item MongoDB: No es orientada a objetos sino a documentos, mala eficiencia en altas cantidades de transacciones.
\end{itemize}

\subsubsection{Debea: Database Access Library}
Se trata de una librería en C++ que actúa como mapper entre el modelo orientado a objetos de la aplicación y el esquema de persistencia elegido (base de datos SQL, CSV o ficheros XML). En el caso de SQL, aunque encapsula la persistencia de los objetos incluyendo una API sencilla para la carga y almacenamiento de los mismos y sus relaciones, sigue siendo posible ejecutar comandos SQL si se desea realizar consultas elaboradas. Usa la licencia de wxWindows por lo que es gratuita y libre. También está disponible para Linux y Windows. Soporta de forma nativa bases de datos SQLite3 y PostgreSQL, además de ficheros csv y xml y ofrece una API sencilla y una documentación clara.\\
\\
\\
licencias usadas: BSD modificada (Panda3D), GPL (doxygen), Boost software license, wxwindows license (wxwidgets, debea), BSD (OpenCV), zlib-png (SFML).


\subsection {Análisis de Requisitos de Usuario}
La instalación se presenta como un espacio en el que el usuario puede entrar y moverse. Dentro de ese espacio se encontrará con una o varias pantallas, cámaras, micrófonos  y altavoces. A través de las cámaras y de los micrófonos se capturará información para su análisis y para la producción de efectos, y mediante las pantallas y los altavoces se mostrará un entorno. Es mediante la actitud e interacción del usuario con el espacio como el entorno virtual es generado. A su vez, este entorno será capaz de actuar y evolucionar por su cuenta dentro de un esquema de comportamiento, también definido mediante la interacción del usuario durante la sesión. Capacidades añadidas permitirían a un usuario entrar en el entorno de otro usuario. Aunque este apartado se dejará como trabajo futuro en función de la evolución del proyecto.
\\

Teniendo en cuenta estos aspectos, se detallan los siguientes requisitos:

\begin{itemize}
\item Captura de imagen del usuario.
\item Captura de sonido del usuario.
\item Reconocimiento de su posición en el espacio, localización de cabeza/cuerpo, posturas o gestos.
\item Reconocimiento de voz (sonidos, palabras) (en función de la evolución del proyecto)
\item Crear entorno propio (crear, borrar, cargar escenarios).
\item generación de esquema de comportamiento.
\item Generación de efectos visuales.
\item Generación de efectos de audio y composición musical.
\item Generación de elementos 3D.
\item Generación de elementos 2D.
\item Generación de vida artificial.
\item Interacción.
\begin{itemize}
\item Con la escena:
\begin{itemize}
\item Vista: Perspectiva Visual (asociación cámara/cabeza).
\item Oído: sonido estéreo ó 3D envolvente.
\item Provocar sonidos: ruidos, palmas, habla.
\item Movimiento dentro de la escena:
\begin{itemize}
\item No estar.
\item Estar.
\item Quedarse quieto.
\item Moverse. 
\item Contacto del usuario con los elementos de la escena (velocidad: estático, roce, golpe, gesto...)
\item Elementos: Crear, destruir, mover, provocar interacción/respuesta.
\item Definición de Esquema de Comportamiento/Psique: propia y de la escena, estudio del comportamiento e interacción que sentará las bases del estado del entorno (tipo de escena, iluminación, sonidos, efectos, tipos de elementos creados, comportamientos de los mismos).
\end{itemize}
\end{itemize}
\item Con otros usuarios: (en función de la evolución del proyecto)
\begin{itemize}
\item Entrar en escenarios de otros usuarios (limitación de interacción según permisos: ver/tocar/crear/destruir/psique-empatía(poder modificar esquema de comportamiento del otro usuario o que se modifique el suyo)).
\item Ver y comunicarse con otro usuario: avatar visual, comunicación por audio.
\end{itemize}
\end{itemize}
\item  Alta capacidad de personalización del entorno resultante y unicidad del entorno.
\end{itemize}

\newpage
\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/casosdeuso.png}
\end{center}
\caption{ \label{F_CasosdeUso} Modelo de Casos de Uso.}
\end{figure}

\subsection {Análisis de Requisitos de Software}

Además de conseguir como resultado una Instalación que muestre las capacidades tecnológicas en el entorno del Arte, así como la experimentación en nuestras interfaces y formas de interacción del usuario, este proyecto pretende ofrecer un entorno de trabajo para el futuro desarrollo de nuevas aplicaciones de este tipo. De esta forma, la aplicación se enfoca como un proyecto open-source, un marco de trabajo compuesto de diferentes módulos independientes, de forma que la implementación subyacente sea modificable sin afectar al resto de los módulos. Para ello se debe definir un completo esquema de interfaces.
\\

Teniendo en cuenta estas características, se detallan los requisitos software del proyecto:
\begin{itemize}
\item Uso de software libre y multiplataforma.
\item Uso de recursos gratuitos.
\item El framework producido será software libre.
\item Diseño del framework orientado a multiplataforma, para facilitar su futura portabilidad.
\item Diseño de interfaces de forma que los módulos sean fácilmente sustituibles.
\item Sistema local/online persistente (según evolución del proyecto).
\end{itemize}


\section{Diseño}

\subsection{Estructuración de conceptos}
\begin{center}
(REVISAR Y DEFINIR BIEN)
\end{center}

\subsubsection{Aplicación}
-Aplicación:
  -Programa de que se ejecuta en un ordenador.
  -Carga un escenario para un usuario.
  -Puede entrar en escenarios cargados por otros usuarios en otras aplicaciones conectados al mismo sistema.

\subsubsection{Usuario}
-Usuario:
  -El usuario se presenta en la instalación y es reconocido por su cara (ID) y se crea un entorno/escenario.
  -El usuario puede querer crear un escenario nuevo o ver uno anterior (default: decidir cargar último/crear).
  -El usuario puede interactuar con un escenario propio o de otro usuario (preferencia, permisos) (default: todo, ver, tocar), and lógico bit a bit.
  -El usuario puede verse a sí mismo (avatar), a otros usuarios (avatar) y a otros escenarios en red (símbolos de escenarios).
  -Los cambios y la interacción del usuario define un patrón de comportamiento (psique, opciones: egocéntrico, tábula rasa).
\\

-Permisos: 
  -Conjunto de opciones que define la interacción entre un usuario(desea) y un escenario(permite).
  -Operación lógica (umpersand) definiría qué puede hacer un usuario en un escenario.
  -Posibles permisos: Ver/Tocar/Coger/Copiar/Crear/Destruir/Influenciar/LibreAlbedrío

\subsubsection{Entorno}
-Escenario/Entorno:
  -Se presenta ante / es creado por / un usuario.
  -Representa elementos vivos o inertes, definidos mediante objetos 3D, 2D, sonidos y efectos.
  -Responde ante la interacción del usuario y cambia según sus acciones.
  -También cambia por su cuenta.
  -Permite al usuario propietario interactuar cómo desee, la interacción de otros usuarios puede estar limitada.
  -Los cambios y la interacción del escenario define un patrón de comportamiento que establece las bases del mismo (psique).
  -Se rige por la psique pero tiene un determinado factor de aleatoriedad.
  -Componentes: Espacio, entidades.
\\

-Espacio: 
  -Terrenos (discutir estático/dinámico, interactivo o no).
  -Sonido ambiental: autogenerado, gramática para generación de composiciones.

-Entidades:
  -Usuarios: avatares.
  -Objetos no interactivos/inertes (estudiar interacción por físicas).
  -Objetos interactivos: vivos/inertes.
  -Sonido de las entidades: acciones (gramática), etc. (autogenerado: gramática).
  -Sonidos de los usuarios: acciones (autogenerado: gramática), captura por micrófono, comunicaciones chat, etc.

-Objetos interactivos:
  -Colección de elementos 3D, 2D, sonidos y efectos.
  -Composición: algoritmo genético que define la creación de un objeto, tanto física (3D, 2D, sonidos, efectos) como de comportamiento.
  -Capacidades de evolución: reproducción, mutación y asimilación.
  -Vivos: Agentes, revisar modelo BDI (Creencias, Deseos, Intenciones), y de Agentes Híbridos (Nivel Reactivo, Conocimiento, Social).
  -No-muertos: Agentes interactivos totalmente reactivos, sin deseos ni intenciones.
\\

\subsubsection{Resumen}
\textsl{Llegados a este punto se observan las siguientes necesidades:}

\begin{itemize}
\item Aplicación que implemente y englobe las funcionalidades.
\item Interfaz gráfica de usuario genérica para la gestión de ventanas y configuración de opciones de aplicación.
\item Modificación de elementos y datos, carga y guardado de los mismos.
\item Base de Datos.
\item Percepción del entorno mediante técnicas de Visión por Computador y Captura de audio.
\item Generación de elementos con los que componer un entorno (elementos 3D, 2D, audio).
\item Análisis y definición del patrón de interacción del usuario y de la evolución del entorno (psique).
\end{itemize}

\subsection{Diseño de la Aplicación}

A partir de esta descripción se pueden diferenciar y extraer los diferentes módulos de los que es necesario que se componga la aplicación. Hay que tener en cuenta además que se desea conseguir una alta modularidad e independencia entre las distintas secciones, que permita extraer e intercambiar módulos con facilidad, p.e para usen distintas librerías o no usen librerías externas en absoluto. Por ello, se sigue un modelo de interfaces, en la que se define el núcleo de la aplicación y las relaciones entre los módulos. Sobre ella, se implementará cada módulo y finalmente se construirá la aplicación final.

\begin{itemize}
\item Core: Conjunto de interfaces de la aplicación.
\begin{itemize}
\item IAplication: Define la interfaz de la aplicación completa. Es además una composición de los distintos módulos de la misma.
\item IGUI: Interfaz gráfica de la aplicación de ventanas, para las opciones básicas de visualización y configuración.
\item IPersistence: Persistencia de la aplicación, encapsula los cambios que se efectúan sobre los datos de la misma.
\item IPercept: Engloba la interfaz de usuario mediante percepción (Visión por Computador, captura de audio).
\item Iprod: Es el módulo referido a la Producción, generación de elementos 3D, 2D, composición de audio, etc.
\item ICog: Para el ánalisis de la interacción del usuario y definición del concepto de Esquema de Comportamiento/Psique de Entorno y de Usuario.
\end{itemize}
\item VOX: Implementación de la aplicación.
\item Monitor: Para la gestión y monitorización de los accesos a la Base de Datos.
\item Base de Datos
\end{itemize}

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/DiagramadeclasesUML.png}
\end{center}
\caption{ \label{F_DiagramadeClasesUML} Diagrama de Clases UML.}
\end{figure}


\chapter{Etapa 2: Aplicación principal, GUI, Percepción y Producción}

Para el desarrollo de la primera demo se pretende conseguir una aplicación ejecutable que integre el modelo de interfaz y la separación de módulos definida en la etapa anterior, centrándose en implementar las funcionalidades de un subconjunto de módulos básicos. 
\\

Los módulos a abordar son aquellos que aporten la tecnología necesaria para que ésta se ejecute y provea de las herramientas necesarias para posteriormente analizar y generar contenido. Estos es, para capturar y mostrar la información que se manejará en la aplicación. Los módulos son: IGUI, IPercept e IProd. 
\\

La intención es únicamente crear la estructura básica de la aplicación, así como establecer una relación básica de dependencias entre módulos, cargando en cada caso correctamente las librerías utilizadas. 
\\

En este punto no existen los conceptos de usuario ni de entorno. Tampoco se abordará la Persistencia ni, por tanto, la Base de Datos.

\section{Análisis}

\subsection{Análisis de Requisitos de Usuario}
Como se ha comentado estudiaremos un subconjunto básico de los casos de uso para esta primera demo ejecutable. En ella se abordarán las funcionalidades básicas de gestión de aplicación y ventanas que permita su ejecución y mostrar información en distintas ventanas. Además se cargarán los módulos de percepción y de producción. Se mostrarán las imágenes capturadas por las cámaras y el audio grabado, así como un entorno 3D por defecto, con sonido posicional.

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/casosdeuso_primerademo.png}
\end{center}
\caption{ \label{F_CasosdeUsoPrimeraDemo} Modelo de Casos de Uso.}
\end{figure}

\subsection{Selección de Herramientas}

A partir del estudio de las herramientas disponibles se han seleccionado las más adecuadas, tanto por sus características como por compatibilidades técnicas en el conjunto del proyecto.

\begin{itemize}
\item GUI: wxWidgets
\item Motor gráfico: Panda3D
\item Gestión de Hilos: Boost
\item Captura de Audio y Audio Posicional: SFML
\item Captura de Video y Visión por Computador: OpenCV
\end{itemize}

\section{Diseño}

\subsection{Diseño de la Aplicación}
\subsubsection{Breve descripción de los módulos}
\begin{itemize}
\item core:
\begin{itemize}
\item ICog: Interfaz básica para la creación del módulo.
\item IGui: Interfaz básica para la creación del módulo y registro de ventanas.
\begin{itemize}
\item IGuiWindow: Creación, Mostrar/Ocultar.
\end{itemize}
\item IPercept: Interfaz básica para la creación e inicialización del módulo.
\item IPersistence: Interfaz básica para la creación del módulo.
\item IProd: Interfaz básica para la creación, intervención e inicialización del módulo.
\item Application: Interfaz básica para la creación de la aplicación.
\end{itemize}
\item igui:
\begin{itemize}
\item Crear ventana de aplicación.
\item Operación básicas de ventana: mostrar, mover, cerrar, cambiar contenido.
\item Menú de Aplicación: Archivo, Vista, Herramientas, Ayuda.
\end{itemize}
\item ipercept: 
\begin{itemize}
\item Capacidad para capturar imágenes de n-cámaras.
\item Capturar imágenes de las n-cámaras.
\item Mostrar imágenes capturadas.
\item Capturar Audio.
\end{itemize}
\item iprod:
\begin{itemize}
\item Cargar una escena 3D por defecto.
\item Introducir y reproducir audio posicional 3D.
\item Capacidad para mostrar n-ventanas de render de la misma escena 3D, con vistas independientes.
\end{itemize}
\item vox: Carga y ejecución de los distintos módulos.
\end{itemize}

\subsubsection{Diseño General en UML}

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/DiagramadeclasesUML_PrimeraDemo.png}
\end{center}
\caption{ \label{F_DiagramadeClasesUML_PrimeraDemo} Diagrama de Clases UML. Resumen}
\end{figure}

En la figura \ref{F_DiagramadeClasesUML_PrimeraDemo} puede verse un esquema general en UML del proyecto completo. Es uns visión simplificada pero que refleja la arquitectura del sistema.
\\

Se distinguen dos grandes bloques: En azul se encuentra el paquete core, que se compone de las interfaces separadas en módulos; en naranja la implementación de los distintos módulos y de la aplicación en sí. Cabe destacar que se persigue intencionadamente una alta modularidad y que en última instancia parte de estos módulos se ejecutarán en hilos independientes, aprovechando la tendencia actual de procesadores de multi-núcleo.
\\

Debe tenerse en cuenta que los módulos implementados sólo podrán comunicarse entre ellos y con la aplicación a través de dicha interfaz. De esta forma se abstraen los detalles de la construcción de cada componente y se asegura su independencia, de forma que cualquier módulo pueda ser reemplazado o reimplementado de forma independiente sin que afecte al resto del proyecto. También permitiría la construcción de forma flexible de distintas aplicaciones, utlizando sólo los módulos necesarios o aprovechando módulos de otras ya creadas.

\subsubsection{Diseño en UML - Núcleo de la Interfaz}

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/DiagramadeclasesUML_PrimeraDemo_core.png}
\end{center}
\caption{ \label{F_DiagramadeClasesUML_PrimeraDemo_core} Diagrama de Clases UML. Detalle - core}
\end{figure}

La interfaz provee de unas herramientas básicas para la creación de cada núcleo y la comunicación entre ellos. Desde la aplicación y entre los módulos, toda interacción se hará a través de esta interfaz, aislando cada completamente dcada componente e los detalles de implementación de los otros.
\\

Si bien en la fase actual no existe una gran funcionalidad, a medida que avance el proyecto se irán añadiendo nuevas capacidades que permitan, por ejemplo el paso de datos, la generación de contenido o la gestión de objetos.
\\

En este apartado, el trabajo se concentra en los módulos de interfaz gráfica de usuario, percepción y producción. Con ellos se podrá disponer de un entorno con las capacidades básicas para la captura de lso datos necesarios y la visualización de los contenidos a generar.

\subsubsection{Diseño en UML - Módulo de GUI}

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/DiagramadeclasesUML_PrimeraDemo_igui.png}
\end{center}
\caption{ \label{F_DiagramadeClasesUML_PrimeraDemo_igui} Diagrama de Clases UML. Detalle - igui}
\end{figure}

Este módulo controla gestión de la interfaz gráfica de usuario de tipo ventana de la aplicación. 
\\

La interfaz principal se define en la clase MainFrame, desde la que se gestionaría la aplicación con los menús y paneles principales. Por otro lado, hay que tener en cuenta que la aplicación está destinada a mostrar multitud de ventanas independientes, que podrían mostarse en distintas pantallas o proyectores. Un modelo MDI de ventanas flotantes libres del entorno principal es el más adecuado.
\\
 
Hay que tener en cuenta que varias de las librerías a usar aportan mecanismos propios para la visualización de ventanas. Considerando también la simplicidad del uso de estos mecanismos y que las distintas ventanas visualizarían distintos tipos de datos, que pertencen a distintos módulos que se ejecutan en distintos hilos, se plantea por simplicidad definir en este módulo una clase IGUIWindow a modo de wrapper que englobe las funcionalidades que se requieren de los mismos, y que permita su gestión centralizada.
\\

\subsubsection{Diseño en UML - Módulo de Percepción}

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/DiagramadeclasesUML_PrimeraDemo_ipercept.png}
\end{center}
\caption{ \label{F_DiagramadeClasesUML_PrimeraDemo_ipercept} Diagrama de Clases UML. Detalle - ipercept}
\end{figure}

El módulo de percepción está destinado a la captura y análisis de los datos de entrada. Enel sistema que se plantea estos datos llegan a través de disntintas cámaras web y un micrófono. Por el momento, en esta primera demo sólo se procede a la captura de datos, dejando el análisis para fases posteriores. 
\\

Debido a la distinta naturaleza de entrada y procesado para video y audio, MainPercept lanza dos módulos que se ejecutan independientemente, cada uno en un hilo: PerceptAudio captura sonido y lo almacena en un buffer, y PerceptVideo captura imágenes. Este último está preparado para gestionar un número configurable de cámaras web y mostrar sus capturas en sus respectivas ventanas. 
\\

Hay que tener en cuenta que se crean las ventanas para visulizar las imágenes utilizando las herramientas de la propia librería. Esto se hace por simplicidad. Sin embargo, debe notarse que se utiliza el wrapper de ventanas IGuiWindow para encapsular las ventanas creadas con de esta forma y permitir su manipulación desde el módulo de interfaz gráfica de usuario.

\subsubsection{Diseño en UML - Módulo de Producción}

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/DiagramadeclasesUML_PrimeraDemo_iprod.png}
\end{center}
\caption{ \label{F_DiagramadeClasesUML_PrimeraDemo_iprod} Diagrama de Clases UML. Detalle - iprod}
\end{figure}

MainProd es el módulo principal de Producción y también se ejecuta en un hilo independiente. Está destinado no sólo a la visualización del entorno sino a la generación de contenido, tanto gráficos como composición de audio. Sin embargo, en el alcance de esta fase el módulo sólo incorpora el primer apartado, la visualización y reproducción. 
\\

Aunque el módulo se ejecuta en un hilo independiente está preparado para que otros hilos intervengan mediante llamadas a unos métodos determinados. Además, de igual forma que lo hace el módulo de percepción, MainProd esta preparado para manejar un número configurable de ventanas, cuya intención es ser visualizadas en distintas pantallas o proyectores. Estas ventanas también son creadas mediante las capacidades propias de la librería y utilizan IGuiWindow como envoltorio para su manipulación centralizada.
\\

Por otro lado se definen en el espacio las fuentes de sonido y las propiedades del observador (listener) para disponder de sonido posicional en tres dimensiones. 

\section{Implementación}
Se ha escogido como entorno de desarrollo Visual Studio 2008 Express Edition. Esto es debido a que se trata del entorno de desarrollo con que el se tiene mayor experiencia, así como por la comodidad de las herramientas de las que dispone. Se ha escogido la versión Express por ser gratuita y permitir el desarrollo de cualquier tipo de proyectos, tanto comerciales como no comerciales.
\\

El proyecto consiste en una solución o colección de proyectos de VS. Estos proyectos son:
\begin{itemize}
\item core: Núcleo de la interfaz.
\item icog: Proyecto para el Módulo Cognitivo.
\item igui: Proyecto para el Módulo de GUI.
\item ipercept: Proyecto para el Módulo de Percepción.
\item ipersistence: Proyecto para el Módulo de Persistencia.
\item iprod: Proyecto para el Módulo de Producción.
\item vox: Aplicación.
\end{itemize}

El núcleo de la interfaz 'core' se implementa como una librería dinámica y es una dependencia necesaria para todos los proyectos. Por simplicidad los proyectos icog, igui, ipercept, ipersistence e iprod, que correspondería cada uno con un módulo, son librerías estáticas de las que depende la aplicación ejecutable 'vox'.
\\

Hay que tener en cuenta que el IDE elegido no es multiplataforma, por lo que sólo puede ser utilizado bajo el sistema operativo Windows. Sin embargo, todas las librerías utilizadas sí lo son, y el código es C++, por lo que se debería poder crear nuevas soluciones para otros entornos sin grandes cambios. Se propone como alternativa para su futura portabilidad el IDE open source Eclipse o la configuración de una distribución preparada con CMAKE, de forma que se puedan generar proyectos para otros entornos. Sin embargo, esto quedará como trabajo futuro.
\\

Finalmente, comentar que se decide seguir una estructura de ficheros que se corresponda directamente con la jerarquía de los proyectos. Además se generarán los resultados en un directorio común llamado $\backslash$bin y las librerías externas a usar se almacenarán dentro del subdirectorio correspondiente dentro de la carpeta $\backslash$extern. El código de los módulos implementados puede encontrarse en $\backslash$src, y el de la aplicación el $\backslash$app.
\\

Llegados a este punto se procede a revisar la implementación de cada apartado:

\subsubsection{Núcleo de la Interfaz}
$\backslash$src$\backslash$core
\\

Contiene las cabeceras que definen clases virtuales puras, de las cuales heredarán las clases de los módulos que se implementen. Esto es así para asegurar la abstracción e independencia entre los detalles de implementación de cada módulo.
\\

%Como se trata de una librería dinámica hay que tener en cuenta la necesidad del uso de una macro de exportación. 
A continuación puede verse un ejemplo de su uso con la interfaz IPercept:

\begin{lstlisting}[language=C++]
namespace core{
class IPercept 
{ public:
   virtual ~IPercept(){}
   virtual void Delete()=0;
   virtual void Init()=0;
};}
\end{lstlisting}

Este ejemplo es válido para el resto de las interfaces, que incluyen en cada caso los métodos virtuales necesarios. Estos métodos implementados se corresponden directamente con los ilustrados en la figura \ref{F_DiagramadeClasesUML_PrimeraDemo_core}, página \pageref{F_DiagramadeClasesUML_PrimeraDemo_core}. 

%La macro de exportación utilizada es \_COREEXPORT\_ y se muestra en el siguiente cuadro:
%\begin{lstlisting}[language=C++]
%#ifndef _COREEXPORT_
%#define _COREEXPORT_
%
%#if defined(_MSC_VER) || defined(__CYGWIN__) || defined(__MINGW32__) || defined( __BCPLUSPLUS__)  || defined( __MWERKS__)
%    #  if defined( CORE_EXPORTS )
%    #    define COREEXPORT __declspec(dllexport)
%    #  else
%    #    define COREEXPORT __declspec(dllimport)
%    #  endif
%#else
%    #  define COREEXPORT
%#endif
%
%#ifdef _DEBUG
%	#define _CRTDBG_MAP_ALLOC
%	#define _CRTDBG_MAP_ALLOC_NEW
%	#include <stdlib.h>
%	#include <crtdbg.h>
%#endif
%#endif
%\end{lstlisting}


\subsubsection{Módulo de GUI}
$\backslash$src$\backslash$igui
\\

La clase principal del Módulo de GUI y que implementa la interfaz IGUI se llama MainGui. Esta clase es la que se encarga de toda la gestión de la interfaz de ventanas del proyecto y la librería escogida para su implementación es wxWidgets. 
\\

Las instrucciones para descargar y compilar wxWidgets pueden encontrarse en la web oficial \cite{wx09}. Los paquetes descargados vienen preparados para VS2008 y compilan sin necesidad de ningún ajuste. Sólo hay que incluir las cabeceras y las librerías necesarias, que pueden verse en la sección \ref{detallesImp:ModuloGUI}, página \pageref{detallesImp:ModuloGUI}. 
\\

Hay que tener en cuenta que una de las particularidades de wxWidgets es que necesita tomar control de la aplicación principal, mediante la macro IMPLEMENT\_APP. Este detalle puede verse en en profundidad en la sección \ref{sec:AplicacionPrincipal}, página \pageref{sec:AplicacionPrincipal}. De todas formas, eso no afecta a la construcción y gestión de la interfaz, que se realizará completamente en este módulo.
\\
 
Por otro lado, hay que tener en cuenta que distintos módulos tienen la capacidad de crear ventanas para mostrar información y que se desea usar esos mecanismos. Para ello se utiliza la interfaz IGuiWindow para encapsular la creación y la gestión, y se define MainGui como estática para poder acceder a la misma instancia desde cualquier módulo que lo necesite. Para asegurar que el acceso a la instancia sea seguro se utiliza un cerrojo como mecanismo de sincronización, implementado con boost.

\begin{itemize}
\item MainGui: Crea y mantiene los elementos de la interfaz. Se compone de un marco principal de aplicación llamado MainFrame y diversos paneles. Así mismo, también mantiene una lista de las ventanas independientes que se hayan registrado. Se crea la macro IMPLEMENT\_MIIAPP para encapsular la macro de wxWidgets IMPLEMENT\_APP. Un resumen de la clase puede verse en el siguiente cuadro:
\begin{lstlisting}[language=C++]
#define IMPLEMENT_MIIAPP(name) IMPLEMENT_APP(name)
namespace core {namespace igui{
class MainGui : public core::IGui
{ public:    ...
  private:		
    MainGui(const std::string &title = "");
    static MainGui* instance; 
    static MainFrame *main_frame;
    static std::map<IGuiWindow*, int> registered_windows;
    static std::map<IGuiWindow*, int> fullscreenable_windows;
};}}

MainGui* MainGui::GetInstance(const std::string &title)
{ boost::mutex::scoped_lock lock(m_mutex);
  if (instance == NULL) 
   instance = new MainGui(title);
  return instance; }
\end{lstlisting}

\item MainFrame: Crea el marco principal de la aplicación. Contiene el menú y los paneles principales. En esta fase los únicos paneles que existen por el momento son el de Inicio y el de Ayuda. Se ha implementado un mecanismo para la sustitución de contenidos de ventanas, de forma que la ventana principal de la aplicación asumirá el contenido de los paneles que se usen en cada momento de forma dinámica.

\begin{lstlisting}[language=C++]
namespace core { namespace igui	{
class MainFrame : public wxFrame
{ public:    ...
 private:
    wxMenu *file_menu, *view_menu, *tools_menu, *help_menu;
    wxMenuItem *item_file_close, *item_view_fullscreen, *item_view_start, *item_tools_configure, *item_help_about;
    ...
    DECLARE_EVENT_TABLE()
    void OnClose(wxCommandEvent& WXUNUSED(event));
    void OnViewStart(wxCommandEvent& WXUNUSED(event));
    void OnHelpAbout(wxCommandEvent& WXUNUSED(event));
    void DismissPanels();};}}
\end{lstlisting}

\item GUIStart: Es el panel principal de la aplicación que contendrá un acceso rápido a las funciones principales de la aplicación mediante los botones: Login, Inicio y Configuración.
\\

El aspecto más relevante de esta clase son los botones  y la necesidad de capturar el evento de render EVT\_PAINT para tener un acceso directo a cómo se dibuja el contenido de la misma. Esto es necesario para conseguir acabados más interesantes; por ejemplo, dibujar una imagen de fondo o texto con fondo transparente sobre imágenes. Debe tomarse el DC (contexto del dispositivo) con el que se va a dibujar, en este caso la pantalla, y usar las operaciones de dibujo directamente.

\begin{lstlisting}[language=C++]
namespace core { namespace igui { 
class GUIStart : public wxPanel 
{ public:   ...
   void OnPaint(wxPaintEvent &evt);
   void paintNow();	        
   void render(wxDC& dc);
 private:
   wxBitmap background_image;
   wxButton *login_button, *start_button, *configure_button;
};}}

BEGIN_EVENT_TABLE(GUIStart, wxPanel)
   EVT_PAINT (GUIStart::OnPaint)
END_EVENT_TABLE()

GUIStart::GUIStart(...):wxPanel(parent, id, pos, size, style, name)
{ background_image = wxBitmap(...);
  login_button = new wxButton(...); } 
  
void GUIStart::OnPaint(wxPaintEvent & evt)
{ wxPaintDC dc(this);
  render(dc); }
  
void GUIStart::render(wxDC& dc)
{ dc.DrawBitmap(background_image, 0, -20, false ); }
\end{lstlisting}

\item GUIHelp: Panel de información de la aplicación donde puede verse la versión y donde añadirá un enlace a la web del proyecto. Los detalles de implementación son muy similares al panel GUIStart.
\end{itemize}

Para ver más detalles sobre la configuración de la librería, dependencias necesarias y la implementación del módulo se puede consultar la sección \ref{detallesImp:ModuloGUI}, en la página \pageref{detallesImp:ModuloGUI}.

\subsubsection{Módulo de Percepción}
$\backslash$src$\backslash$ipercept
\\

La clase principal que implementa la interfaz IPercept y que está destinada a gestionar el módulo se llama MainPercept. Las tareas que se resuelven aquí son la captura de información del espacio de la instalación y de su interpretación. La entrada de datos del sistema consiste en audio mediante micrófonos, y de video, mediante webcams. En esta fase las tareas iniciales se centran en capturar audio y almacenarlo en un buffer, y capturar imágenes de las cámaras para mostarlas. Más adelante, se usará esta información para calibrar los puntos de vista o localizar al usuario dentro del espacio de la instalación para posicionarlo dentro del entorno virtual, entre otras funcionalidades.
\\

Puede apreciarse que la captura de información y su procesado son de naturaleza distinta y, de hecho, independiente; no sólo del resto de la aplicación, sino entre sí. Además hay que tener en cuenta que los accesos a los periféricos son tareas costosas en tiempo. Por ello, la mejor opción es mantener estas tareas de forma independiente, ejecutándose en hilos separados. Cuando el sistema necesite alguna información, solicitará al módulo la más reciente, pero no será necesaria la espera para que éste termine de acceder a los dispositivos o de realizar sus tareas.
\\

Las librerías utilizadas son SFML para la captura de audio, y OpenCV para la captura de video y su posterior proceso. Son librerías que han requerido ajustes importantes para su funcionamiento en el proyecto. Pueden verse los detalles en el anexo \ref{detallesImp:ThirParties} página \pageref{detallesImp:ThirParties}.

\begin{itemize}
\item MainPercept: Implementa los mecanismos necesarios para la gestión global del módulo y de sus componentes, de tal forma que se pueda controlar la entrada de datos y su procesado. Por ahora sólo instancia los módulos necesarios y los inicializa.

\begin{lstlisting}[language=C++]
namespace ipercept { 
class MainPercept : public core::IPercept
{ public:   ...
  private:
   static int num_cam;
   static PerceptAudio* perceptAudio_module;
   static PerceptVideo* perceptVideo_module;
};}}

MainPercept::MainPercept()
{ perceptAudio_module = new PerceptAudio();
  perceptVideo_module = new PerceptVideo(); }

void MainPercept::Init()
{ perceptAudio_module->Init();
  perceptVideo_module->Init(); }
\end{lstlisting}

\item PerceptAudio: Esta clase implementa la interfaz IPerceptAudio y se encargará de ejecutar la captura y procesado de audio. Por ahora sólo captura y almacena el sonido en un buffer. Las librerías usadas son SFML para el acceso a los dispositivos de audio y grabación, y Boost para la gestión de hilos.
\\

Por coherencia se decide seguir un mismo esquema para los módulos de este tipo. Se tratan de clases con métodos y atributos estáticos que internamente lanzan un hilo independiente para la ejecución de su código principal. Todas tienen un método llamado Init(), que internamente llama a DoInit() para inicializar el módulo y ejecutar la llamada a DoMainloop(), el bucle principal, en un hilo aparte. Internamente Iterate() recoge el código destinado a ejecutarse para cada iteración, mientras que Capture() realiza la captura de datos. Se hace uso de cerrojos no bloqueantes definidos en contexto para el acceso a los atributos. Cuando la ejecución de hilo llega a este cerrojo se permite saltar el código bloqueante a la espera de que en la siguietne iteración el cerrojo esté disponible, por lo que el hilo no llega a bloquearse nunca.
\\

\begin{lstlisting}[language=C++]
class PerceptAudio : public core::IPerceptAudio
{ public:    ...
  private:
    static void DoInit();
    static void DoMainLoop();
    static void Iterate();
    static void Capture();
    
    static boost::shared_ptr<boost::thread> m_thread;
    static boost::try_mutex m_mutex;
    static bool initialized, stop_requested;
    
    static sf::SoundBufferRecorder Recorder;
    static sf::SoundBuffer recordingBuffer;};}}
\end{lstlisting}

\item PerceptVideo: Esta clase implementa la interfaz IPerceptVideo y se encarga de ejecutar la captura y procesado de información visual. Por ahora la tarea consiste en capturar imágenes de un número configurable de cámaras web y mostrarlas en ventanas independientes. Como estás tareas son costosas en tiempo es altamente relevante que este módulo se ejecute en un hilo aparte. Las librerias a usar son Boost (\ref{detallesImp:Boost}), para la gestión de hilos, y OpenCV (\ref{detallesImp:OpenCV}), librería de Visión por Computador, para la captura de imágenes y su posterior procesado.
\\

En la construcción del módulo se sigue el mismo esquema que se ha visto en casos anteriores para mantener la coherencia. Así, se dispone de métodos para la inicialización y se separa la ejecución del bucle principal, que es lanzando en un hilo independiente, con cerrojos no bloqueantes. Así mismo, se distingue la ejecución de cada iteración, donde se incluye un método para la captura de datos de entrada. Se dispone de un vector de cámaras que permita la flexibildiad del módulo para recoger datos de un número configurable de fuentes.
\\
A continuación se muestran algunos detalles relevantes. 
\begin{lstlisting}[language=C++]
class PerceptVideo : public core::IPerceptVideo
{ public:   ...
  private:
   static void DoInit();
   static void DoMainLoop();
   static void Iterate();
   static void Capture();
   
   static int num_cam;
   static std::map< int, CvCapture* > capture_cam_array;
   static std::map< std::string, CamWindow* > camWindow_array;};}}
\end{lstlisting}

A continuación se muestra un cuadro de código de resumen que ilustra el proceso de inicialización, creación del nuevo hilo de ejecución y el bucle principal del mismo, donde se capturan los datos de entrada. 

\begin{lstlisting}[language=C++]
PerceptVideo::PerceptVideo()
{ capture_cam_array[i] = cvCaptureFromCAM(i);
  camWindow_array[window_name] = new CamWindow(window_name);}}}

void PerceptVideo::DoInit()
{ assert(!m_thread);
  m_thread = boost::shared_ptr<boost::thread>(new boost::thread(boost::function0<void>(&PerceptVideo::DoMainLoop)));}}

void PerceptVideo::DoMainLoop()
{ while(!stop_requested)
  { Iterate();
    m_thread->sleep(system_time()+milliseconds(10));}}

void PerceptVideo::Iterate()
{  boost::try_mutex::scoped_try_lock lock(m_mutex);
   if (lock) {Capture();}}

void PerceptVideo::Capture()
{ for (...)
  { capture_img = cvQueryFrame(iter->second);
    std::map<...>::iterator cam_iter = camWindow_array.find(window_name);
    cam_iter->second->ShowImage(capture_img);}}
\end{lstlisting}

Para más información se puede consultar la sección \ref{detallesImp:ModuloPercepcion}, página \pageref{detallesImp:ModuloPercepcion}.

\item CamWindow: Esta clase es utilizada para encapsular la creación de ventanas mediante las herramientas que provee OpenCV, de forma que se adapten a la interfaz IGUIWindows. Esto permitirá su acceso y gestión desde el módulo de GUI. Se elige usar los mecanismos que ofrece OpenCV para la creación de ventanas para simplificar el uso de las mismas y permitir su actualización a partir de las herramientas propias de la librería, sin necesidad de estar delegando ni transmitiendo información innecesariamente entre módulos. 
\begin{lstlisting}[language=C++]
namespace core { namespace ipercept {	
class CamWindow : public core::IGuiWindow
{ public:   ...
  private:
   std::string window_name;
   bool isShown;
};}}

CamWindow::CamWindow(const std::string &_window_name)
{ cvNamedWindow(window_name),1);
  core::igui::MainGui::GetInstance()->RegisterWindow(((core::IGuiWindow*)this)); }

void CamWindow::ShowImage(const IplImage *image)
{ if (isShown) cvShowImage(window_name, image);}
\end{lstlisting}

\end{itemize}

\subsubsection{Módulo de Producción}
$\backslash$src$\backslash$iprod
\\

Siguiendo el mismo modelo que en los módulos comentados anteriormente, la clase principal que implementa la interfaz IProd recibe el nombre de MainProd. Es a través de esta interfaz como se manejará el contenido a generar para construir y exponer el entorno. En esta primera demo el obejtivo es integrar un motor gráfico que sea capaz de visualizar una escena 3D por defecto, con sonido 3D en un sistema envolvente, mostrando una cantidad configurable de ventanas.
\\

De la misma forma que en el resto de los casos, este módulo se ejecuta en un hilo independiente, utilizando los mecanismos de sincronización necesarios para el manejo de hilos gracias a la librería Boost (\ref{detallesImp:Boost}). Por otro lado, el motor gráfico integrado es Panda3D. Sin embargo, es necesario destacar que fueron necesarios ajustes en las opciones de compilación del mismo para que las librerias generadas pudieran funcionar correctamente dentro del proyecto. También impone algunas limitaciones como una política restrictiva en el nombrado y jerarquía de directorios, así como algunas modificaciones en los fuentes poco comunes pero que fueron necesarias realizar y aceptadas en la revisión oficial. Para más detalles, consultar \ref{detallesImp:Panda3D}, página \pageref{detallesImp:Panda3D}. 
\\

Finalmente para la localización de las fuentes de sonido y del oyente en el espacio 3D, se ha hecho uso de la librería SFML. De nuevo, es necesario destacar la necesidad de realizar modificaciones en el código fuente de la libreria para que ésta funcione correctamente. Esto es debido que existen errores en el diseño de la API que impide que pueda situarse libremente al oyente en el espacio 3D. Para ver más detalles sobre la integración de esta librería y las modificaciones necesarias se puede consultar la sección \ref{detallesImp:SFML}, página \pageref{detallesImp:SFML}.

\begin{itemize}
\item MainProd: Implementa la interfaz IProd y provee los mecanismos para cargar una escena 3D y mostrarla en una cantidad de ventanas configurable. También carga y localiza en el espacio sonidos y el oyente.

\begin{lstlisting}[language=C++]
namespace core { namespace iprod {
class MainProd : public core::IProd
{ public: ...
  private:
   static void CreateDefaultWindows(int numWindows);
   static void LoadDefaultScene();

   static PandaFramework framework;
   static std::map<int, WindowFramework*> pandawindows_array;
   static std::map<int, NodePath>         windowcamera_array;
   static NodePath cam_viewpoint, origin, up;
   static double listener_position[],listener_target[],sound_pos[];
   static sf::Sound Sound;
};}}
\end{lstlisting}

Las secciones más relevantes son la carga de la escena y el render de las distintas vistas. Por otro lado, se tienen en cuenta políticas de rendimiento y se trata que la escena sea ligera. Por ejemplo, los objetos se cargan una sóla vez para el marco principal y se instancian como referencias en los grafos de escena del resto de vistas, sin necesidad de cargar varias veces el mismo modelo.
\\

Para poder realizar pruebas para comprobar el buen funcionamiento y para preparar trabajo futuro, se añade un mecanismo para el acceso externo a la escena, donde realizar operaciones DoStuff(). En este caso, rotar la cámara. Así mismo, se habilita la navegación libre en la segunda ventana de visualización mediante el uso del ratón (para comprobar que el correcto funcionamiento de la localización 3D tanto de oyente como de sonidos posicionados). 
\\

Gran parte del código se asemeja a los módulos explicados anteriormente, siguiendo por coherencia la misma política de incialización, ejecución del hilo independiente, y bloques de codigo a ejecutar en cada iteración. por lo que sólo se detallanA continuación se muestran las secciones de código más relevantes:
\begin{lstlisting}[language=C++]
void MainProd::DoMainLoop()
{ framework.open_framework(m_argc,m_argv);
  CreateDefaultWindows(DEFAULT_NUM_WINDOWS);
  LoadDefaultScene();
  sf::SoundBuffer Buffer;
  Sound.SetBuffer(Buffer);
  
  while(!stop_requested) 
  { Iterate();
    m_thread->sleep(get_system_time()+milliseconds(10)); }

  framework.close_all_windows();
  framework.close_framework();}
\end{lstlisting}

Algo a destacar es la función Iterate(). Por lo general, los motores gráficos y de juego toman posesión del bucle principal de la aplicación. Este también es el caso del motor de juego elegido, Panda3D. Esta peculiaridad limita el número de actividad y la facilidad con la que nuevas tareas pueden ser añadidas, o no sólo añadidas, sino generadas mientras se ejecuta la aplicación. Sin embargo, la flexibilidad del motor escogido permite abstraerse de las secciones de inicialización y del bucle principal, de forma que se pueden ejecutar las tareas de cada iteración mediante el método step(). Gracias a este mecanismo podemos disponer de la ejecución del bucle principal en un hilo aparte, mientras se ejecutan o generan tareas y objetos dinámicamente y de forma sincronizada, desde el mismo u otros hilos.

\begin{lstlisting}[language=C++]
void MainProd::Iterate()
{ boost::try_mutex::scoped_try_lock lock(m_mutex);
  if (lock)
  { framework.do_frame(graphic_thread);
    CIntervalManager::get_global_ptr()->step();}}

void MainProd::DoInit()
{ if (!initialized)
	{ assert(!m_thread);
      m_thread = boost::shared_ptr<boost::thread>(new boost::thread(boost::function0<void>(&MainProd::DoMainLoop)));}}
\end{lstlisting}
Como se comentaba, el siguiente fragmento de código muestra la forma en que, desde la ejecución de otro hilo puede solicitarse a la clase realizar determinadas tareas. En este caso, se rota la cámara de la primera vista para que gire entorno al centro de la escena, a la vez que se actualiza la posición del oyente en el espacio 3D según la posición de la cámara, con navegación mediante el movimiento del ratón, de la vista2. 
\\

Es necesario tener en cuenta dos puntos en relación al sonido 3D en la aplicación para un sistema envolvente. Por un lado los sistemas de referencia espacial del motor gráfico y del motor de audio no coinciden, por lo que es necesario realizar las transformaciones pertinentes. Por otro lado, la librería de audio utilizada comete errores importantes que deben ser solucionados para poder disponer de un posicionamiento y orientación correctos de las fuentes de sonido y del oyente. Estos detallen pueden verse con mayor profundidad en la sección \ref{detallesImp:SFML}, página \pageref{detallesImp:SFML}.

\begin{lstlisting}[language=C++]
void MainProd::DoDoStuff()
{ boost::try_mutex::scoped_try_lock lock(m_mutex);
  if (lock && initialized)
  { //rotar la cámara en vista 1
    windowcamera_array[1].set_pos(20*sin(angleradians),-20.0*cos(angleradians),3);
    windowcamera_array[1].set_hpr(angledegrees, 0, 0);
    //actualizar oyente según navegación en vista 2
    sf::Listener::SetPosition(new_pos.get_x(), new_pos.get_y(), new_pos.get_z());
    sf::Listener::SetTarget(new_at.get_x(), new_at.get_y(), new_at.get_z() , new_up.get_x(), new_up.get_y(), new_up.get_z());
   }
   else 
   { //no se pudo coger el cerrojo, pero no se bloquea  }
}
\end{lstlisting}
Finalmente se muestra fragmentos que ilustran la carga de la escena y cómo se enlazan a los disintos renderers. Como se comentaba inicialmente, y como es lógico, se ha tenido en cuenta cuestiones de eficiencia para visualización en múltiples vistas. Como en el problema al que nos enfrentamos la escena a visualizar es la misma para todas las vistas, se sigue la política de cargar una única vez los elementos en la escena, ligarlos al render principal e instanciarlos para el resto, de forma que sólo se cargan una vez y no existen copias del mismo objeto en los distintos grafos de la escena.
\begin{lstlisting}[language=C++]
void MainProd::LoadDefaultScene()
{ if (pandawindows_array.begin() != pandawindows_array.end())
  { NodePath environment = pandawindows_array[1]->load_model(framework.get_models(),"environment");
    environment.reparent_to(pandawindows_array[1]->get_render());
    NodePath pandaActor = pandawindows_array[1]->load_model(framework.get_models(), "panda-model");
    pandaActor.reparent_to(pandawindows_array[1]->get_render());
    pandawindows_array[1]->load_model(pandaActor, "panda-walk4");
    pandawindows_array[1]->loop_animations(0);

    std::map<...>::iterator iter = pandawindows_array.begin(); iter++;
    while(iter != pandawindows_array.end())
    { environment.instance_to(iter->second->get_render());
      iter->second->setup_trackball();
      iter++;}}}
\end{lstlisting}

\item Prod3DWindow: Esta clase es utilizada para encapsular la creación de ventanas mediante las herramientas que provee Panda3D, de forma que se adapten a la interfaz IGUIWindows. Esto permitiría su acceso y gestión desde el módulo de GUI. De la misma forma que sucede en el módulo de Percepción para la visualización de la entrada de datos en distintas ventanas, se utiliza por simplicidad las herramientas propias de la librería para la creación de las mismas y la actualización de su contenido.
\begin{lstlisting}[language=C++]
namespace core { namespace iprod {
class Prod3DWindow : public core::IGuiWindow
{ public:
   WindowFramework *GetWindowFrameWork() {return m_windowFramework;}
  private:
   bool isShown;
   WindowFramework *m_windowFramework;};}}
\end{lstlisting}
\end{itemize}

\subsubsection{Aplicación Principal}\label{sec:AplicacionPrincipal}
$\backslash$apps$\backslash$vox
\\

Finalmente se muestra la sección correspondiente a la aplicación principal. Su objetivo es enmarcar los múltiples módulos, gestionar la ejecución de la misma y de sus componentes.
\\

Algo a tener en cuenta es que al usar wxWidgets como librería para crear la interfaz de usuario de la aplicación, se impone un requisito incómodo: wxwidgets necesita tomar control de la aplicación principal. Esto impide la completa independencia entre la aplicación y la librería usada para la GUI. Sin embargo, hay que tener en cuenta que la Aplicación principal no hará nada: esta sección está únicamente destinada la carga de los distinto módulos. Por ello, su complejidad y contenido son mínimos. Es por ello que se decide proseguir.
\\
En el caso de desear cambiar el módulo de GUI, son dos los cambios necesarios en la aplicación: eliminar la herencia de la clase wxApp, y sustituir la macro IMPLEMENT\_MIIAPP(Application) por un cuerpo main() donde se cree una instancia de la clase Application.
\\

A continuación se muestran algunos detalles relevantes:

\begin{itemize}
\item Application: Implementa la interfaz IApplication que se usa para la creación de la aplicación. Su cometido es cargar los distintos módulos y proveer herramientas para su gestión. En caso de no desear usar wxWidgets como librería para la GUI, es necesario eliminar la herencia de la clase a wxApp.
\\

También hay que tener en cuenta que, en Windows, por motivos de incompitibilidades entre librerias, es necesario incluir la cabecera winsock2.h al inicio del fichero. Para más detalles se puede consultar la sección \ref{detallesImp:Incompatibilidades}, página \pageref{detallesImp:Incompatibilidades}

\begin{lstlisting}[language=C++]
#ifdef _WINDOWS
#include <winsock2.h>
#endif
class Application : public wxApp, public core::IApplication
{ public:    ...
  private:
   core::IGui		*app_maingui;
   core::IPercept	*app_mainpercept;
   core::IProd		*app_mainprod;};

bool Application::OnInit()
{ app_maingui = MainGui::GetInstance("VOX");
  app_mainpercept=(core::IPercept*)new core::ipercept::MainPercept(); 
  app_mainprod=(core::IProd*)new core::iprod::MainProd(argc, argv); 
  app_mainpercept->Init();
  app_mainprod->Init();
  return true; }
\end{lstlisting}

\item Main: Fuente en el que se define el punto de entrada de la aplicación. En caso de no desear usar wxWidgets es necesario sustituir la línea IMPLEMENT\_MIIAPP(Application) por la definición de una función main(), donde crear una instancia de la clase Application.

\begin{lstlisting}[language=C++]
#define _WINSOCKAPI_
#include "Application.h"
IMPLEMENT_MIIAPP(Application)
\end{lstlisting}
\end{itemize}


\section{Validación y Publicidad}
\subsection{Validación}
Se sigue una filosofia de pruebas continuas para resolver en el momento en que aparecen los incidentes que puedan surgir al hacer cambios. Así mismo, cuando se añade una nueva funcionalidad, se usan mecanismos para comprobar que que las capacidades incorporadas funcionan correctamente y se mantiene un buen rendimiento.

\begin{itemize}
\item Comprobación de uso de recursos de la máquina mediante las herramientas del sistema. En la máquina en la que se desarrolla la aplicación muestra consumir un 5\% de CPU y 75Mb de memoria mantenidos. Hay que destacar la posibilidiad de que la aplicación consuma mayor CPU en máquinas con menor cantidad de procesadores debido al overhead introducido por la gestión de hilos. Sin embargo, la tendencia actual se mueve hacia el aumento de número de procesadores.
\item Comprobración de la ejecución y cierre correctos, sin salidas de la aplicación inesperadas ni memory leaks.
\item Uso de herramientas para medición de frames por segundo para comprobar el rendimiento de la ventana de render. Se comprueba que se mantiene siempre a 60fps para dos ventanas, y 30fps con hasta 10 ventanas de render. Hay que tener en cuenta que 60 es el límite máximo impuesto por la sincronización vertical del monitor.
\item Uso de distintas opciones de configuración para comprobar las variantes del uso de los módulos, por ahora mediante macros: Abrir distinto número de ventanas de render (entre 1 y 10) y captura desde distintas cantidades de cámaras web (entre 1 y 2).
\end{itemize}

Las características de la máquina de referencia son las siguientes:

\begin{itemize}
\item Procesador Intel Core i7 CPU 870 2.92Ghz (4 núcleos reales con hyperthreading, símil 8 núcleos).
\item 6Gb de Memoria Principal.
\item Sistema Operativo Windows 7 64 bits
\item Tarjeta gráfica GeForce GTS 240
\end{itemize}

Finalmente, se muestran capturas de ejemplo del estado actual de la aplicación, como pueden verse en las siguientes imágenes:

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/screenshot_20100214_p.jpg}
\end{center}
\caption{ \label{F_CapturaFase2PrimeraDemo_Fig1} Captura 1 - Fase2: Primera Demo}
\end{figure}
\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/6windows_30fps_release_01_p.jpg}
\end{center}
\caption{ \label{F_CapturaFase2PrimeraDemo_Fig2} Captura 2 - Fase2: Primera Demo}
\end{figure}

\subsection{Publicidad}
Aunque el desarrollo del proyecto se encuentra aún en sus inicios se considera interesante tener una visión del panorama actual y valorar distintas opciones de publicidad e incluso posibles destinos para la exposición de la instalación. Además, se considera la creación de un portal web para la iniciativa, donde alojar el el entorno de trabajo resultante, asi como el proyecto. 
\\

Se valora establecer contacto con las siguientes instituciones:
\begin{itemize}
\item ULPGC.
\item CAAM: Centro Atlántico de Arte Moderno.
\item Gran Canaria Espacio Digital.
\item Museo Elder de la Ciencia y la Tecnología.
\item La Casa Encendida.
\end{itemize}
Posibles festivales:
\begin{itemize}
\item Artfutura
\item Arco
\item Estampa
\item Sónar
\end{itemize}

\chapter{Etapa 3: Usuario, Entorno, Configuración y Persistencia}
Partiendo de la primera demo estable, se desea añadir añadir nuevas funcionalidades para completar la aplicación. En este punto se dispone de la estructura básica de la aplicación y de las herramientas necesarias para la captura de información y la reproducción de las vistas. Ahora se abordará el problema de la creación de usuarios, proyectos, escenas y su gestión. También se trabajará en las opciones de configuración del sistema básicas.
\\

El punto más relevante en esta etapa consiste en el modelo de persistencia a seguir. Para ello, se propone hacer uso de una base de datos para guardar a los usuarios y sus escenarios, teniendo en mente la posibilidad de implementar en un futuro un sistema multiusuario persistente. Para este tipo de aplicación, convendría conseguir una Base de Datos Orientada a Objetos, a ser posible en Tiempo Real o con un buen tiempo de respuesta. 
\\

Entre otras ventajas, las OODBMS permiten eliminar la necesidad de un doble modelado de la información. El modelo de la base de datos se corresponde directamente con el modelo de datos de la aplicación. De esta forma se ahorra tiempo de diseño y se evita el problema del desajuste por impedancia. Además no es necesaria ninguna transformación explícita del modelo de datos. Por otro lado, se observan mejores respuestas en estos tipos de bases de datos, especialmente en el caso de accesos de tipo navigacional, es decir, aquellos en los que se accede a objetos a partir de relaciones con otros objetos, caso de esta aplicación.
\\

Sin embargo, tras el estudio realizado no ha sido posible encontrar una herramienta que encaje con los requisitos software del proyecto. De entre las candidatas, EyeDB destacaba por ser la más prometedora; sin embargo, actualmente sólo está disponible para sistemas Linux. Del resto, o no disponían de licencias gratuitas, éstas eran muy limitadas, no disponían de APIs para C++ o las librerías eran antiguas y estaban abandonadas. Finalmente, se decidió apostar por usar una opción intermedia. 
\\

Finalmente, se usará una BBDD relacional (PostgreSQL), con un mapper objeto-relacional (Debea). Aunque no se disponga de las ventajas de tener una base de datos directamente implementada como Orientada a Objetos, Postgre es actualmente consideraba la mejor opción libre para Bases de Datos Relacionales: potente, veloz y con un buen soporte de transacciones. Por otro lado, con el mapper objeto-relacional Debea se conservarán los beneficios para el desarrollo al eliminar la traducción del modelo OO al modelo relacional y simplificar la capa de persistencia. De esta forma se relega el problema del desajuste por impedancia al mapper. Con esto se espera reducir significativamente el tiempo de desarrollo y mantener un modelo fácil de mantener y flexible frente a cambios futuros.

\section{Análisis}
\subsection{Análisis de Requisitos de Usuario}

El subconjunto de funcionalidades añadidas se centran en la gestión de usuarios y de escenas con sus Entidades, además del equema de persistencia y la configuración de la aplicación. 
\\

En el apartado de Gestión, la aplicación permitirá dar de alta nuevos usuarios, crear o cargar entornos con entidades asociadas y editar sus propiedades. 
\\

De esta forma se permite la creación y borrado de usuarios, la modificación de permisos y que éste haga login en la aplicación. Un usuario registrado puede crear nuevos escenarios, modificar sus permisos, cargarlos o eliminarlos del sistema. Finalmente, esta información será guardada y cargada en futuras sesiones siguiendo el esquema de persistencia discutido. Sin embargo, las escenas y elementos serán objetos de prueba generados manualmente. Tampoco existirá interacción en esta fase.
\\

En relación a la configuración de la aplicación se permitirá guardar datos relacionados a la configuración del sistema instalado en el equipo. Se utilizará para obtener directorios de uso común por los distintos módulos. También se usará para guardar y recuperar la configuración de las cámaras (número, orientación), así como de las ventanas de render (número, resolución, orientación). Se incluye de la misma forma una ventana de log donde los distintos módulos puedan volcar información útil.

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/casosdeuso_segundademo.png}
\end{center}
\caption{ \label{F_Casos_de_Uso_Etapa3} Diagrama de Casos de Uso. Usuario, Entorno, Configuración y Persistencia}
\end{figure}


\subsection{Selección de Herramientas}

A partir del estudio de las herramientas disponibles se han seleccionado las más adecuadas, tanto por sus características como por compatibilidades técnicas en el conjunto del proyecto.

\begin{itemize}
\item BBDD: PostgreSQL
\item Mapper O-R: Debea
\end{itemize}

\section{Diseño}
\subsection{Diseño de la Aplicación}

Se añaden elementos al módulo IAplication para manejar los conceptos de ususario y proyecto, y se describe el módulo IPersistence. 

\begin{itemize}
\item Core: Conjunto de interfaces de la aplicación.
\begin{itemize}
\item IAplicationConfiguration: Se añade interfaz para la configuración de la aplicación.
\item IPersistence: Interfaz de la capa de Persistencia para el guardado y recuperación de los datos de la aplicación.
\begin{itemize}
\item IUserPersistence: Interfaz que refleja del modelo de datos y persistencia del objeto Usuario. 
\item IWorldPersistence: Interfaz que refleja del modelo de datos y persistencia del objeto Mundo (escenario). 
\item IEntityPersistence: Interfaz que refleja del modelo de datos y persistencia del objeto Entidad. Una entidad es una elemento contenido en un escenario.
\end{itemize}
\end{itemize}
\item igui: Interfaz básica para la creación del módulo y registro de ventanas. Modificaciones para soporte de opciones de configuración. Login, inicio y cierre de sesión de usuarios.
\begin{itemize}
\item GUIConfiguration: Paneles de Configuración de la aplicación.
\item GUILogPanel: Panel de log donde se muestra información de la aplicación de los módulos que vuelquen datos en él.
\item GUIUser: Panel de login y de creación de nuevos usuarios.
\item GUIUserInfo: Panel de gestión de usuarios y creación y gestión de escenarios.
\end{itemize}
\item ipersistence: Módulo de persistencia
\begin{itemize}
\item EntityPersistence: Refleja el modelo de datos y la persistencia del objeto Entidad.
\item WorldPersistence: Refleja el modelo de datos y la persistencia del objeto Mundo.
\item UserPersistence: Refleja el modelo de datos y la persistencia del objeto Usuario.
\end{itemize}
\item iprod: Módulo de producción
\begin{itemize}
\item Prod3DEntity: Encapsula el modelo de datos del objeto Entidad y gestiona las características particulares relativas al módulo de producción.
\end{itemize}
\item vox: 
\begin{itemize}
\item ApplicationConfiguration: Implementación de los mecanismos para guardar y cargar las opciones de configuración de la aplicación.
\end{itemize}
\end{itemize}

\subsubsection{Breve descripción de los módulos}
Los cambios principales se realizan en los módulos de Aplicación, GUI y Persistencia.\\

En el primero se introducen los controladores de sesión y de configuración y la API necesaria para su uso desde otros módulos. En GUI, se añade un controlador genérico y las interfaces gráficas necesarias para la configuración del sistema y para el inicio de sesión en la misma por parte de usuario. Finalmente, en el módulo de Persistencia se modelan los objetos a conservar y los mecanismos para poder guardarlos y recuperarlos en la base de datos. \\

Por otro lado, tanto el módulo de persistencia como el de GUI, así como el de percepción y producción reciben ajustes para poder usar las opciones de configuración. \\

Como modelo, recordar que cada módulo es responsable de las tareas a su cargo y las comunicaciones se establecen a través de los módulos principales. Ellos disponen de todos los mecanismos necesarios para realizarlas, delegando en su caso en controladores, y ofreciendo una API sencilla de usar. I.e., cuando el usuario introduce los datos de inicio de sesión el controlador de GUI recibe los datos desde el panel de la interfaz, que termina accediendo al módulo principal de GUI para que llame al módulo principal de aplicación, que es el responsable de gestionar la sesión. En este punto, la aplicación hace uso del controlador de sesión que terminará accediendo al módulo de persistencia para recuperar los datos.
\newpage

\subsubsection{Diseño General en UML}
\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/DiagramadeclasesUML_SegundaDemo.png}
\end{center}
\caption{ \label{F_DiagramadeClasesUML_SegundaDemo} Diagrama de Clases UML, Segunda Demo. Resumen.}
\end{figure}

En este momento se aborda de lleno el módulo de persistencia, añadiendo los elementos Usuario, Mundo (Escenario) y Entidad. Este módulo será accedido para cargar o guardar los datos que se desean conservar. Por otro lado, también se da la posibilidad de configurar elementos de la aplicación, como pueden ser características relacionadas a los dispositivos de entrada o salida del sistema. Además se añade la capacidad de hacer login en la aplicación para cargar los mundos que tenga asociados un usuario, crear escenarios nuevos o editar propiedades del mismo.\\
 
Por simplicidad, en el diagrama se muestran sólo los detalles principales. Para ver los detalles se puede consultar las secciones siguientes.

\subsubsection{Diseño en UML - Núcleo de la Interfaz}

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/DiagramadeclasesUML_SegundaDemo_core.png}
\end{center}
\caption{ \label{F_DiagramadeClasesUML_SegundaDemo_Core} Diagrama de Clases UML, Segunda Demo. Núcleo de la Interfaz.}
\end{figure}

Se introducen cambios de relevancia en la mayoría de las clases, especialmente debido al uso de la interfaz de configuración que usarán los módulos. También cabe destacar el desarrollo de la interfaz de Persistencia que usarán aquéllos módulos que requieran acceder o guardar datos que se deseen conservar.\\

IApplicationConfiguration está destinada a sevir de interfaz para conservar datos de directorios de relevancia, como pueden ser el de datos genéricos, recursos de la aplicación, modelos 3D, sonidos o imágenes, lenguaje, datos de conexión con el dispositivo de almacenamiento, el número de cámaras, el número de displays, además de la configuración particular de los mismos (localización en el espacio de la instalación, orientación y resolución).\\

Por otro lado tenemos las interfaces de persistencia. Es interesante recordar que se tiene como objetivo independizar los detalles de implementación de los módulos entre sí, de forma que sean fácilmente sustituíbles por otros nuevos, sin necesidad de realizar modificaciones en los demás. También se ha optado por un diseño de persistencia en el que existe una correspondencia directa entre el modelo OO y el modelo de datos a persistir. Por ello el modelo fundamental de los datos recae en el mismo módulo de persistencia. Como puede verse, es un esquema bastante simple en el que existen tres objetos: usuarios, mundos y entidades. \\

Los usuarios tienen los datos clásicos de conexión, para hacer login en el sistema pero también datos relativos a su experiencia dentro del mismo, como pueden ser desde la localización o permisos, hasta el término nombrado como psique. Psique es una codificación de la conducta y actividades que realiza el usuario en el sistema que será abordada más adelante. Para más detalles al respecto, consultar sección ?? página ??.\\

Las entidades son los elementos componentes de un mundo. Hasta este punto sólo se ha tenido en cuenta que sean elementos 3D posicionados en el espacio. Estas entidades también actúan y sufren experiencias que se conservará en la codificación psique, que a su vez influirá en sus acciones siguientes.\\

Finalmente, los mundos son espacios creados por un usuario y que contendrán colecciones de entidades conservando una visión global de la experiencia en su atributo psique. Como es razonable desear que un escenario permanezca inalterable, o poder acceder sólo a determinadas opciones en él, el usuario puede definir unos permisos sobre cada mundo. 
\newpage

\subsubsection{Diseño en UML - Módulo de Aplicación}

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/DiagramadeclasesUML_SegundaDemo_vox.png}
\end{center}
\caption{ \label{F_DiagramadeClasesUML_SegundaDemo_Vox} Diagrama de Clases UML, Segunda Demo. Módulo Principal de la Aplicación.}
\end{figure}

El módulo principal de la aplicación es el responsable de todas las acciones fundamentales del mismo. Es el responsable de instanciar y gestionar los módulos necesarios y suministrar métodos para la manipulación de la aplicación. \\

De esta forma, es ahora también responsable de la configuración de la aplicación, que realiza mediante el controlador ConfigurationController, así como de la sesión mediante SessionController.\\

SessionController da capacidades para cargar los datos de un usuario y permitir su entrada en el sistema, así como su salida, además de capacidades para saber cuál es el usuario y escenario actuales, o si un determinado usuario o escenario es el actual. Se entiende como sesión el estado en el cual un usuario que accede al sistema carga de un escenario para comenzar el uso del mismo. Esto es, se establece una sesión cuando se asigna un usuario y un escenario en el sistema.\\

Por su lado, ConfigurationController es usado fundamentalmente al comienzo y final de la aplicación para cargar y guardar datos relativos al funcionamiento de la misma. Por ello, sus métodos principales son Load y Save.

\subsubsection{Diseño en UML - Módulo de Persistencia}

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/DiagramadeclasesUML_SegundaDemo_ipersistence.png}
\end{center}
\caption{ \label{F_DiagramadeClasesUML_SegundaDemo_IPersistence} Diagrama de Clases UML, Segunda Demo. Módulo de Persistencia.}
\end{figure}

En este módulo se implementan las interfaces definidas referentes a persistencia en el núcleo de la interfaz y son un reflejo del mismo adaptado a la implementación final. Como se ha comentado se ha optado por fusionar los conceptos del modelo de objetos y modelo de persistencia. Siendo así la gestión de la persistencia de un objeto recae sobre él mismo (pe. para crear un objeto basta instanciarlo, y para guardar sus datos basta con llamar a su método 'Save'). Esta fusión podrá verse en mayor detalle en la sección de Implementación.\\

Como con el resto de módulos independientes existe un núcleo principal que será el encargado de iniciar el dispositivo de almacenamiento y de atender peticiones de otros módulos. Estas peticiones son consultas que pueden hacerse sin necesidad de instanciar el objeto, por ejemplo, para saber si un usuario determinado o un mundo existe u obtener la lista de mundos que un usuario puede ver. 

\subsubsection{Diseño en UML - Módulo de GUI}

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/DiagramadeclasesUML_SegundaDemo_igui.png}
\end{center}
\caption{ \label{F_DiagramadeClasesUML_SegundaDemo_IGUI} Diagrama de Clases UML, Segunda Demo. Módulo de Persistencia.}
\end{figure}

Una de las primeras consecuencias lógicas con la entrada de la capacidad para gestionar la configuración de la aplicación es su impacto sobre la interfaz gráfica de usuario. Este impacto se debe, por un lado, a la necesidad de localizar recursos estéticos como las imágenes de los botones, siendo el caso más común y básicamente el que sucede sobre las clases previamente existentes. Por otro lado, es también a través de la interfaz de usuario como éste va a editar los parámetros de configuración del sistema, mediante el panel de configuración. Además puede verse cambios relativos al uso del sistema, como son los paneles de login, de gestión de la información del usuario, y de log del sistema.\\

De esta forma, tenemos por un lado el panel GUIConfiguration. Este panel permitirá al usuario cambiar la cantidad de cámaras de las que se alimentará el módulo de Percepción, así como los parámetros de las mismas. De igual forma, podrá establecer la cantidad de displays que se mostrarán y su configuración, administrando vistas independientes de la escena. Gracias a esta posibilidad se dota de una gran flexibilidad al sistema para establecer modelos de instalaciones distintos: posibilidad de vision estéreo, frontal-cenital, etc. o espacios de escritorio, proyecciones frontales o sistemas Cave que envuelven al usuario con pantallas.\\

Por otro lado, tenemos el panel GUIUser, desde el cual un usuario podrá hacer login en el sistema o registrarse en el mismo como nuevo usuario. Este panel está estrechamente relacionado con GUIUSerInfo, que muestra al usuario actual la lista de mundos que ha creado y donde le permite modificar sus atributos, así como crear nuevos usuarios, destruirlos o borrarse él mismo del sistema.\\

Finalmente, se ha añadido un panel de log en el cual se mostrará información de interés del funcionamiento del sistema.\\

Hay que tener en cuenta que estas interfaces son sólo componentes que muestran y capturan información. No contienen ninguna lógica relevante de aplicación. Para estas tareas se hace uso, como es lógico, de un controlador. Sin embargo, las necesidades son mínimas y en su mayor parte este controlador sólo derivará las acciones hacia sus responsables. Por ello y por simplicidad, se ha decidido usar un controlador genérico para todo el módulo. Este controlador responderá a las necesidades de gestión de los paneles, para mostrar unos u otros. Pero a su vez, también encauzará las peticiones del usuario al interactuar con la interfaz, hacia el responsable de su gestión (pe. para crear un usuario nuevo o cargar un escenario).

\subsubsection{Diseño en UML - Módulo de Producción}

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/DiagramadeclasesUML_SegundaDemo_iprod.png}
\end{center}
\caption{ \label{F_DiagramadeClasesUML_SegundaDemo_IProd} Diagrama de Clases UML, Segunda Demo. Módulo de Producción.}
\end{figure}

Durante esta fase los cambios en el Módulo de Producción son moderados. Por lado, usará la información de los parámetros de configuración para los displays que mostrará; por otro, se le ha añadido capacidad para limpiar y cargar datos en la escena. Estos datos serán los relativos al mundo que el usuario desee cargar.\\

De esta forma, destacan los métodos RunWorld() y CloseWorld(), que cargan un escenario y libera la escena respectivamente. También LoadEntityIntoScene() que carga una entidad persistida dentro de la escena, o LoadDefaultScene() que carga una escena sencilla por defecto, que se usará con carácter estético cuando no hay cargado ningún escenario.\\ 

Por otro lado, se puede ver que aparece una nueva clase llamada Prod3DEntity. Esta nueva clase representa las entidades 3D cargadas en la escena, que efectivamente se corresponderán con las Entidades persistidas que forman un escenario. Prod3Dentity encapsula los datos de EntityPersistence y añade información concreta dependiente de la implementación, relacioanda con el motor gráfico. En este punto, esta información es concretamente el nodo del grafo de la escena.

\subsubsection{Diseño en UML - Módulo de Percepción}

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/DiagramadeclasesUML_SegundaDemo_ipercept.png}
\end{center}
\caption{ \label{F_DiagramadeClasesUML_SegundaDemo_IPercept} Diagrama de Clases UML, Segunda Demo. Módulo de Percepción.}
\end{figure}

Al igual que en el caso del módulo de Producción, este módulo sufre pocos cambios. En realidad incluso menos, ya que en esta fase del desarrollo no se abordarán funcionalidades nuevas en la percepción. Sin embargo, el módulo sí se ve afectado por los parámetros de configuración de la aplicación, permitiendo establecer el número de cámaras desde las que se capturan imágenes y los parámetros de las mismas.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Implementación}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Los módulos afectados en la presente fase son los siguiente:

\begin{itemize}
\item core: Añadida interfaz de configuración de aplicación IApplicationConfiguration.
\item igui: añadidas interfaces gráficas para la gestión de usuarios, escenas y paneles de configuración de la aplicación.
\item ipercept: Configuración del módulo.
\item ipersistence: Persistencia de los objetos usuario, mundo y entidad.
\item iprod: Configuración del módulo.
\item vox: Controlador de la configuración de la Aplicación.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Núcleo de la Interfaz}
$\backslash$src$\backslash$core
\begin{itemize}
\item IApplicationConfiguration: Interfaz destinada a la configuración de la aplicación y que puede ser usada por parte de los módulos con independencia de la implementación final. 
\\
Algunas características son, sin embargo, inherentes a la naturaleza de los módulos y los dispositivos genéricos. Hablamos del caso de las propiedades clásicas de los dispositivos de entrada como son las cámaras y de salida como las ventanas de visualización. Algunas de estas opciones son la resolución y orientación de los mismos. De esta forma, se añaden las clases CameraData y DisplayData.
%\\

\begin{lstlisting}[language=C++]
class DisplayData
{ public:
   DisplayData() : x(0),y(0),z(0),flip_h(false),flip_v(false), resolution_x(800),resolution_y(600){}
   ~DisplayData() {}
   double x, y, z;
   bool flip_h, flip_v;
   unsigned int resolution_x, resolution_y; };
\end{lstlisting}

Finalmente, se añaden los métodos a utilizar para acceder y modificar las opciones de configuración de la aplicación. Se muestran algunas de las principales. Para ver más detalles consultar la sección \ref{detallesImp:InterfacesCore}, página \pageref{detallesImp:InterfacesCore}.  

\begin{lstlisting}[language=C++]
class IApplicationConfiguration
{ public:
   virtual unsigned int GetNumCams()     = 0;
   virtual unsigned int GetNumDisplays() = 0;   
   virtual core::CameraData  GetCameraData(const int &id)=0;
   virtual core::DisplayData GetDisplayData(const int &id)=0;	
   virtual std::string GetSDHost() = 0;
   virtual std::string GetSDPort() = 0;
   virtual std::string GetSDPassword()          = 0;
   virtual std::string GetUIResourceDirectory() = 0;			
\end{lstlisting}

\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Aplicación Principal}
$\backslash$apps$\backslash$vox

Se añade la clase ApplicationConfiguration que representa el modelo de datos de la configuración del sistema, y los controladores ConfigurationController y SessionController. Se describen a continuación:

\begin{itemize}
\item $\backslash$apps$\backslash$ApplicationConfiguration: Implementa la interfaz IApplicationConfiguration que peude verse en el apartado anterior, disponiendo de los mismo métodos para obtener y asignar valores.
\item $\backslash$apps$\backslash$controllers$\backslash$ConfigurationController: Controllador destinado a la gestión de la configuración. Carga y guarda los parámetros mediate los métodos Load() y Save() de forma estructurada en un archivo de configuración en texto plano llamado config.ini. Como puede verse, para esta tarea se buscan las etiquetas asociadas a cada atributo y toma o guarda directamente su valor. 
\begin{lstlisting}[language=C++]
bool ConfigurationController::Load()
{   if ( config_file.is_open() )
    { while (!config_file.eof())
      { if ( tag == "[DATA_DIRECTORY]" )
        { config_file.getline(str, size);
          app_config->SetDataDirectory(str); }
\end{lstlisting}
Hay que tener en cuenta que la mayoría de los datos son atómicos, es decir, un atributo está asociado sólo con un único valor indivisible. Sin embargo, existen atributos compuestos de múltiples valores que requieren parsing para su lectura y escritura. Este es el caso de la configuración particular de las distintas cámaras y displays. Para ellos se usan etiquetas a modo de separadores para distinguir cada dispositivo y sus atributos.
\begin{lstlisting}[language=c++]
std::string ConfigurationController::ParseDisplayConfig()
{ ...
  unsigned int win_num = app_config->GetNumDisplays();
  for (unsigned int i = 1; i <= win_num; i++)
  { core::DisplayData data = app_config->GetDisplayData(i);
    wop << "::ID::" << i 
    << "::X::" << data.x << "::Y::" << data.y << "::Z::" << data.z 
    << "::FLIPH::" << data.flip_h << "::FLIPV::" << data.flip_v
    << "::RES_X::" << data.resolution_x 
    << "::RES_Y::" << data.resolution_y;	}
  ... }
\end{lstlisting}
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Módulo de Persistencia}\label{Etapa3detallesImp:ModuloPersistencia}
$\backslash$src$\backslash$ipersistence\\

La clase principal del Módulo de Persistencia y que implementa la interfaz IPersistence es MainPersistence. Esta clase se encarga de la inicialización del módulo y del dispositivo de almacenamiento. La verdadera intención era hacer uso de una libería que permitiera hacer uso de un esquema de persistencia Orientado a Objetos. Sin embargo, no fue posible encontrar una que cumpliera los requisitos software de la aplicación. Por ello, se decidió adoptar una solución intermedia. De esta forma se decide utilizar una base de datos relacional, PostGreSQL, y un mapper objeto-relacional, Debea. Para ver los detalles de implementación necesarios para configurar el módulo con estas librerías se puede consultar la sección ?? página ??. De esta forma, el mapper permite fusionar el modelo de objeto y el modelo de persistencia en uno. Sin embargo, es interesante saber que, en el fondo, al tratarse el dispositivo de almacenamiento de una base de datos SQL, también se permiten realizar consultas de forma manual.

\begin{itemize}
\item $\backslash$apps$\backslash$MainPersistence: Inicializa la conexión con la base de datos y en caso de no existir contenido, es la responsable de crear las tablas de las entidades e introducir datos por defecto. Para crear las tablas, solicita los esquemas de traducción a las clases de los objetos a persistir. 
\begin{lstlisting}[language=C++]
MainPersistence::MainPersistence(IApplicationConfiguration *app_cfg)
{ std::string db_name = app_cfg->GetSDName();
  std::string db_user = app_cfg->GetSDUser();
  std::string db_passwd = app_cfg->GetSDPassword();
  std::stringstream connect;
  connect << "dbname=" << db_name << " user=" 
          << db_user << " password=" << db_passwd;
  ar.setIdFetcher(new dba::GenericFetcher()); 
  unlink(db_name.c_str());
  ar.open("dbapgsql-static", connect.str().c_str());
  //create needed tables
  ar.getOStream().sendUpdate(counter_create);
  ar.getOStream().sendUpdate((*(UserPersistence::GetSchema())));
  ar.getOStream().sendUpdate((*(EntityPersistence::GetSchema())));
  ar.getOStream().sendUpdate((*(WorldPersistence::GetSchema())));
  //first id //
  ar.getOStream().sendUpdate(dba::SQL("INSERT INTO debea_object_count VALUES (:d)") << 1);
...
\end{lstlisting}
Los modelos de los objetos están ligados directamente a su persistencia, por lo que MainPersistence no tiene responsabilidad explícita sobre la gestión particular de cada uno de ellos. Sin embargo, dispone de mecanismos para consultar la base de datos y obtener información de interés, como puede ser consultar si un usuario o un mundo existe, o recuperar la lista de mundos de los que un usuario es propietario. También permite borrar un escenario o un usuario, sin necesidad de cargarlo en el sistema, o modificar las propiedades de un escenario.\\

\item $\backslash$apps$\backslash$UserPersistence: Representa al objeto usuario y contiene sus datos y métodos para guardarlos y recuperarlos. Como puede verse, la clase hereda de dba::Storeable lo que permitirá hacer uso de los mecanismos de persistencia. Aunque el mapper tiene una API suficiente posee algunas carencias. Por un lado es necesario establecer el esquema de la clase (definición de la tabla en la base de datos) y la relación de las variables con las mismas. 
\begin{lstlisting}[language=C++]
//::MAPPING:: Class name, parent class name and relation name
BEGIN_STORE_TABLE(UserPersistence, dba::Storeable, "user_table")
 BIND_STR(UserPersistence::name,     dba::String, "name"    )
 BIND_STR(UserPersistence::password, dba::String, "password")
 BIND_INT(UserPersistence::psique,   dba::Int,    "psique"  )
 ...
END_STORE_TABLE()
//SQL schema
dba::SQL UserPersistence::schema(
"CREATE TABLE user_table ("
"  id INT PRIMARY KEY,"
"  name VARCHAR,"
"  password VARCHAR,"
"  psique INT,"
...
\end{lstlisting}
Una vez mapeado se pueden crear instancias de la clase que ya tienen capacidad directa de persistencia. Sin embargo, se desea una abstracción completa de la implementación de estos mecanismos, además de un mejor diseño para ofrecer una mejor interfaz de uso. Para ello, se encapsulan los métodos básicos de carga, almacenamiento y borrado en los métodos Load(), Save() y Delete().
\begin{lstlisting}[language=C++]
bool UserPersistence::Load(const int &id)
{ try 
  { boost::mutex::scoped_lock lock(m_mutex);
    UserPersistence new_object;
    dba::SQLIStream istream = ar->getIStream();
    istream.setWhereId(id);
    bool success = istream.get(&new_object);
    this->operator =(new_object);
    return success; } 
  catch (const dba::SQLException& pEx){ ProcessException(pEx); }
  catch (const dba::Exception& pEx)   { ProcessException(pEx); }
  return false;
} 
void UserPersistence::Save()
{ try 
  { boost::mutex::scoped_lock lock(m_mutex);
    ar->getOStream().put(this);	} 
  catch (const dba::SQLException& pEx){ ProcessException(pEx); }
  catch (const dba::Exception& pEx)   { ProcessException(pEx); }
} 
void UserPersistence::Delete()
{ try 
  { boost::mutex::scoped_lock lock(m_mutex);
    this->setState(dba::Storeable::stState(DELETED));
    ar->getOStream().put(this); } 
  catch (const dba::SQLException& pEx){ ProcessException(pEx); }
  catch (const dba::Exception& pEx)   { ProcessException(pEx); }
}
\end{lstlisting}
Hay que tener en cuenta que los objetos puedes ser accedidos desde distintos módulos que se ejecutan en hilos en paralelo, por lo que es necesario el uso de cerrojos para sincronizar los accesos. Se utiliza para ello los cerrojos de ámbito que ofrece boost.\\

\item $\backslash$apps$\backslash$EntityPersistence: Es la clase que implementa la interfaz IEntityPersistence que modela el objeto Entidad. Una entidad es un elemento básico de los cuales se compone una escena. Su implementación sigue exactamente el mismo esquema que el caso anterior: un mapeo de variables, una definición de esquema, y métodos que encapsulan la API de Debea para las operaciones de persistencia.\\

Otro detalle a tener en cuenta es la necesidad de un filtro para mapear las variables de tipo Float o Double correctamente, ya que la librería demostraba problemas en sistemas de localización no inglesa, debido a los separadores usados como punto decimal. Estudiado el problema se corrigió y se añadió un filtro adecuado vFloat. Los cambios fueron remitidos y pendientes de admisión en revisión.\\

Finalmente, para reflejar la relación de pertenencia que tienen los escenarios con las entidades, es necesario incluir en el esquema de la tabla de la base de datos una entrada que es la clave ajena de mundo al que pertence.

\begin{lstlisting}[language=C++]
//::MAPPING:: Class name, parent class name and relation name
BEGIN_STORE_TABLE(EntityPersistence, dba::Storeable, "entity_table")
 BIND_STR(EntityPersistence::name,       dba::String, "name"       )
 BIND_FLT(EntityPersistence::position_x, dba::vFloat, "position_x" )
END_STORE_TABLE()
//SQL schema
dba::SQL EntityPersistence::schema(
"CREATE TABLE entity_table ("
"  id INT PRIMARY KEY,"
"  name VARCHAR,"
"  position_x FLOAT,"
"  fk_world INT",
...
\end{lstlisting}

\item $\backslash$apps$\backslash$WorldPersistence: El último modelo de objeto a persistir es el Mundo o escenario. Simplemente define una colección de Entidades y algunos atributos como nombre, permisos o el usuario que creó dicho mundo.\\

Para mapear la lista de entidades que contiene un Mundo se usa la macro BIND\_COL, que establece una relación de cada elemento con la tabla correspondiente y que implica la necesidad de que exista una clave ajena en el esquema de la otra clase.\\

Puede apreciarse que existe una relación similar entre Usuario y Mundo, pero en ese caso no se lleva a cabo por dos motivos: Primero, porque conceptualmente el objeto usuario no se compone de Mundos, y segundo porque, aunque en el desarrollo de este proyecto un usuario sólo puede acceder a los mundos que ha creado, se mantiene una perspectiva abierta para trabajo futuro en la que otros usuarios conectados puedan acceder o compartir cualquier mundo, siendo sus acciones limitadas por los permisos que el creador definiera para el mismo.
\begin{lstlisting}[language=C++]
//::MAPPING:: Class name, parent class name and relation name
BEGIN_STORE_TABLE(WorldPersistence, dba::Storeable, "world_table")
 BIND_STR (WorldPersistence::name,        dba::String, "name")
 BIND_STR (WorldPersistence::owner,       dba::String, "owner")
 BIND_INT (WorldPersistence::permissions, dba::Int,    "permissions")
 BIND_COL (WorldPersistence::entities,    
           dba::stdList<EntityPersistence>, "fk_world")
END_STORE_TABLE()
//SQL schema
dba::SQL WorldPersistence::schema(
"CREATE TABLE world_table ("
"  id INT PRIMARY KEY,"
"  name VARCHAR,"
"  owner VARCHAR,"
"  permissions INT"
\end{lstlisting}

\end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Módulo de GUI}
$\backslash$src$\backslash$igui

\begin{itemize}
\item $\backslash$src$\backslash$igui$\backslash$controllers$\backslash$GUIGenericController: Dado que la lógica asociada al módulo de GUI es simple y con pocas opciones, que además son comunes, se ha incluido un controlador genérico. Este controlador es utilizado para alternar entre la visualización de distintos paneles de GUI, así como para gestionar los accesos a funcionalidades del núcleo principal del módulo, maingui. 

\begin{lstlisting}[language=C++]
class GUIGenericController
{public:
  static GUIGenericController* GetInstance() {return instance;}
  bool CreateUser(const string &name,const string &passwd);
  bool LoginUser(const string &name,const string &passwd);
  bool DeleteUser(const string &name);
  bool DeleteWorld(const string &name);
  void LogOut();
  void ViewUserInfoPanel();
  void ViewLogPanel();
  void ViewConfigurePanel();
  private:
  static GUIGenericController *instance; 
  ... };
\end{lstlisting}

\item $\backslash$src$\backslash$igui$\backslash$GUIUser: Interfaz destinada al acceso de los usuarios, mediante nombre y contraseña, y para la creación de nuevas cuentas de usuario. La lógica relacionada al login y a la creación de nuevos usuarios se realiza a través del controlador GUIGenericController. Sin embargo, si las acciones no son responsabilidad de este módulo lo único que hace GUIGenericController es derivarlas al responsable.

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/gui_user.png}
\end{center}
\caption{ \label{F_CapturaFase3SegundaDemo_GUIUSer} GUI: User login}
\end{figure}

El código de la interfaz es simple y común a las mostradas anteriormente. De forma ilustrativa se muestra el caso de la operación de login. Primero, Se recuperan los valores introducidos por el usuario en los controles, luego se usa el controlador, que relegará la consulta. En caso de no tener éxito la petición se muestran los mensajes de error.

\begin{lstlisting}[language=C++]
BEGIN_EVENT_TABLE(GUIUser, wxPanel)
   EVT_PAINT   (                 OnPaint         )
   EVT_BUTTON  ( wxID_LOGIN    , OnLoginButton   )
   EVT_BUTTON  ( wxID_REGISTER , OnNewUserButton )
END_EVENT_TABLE()

void GUIUser::OnLoginButton(wxCommandEvent& WXUNUSED(event))
{ s_user_name = user_name->GetValue();
  s_user_password = user_passwd->GetValue();
  DoLogin(s_user_name, s_user_password);   }

void GUIUser::DoLogin(const string &name, const string &passwd)
{ GUIGenericController *guiGc = GUIGenericController::GetInstance();
  if (guiGc != NULL) login = guiGc->LoginUser(name, passwd);
  if (login)
  { user_logged_in = true;
    guiGc->ViewUserInfoPanel(); }
  else
  { user_logged_in = false;
    wxMessageDialog message_dialog(this, _("User or password incorrect"), "Message box", wxOK \| wxICON_EXCLAMATION \| wxSTAY_ON_TOP);
    message_dialog.ShowModal(); } }
\end{lstlisting}

\item $\backslash$src$\backslash$igui$\backslash$GUIUserInfo: Interfaz destinada a la gestión del usuario y de sus datos. Desde este panel el usuario puede ver la lista de Entornos que tiene, crear nuevos entornos, borrarlos, modificar sus permisos o lanzarlos para su ejecución en la sesión. Estas acciones se realizan a través del controlador GUIGenericController. 
\\

Como se comentaba en el punto anterior, no es el controlador GUIGenericController el que realiza directamente las acciones de login en el sistema, y tampoco el núcleo principal del módulo de gui. De hecho, sólo derivan las peticiones a los módulos responsables. En este caso, la responsabilidad de gestionar la sesión, así como todas las acciones sobre el sistema caen sobre el módulo principal de la Aplicación, que en este caso, a su vez, relegará en en el controlador de la sesión SessionController, que hará uso finalmente del módulo de Persistencia. Para más detalles a este respecto puede verse ??

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/gui_userinfo.png}
\end{center}
\caption{ \label{F_CapturaFase3SegundaDemo_GUIUserInfo} GUI: User info}
\end{figure}

\item $\backslash$src$\backslash$igui$\backslash$GUIConfiguration: Este panel de la interfaz está destinado a la configuración de la aplicación. En el puede establecerse el número de cámaras que se van a conectar y el número de ventantas de render que se quieren ver. Además por cada cámara o display se pueden establecer parámetros de configuración, como puede ser su orientación o resolución. Debido a limitaciones de las librerías y a la relevancia de algunos cambios, algunos de éstos requieren el reinicio de la aplicación y de tal forma es notificado al usuario. Por otro lado, hay que tener en cuenta que algunos parámetros, como el calibrado posicional de las cámaras en el espacio, no son usados todavía aunque ya se almacenan y se permite su gestión.

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/gui_configure.png}
\end{center}
\caption{ \label{F_CapturaFase3SegundaDemo_GUIConfiguration} GUI: Panel de configuración}
\end{figure}

\item $\backslash$src$\backslash$igui$\backslash$GUILogPanel: La última de las interfaces añadidas muestra una ventana donde aparecerán mensajes que puedan resultar de interés enviados desde cualquier módulo de la aplicación. No pretende ser un log exhaustivo del funcionamiento de la aplicación sino una forma de ofrecer información a modo de curiosidad. En el gráfico pueden verse mensajes de incialización, así como la entrada y salida de usuarios en el sistema.

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/gui_logpanel.png}
\end{center}
\caption{ \label{F_CapturaFase3SegundaDemo_GUILogPanel} GUI: Panel de log}
\end{figure}
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Módulo de Producción}
$\backslash$src$\backslash$iprod
En esta fase el módulo de Producción sufre pocos cambios. Se añade la clase Prod3DEntity que encapsula el objeto entidad añadiendo particularidades propias del motor gráfico y métodos a MainProd que permiten cargar y limpiar la escena.
\begin{itemize}
\item $\backslash$src$\backslash$iprod$\backslash$Prod3DEntity: Como se ha comentado encapsula el objeto EntityPersistence y añade particularidades relacionadas con la implementación del módulo de aplicación. En este caso se añade un puntero al nodo del grafo de la escena que se corresponde con el objeto 3D asociando.
\begin{lstlisting}[language=C++]
class Prod3DEntity
{ public:
  Prod3DEntity(core::IEntityPersistence* ent);
  virtual ~Prod3DEntity();
  std::string GetData()               { return data;     }
  NodePath*	GetNodePath()             { return nodepath; }
  core::IEntityPersistence* GetEntity(){return entity;   }
  void SetData(const std::string &value){data = value;   }
  void SetNodePath(NodePath *value);
 private:
  std::string data;
  core::IEntityPersistence* entity;
  NodePath *nodepath; };
  
Prod3DEntity::Prod3DEntity(core::IEntityPersistence* ent)
{ if (entity != NULL ) data = entity->GetModelData(); }

void Prod3DEntity::SetNodePath(NodePath *value)		
{ nodepath = value; 
  if ( (entity != NULL) && (nodepath != NULL) )
  { float posx, posy, posz, rotx, roty, rotz, scale;
    entity->GetPosition(posx, posy, posz);
    entity->GetOrientation(rotx, roty, rotz);
    entity->GetScale(scale);
    nodepath->set_pos(posx,posy,posz);
    nodepath->set_hpr(rotx, roty, rotz);
    nodepath->set_scale(scale,scale,scale);	} }  
\end{lstlisting}

\item $\backslash$src$\backslash$iprod$\backslash$MainProd: Durante esta fase el módulo de Producción sufre algunos cambios leves, aunque ha tenido que hacerse frente a ciertas dificultades del motor gráfico. Se hace uso de la configuración de la aplicación para mostrar las ventanas de render y se dota de capacidad para cargar y cerrar escenarios. Por ello, cabe destacar los métodos RunWorld(), que lee los datos de un objeto Mundo, extrae sus entidades y las carga en escena, y CloseWorld(), que vacía la escena y limpia los datos internos.

\begin{lstlisting}[language=C++]
void MainProd::CloseWorld()
{ ClearScene();
  { boost::mutex::scoped_lock lock(m_mutex);
    current_user  = NULL;
    current_world = NULL;
    for (unsigned int i = 0; i < scene_entities.size(); i++)
     delete scene_entities[i];
    scene_entities.clear(); } }
bool MainProd::RunWorld(core::IUserPersistence *user, core::IWorldPersistence *world)
{ boost::mutex::scoped_lock lock(m_mutex);
  current_user  = user;
  current_world = world;	
  for (int i=0; i < current_world->GetNumEntities(); i++)
  { IEntityPersistence *ient = current_world->GetEntity(i);
    Prod3DEntity *new_entity = new Prod3DEntity(ient);
    scene_entities.push_back(new_entity);
    LoadEntityIntoScene(new_entity); }
  initialized = true;
\end{lstlisting}

Dentro de RunWorld es usado el método LoadEntityIntoScene() que es usado para cargar las entidades leídas en la escena 3D. Se recuerda que por motivos de eficiencia el modelo 3D leído es cargado sólo en el primer framework, correspondiente a la primera vista, y posteriormente se instancian en el resto.

\begin{lstlisting}[language=C++]
void MainProd::LoadEntityIntoScene(Prod3DEntity * entity)
{ std::string data = entity->GetData();
  scene_nodepaths[entity] = pandawindows_array[1]->load_model(framework.get_models(),data);
  entity->SetNodePath(&(scene_nodepaths[entity]));
  scene_nodepaths[entity].reparent_to(pandawindows_array[1]->get_render());
  std::map<int, WindowFramework*>::iterator iter = pandawindows_array.begin(); iter++;
  while(iter != pandawindows_array.end())
  { scene_nodepaths[entity].instance_to(iter->second->get_render());
    iter++; } }
\end{lstlisting}
Las últimas modificaciones afectan a la configuración de las ventanas de render. Por un lado se cargan los valores de la resolución de cada ventana de render y por otro se realiza una transformación horizontal o vertical de la imagen renderizada. De hecho, por eficiencia lo que se hace es escalar la cámara por el valor -1 en los ejes horizontal o vertical según corresponda. De esta forma no será necesario transformar la imagen después de ser renderizada. Un detalle a tener en cuenta es que al realizar el escalado en uno de los ejes del plano de render, el vector normal se invierte, por lo que la ocultación de caras usada en la escena (cull facing) también debe ser invertido o se verán las caras de los polígonos contrarias a las deseadas.
\begin{lstlisting}[language=C++]
for (unsigned int i = 1; lock && (i <= num_windows); i++)
{ int flip_me_x = (app_config->GetDisplayData(i).flip_h) ? -1 : 1;
  int flip_me_y = (app_config->GetDisplayData(i).flip_v) ? -1 : 1;
  windowcamera_array[i].set_sx(flip_me_x);
  windowcamera_array[i].set_sz(flip_me_y);
  if ( (flip_me_x == -1) ^ (flip_me_y == -1) )
   pandawindows_array[i]->get_render().set_attrib(CullFaceAttrib::make_reverse());
  else
   pandawindows_array[i]->get_render().set_attrib(CullFaceAttrib::make_default()); } }
\end{lstlisting}

\end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Módulo de Percepción}
$\backslash$src$\backslash$ipercept$\backslash$PerceptVideo: Los cambios en este módulo son mínimos y afectan sólamente al establecimiento de los parámetros de configuración. En este caso se aplica una transformación de reflejo horizontal o vertical según se haya especificado en la configuración.
\begin{lstlisting}[language=C++]
if ( app_config != NULL )
{ flip_me_x = app_config->GetCameraData(iter->first).flip_h;
  flip_me_y = app_config->GetCameraData(iter->first).flip_v;  
  if(flip_me_x && !flip_me_y) //Flip Horizontally
  {	IplImage *aux = capture_img;
    cvFlip(capture_img, capture_img, 1); }
  else if(!flip_me_x && flip_me_y) //Flip Vertically
  { IplImage *aux = capture_img;
    cvFlip(capture_img, capture_img, 0); }
  else if(flip_me_x && flip_me_y) //Flip Both
  { IplImage *aux = capture_img;
    cvFlip(capture_img, capture_img, -1); }
\end{lstlisting}

\section{Validación y Publicidad}
\subsection{Validación}
\begin{itemize}
\item Comprobación de uso de recursos de la máquina mediante las herramientas del sistema. En la máquina en la que se desarrolla la aplicación muestra consumir un 4 y un 6\% de CPU, con 21 subprocesos asociados y 85Mb de memoria mantenidos con el escenario por defecto de prueba. Al cargar los distintos escenarios se ven cambios dependiendo de la complegidad de los mismos, lo cual es esperado. 
\item Comprobración de la ejecución y cierre correctos, sin salidas de la aplicación inesperadas ni memory leaks.
\item Uso de herramientas para medición de frames por segundo para comprobar el rendimiento de la ventana de render. No se aprecian cambios en la tasa de frames por segundo que se mantiene a 60fps para dos ventanas de render. Teniendo en cuenta que 60 es el límite máximo impuesto por la sincronización vertical del monitor.
\end{itemize}
Se muestran algunas capturas del estado actual de la aplicación:
\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/screenshot_20100717_o.jpg}
\end{center}
\caption{ \label{F_CapturaFase3SegundaDemo_20100717} Fase 3: Segunda Demo}
\end{figure}

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/screenshot_20100717_p.jpg}
\end{center}
\caption{ \label{F_CapturaFase3SegundaDemo_20100717} Fase 3: Segunda Demo}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Etapa 4: Detección, Navegación e Interacción}

Siguiendo la mecánica establecida se continúa añadiendo funcionalidades a partir de la demo anterior. En esta ocasión se abordará fundamentalmente el problema de la detección, reconocimiento y localización del usuario. Se hará uso de técnicas de visión de computador para situarlo en el espacio de la instalación y establecer la situación de las cámaras del espacio 3D en correspondencia. También se establecerán mecanismos de interfaz visual de usuario, que pretenden ser simples y lo más transparentes posible, para permitir el movimiento del sujeto por el espacio 3D, así como la posibilidad de activar autologin mediante reconocimiento facial. Finalmente, se hace una introducción a los mecanismos de interacción del usuario con el entorno. En un principio, se integrará un esquema de navegación y preparación del sistema de colisiones básicos. \\

Al finalizar esta fase se dispondrá de los mecanismos principales para poder crear los contenidos, así como para poder utilizar la interfaz visual con el usuario, tanto para la navegación por la escena como la interacción con los elementos que existan en ella. En la siguiente etapa se seguirá trabajando en estos apartados para incorporar capacidades más avanzadas, pero además se comenzará a abordar el esquema a usar para la creación de contenido.\\

Como se ha comentado, existen tres apartados a abordar:\\

Por un lado, la percepción: Para ello es necesario establecer y configurar el espacio de la instalación mediante la calibración de las cámaras. Esta opción está ya disponible a través de las ventanas de configuración de la aplicación, pero es ahora cuando se implementará su funcionalidad. Una vez definido el espacio se procederá a la localización del usuario dentro del mismo, estableciendo una relación entre el espacio de la instalación y su correspondencia con el espacio de la escena 3D. Se preparán diversos detectores, a saber: presencia, movimiento y caras. También se incorporará reconocimiento facial. \\

Partiendo del punto anterior, se abordará la Navegación, esto es, los mecanismos que se establecerán para permitir el movimiento del usuario por la escena. Se utilizará el detector de presencia y el de caras para la localización global y el movimiento; la cara se usará para el movimiento relativo de las cámaras de la escena. \\

De forma paralela se usará el detector de caras para alimentar el entrenamiento del reconocedor de caras que permitirá hacer login de forma automática en la aplicación. Cuando la aplicación encuentre un sujeto que no pueda reconocer lo añadirá al registro, creando una nueva cuenta de usuario y una escena por defecto vacía. Cuando el sistema sea capaz de reconocer a un usuario iniciará sesión y cargará la última escena creada. \\

Para terminar esta fase, se hará una introducción a los mecanismos que permitirán al usuario interactuar con el entorno y sus elementos. De esta forma, se propone detectar colisiones entre objetos de la escena 3D, estableciendo una clasificación de tipos de objetos.\\

En relación a las áreas de trabajo, el módulo en el que se centrará el desarrollo en esta fase será el de Percepción. A partir de ahora será capaz de ofrecer acceso a las imágenes capturadas, así como de analizar las mismas para extraer información. También se añadirán controladores al a módulo de Applicación principal, especialmente para incorporar el controlador de navegación; y se realizarán ajustes en los módulos de GUI y Producción.


\section{Análisis}

\subsection{Análisis de Requisitos Hardware}

Entrando de lleno en el campo de la detección en este capítulo, se comentarán los supuestos sobre los que se espera trabajar. Para ello, se ha tenido en cuenta el ranking de clasificación de Thomas B. M. \cite{THOMASB00}, para sistemas basados en Visión por Computador que puede verse en la figura \ref{F_Typical_Assumptions_Etapa4}, donde se relacionan los supuestos más comunes. \\

Las instalaciones creadas con esta librería se suponen situadas en un entorno controlado. Los paneles de proyección y las cámaras pueden colocarse donde se desee, y pueden ser configuradas para estudiar planos de estudio no frontales a la misma. Sin embargo, las cámaras deben estar fijas durante su funcionamiento. Por otro lado, no es necesario que el fondo sea uniforme, pero sí debe ser estático. También se espera que las condiciones de iluminación sean constantes y adecuadas y, aunque no es mandatorio, se presupone que el usuario se moverá sobre un suelo plano. \\

Se recuerda que puede hacerse uso de cualquier cantidad de cámaras para la captura de datos, así como cualquier cantidad de paneles de proyección que el sistema sea capaz de soportar. Para la realización de este proyecto no se abordará el trabajo sobre imágenes en estéreo, en su lugar las cámaras pueden prepararse para analizar un plano de estudio que no necesariamente sea frontal. Finalmente, dependiendo de las intenciones que se tengan para la implementación de una instalación, hay que tener en cuenta que si una cámara se coloca de forma que capture la imagen proyectada, ésta puede afectar a la sustracción de fondo.

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/typical_assumptions.png}
\end{center}
\caption{ \label{F_Typical_Assumptions_Etapa4} Cuadro de supuestos considerados. Detección, Navegación e Interacción \cite{THOMASB00}}
\end{figure}

\subsection{Análisis de Requisitos de Usuario}

Observamos por un lado la necesidad de configurar las cámaras para hacer uso de las mismas para la percepción. Para ello, se podrán calibrar a través de las opciones de configuración de la aplicación. Se ofrecerán mecanismos para corregir la distorsión de las cámaras, reorientarlas (flip vertical-horizontal) y calcular la homografía si se desea definir un plano de estudio. \\

También existen opciones similares para configurar las ventanas de render para ajustarse a su localización. \\

Por otro lado, se hará uso de las técnicas de reconocimiento para dos tareas: primero para el reconocimiento, poder indentificar al sujeto; y segundo para la interacción, donde consideramos la navegación dentro de la escena así como la interacción con los objetos que existan en la misma. Desde este punto de vista se considerará una clasificación de los tipos de objetos. Se tendrán objetos interactivos, capaces de participar en las acciones, ya sea de forma pasiva o activa; por otro lado, se tendrán los objetos no interactivos, que simplemente se dibujan, podrán ser modificados o verse afectados por el entorno pero no tendrán la capacidad de participar en las acciones o interactuar.

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/casosdeuso_tercerademo.png}
\end{center}
\caption{ \label{F_Casos_de_Uso_Etapa4} Diagrama de Casos de Uso. Detección, Navegación e Interacción}
\end{figure}




\subsection{Selección de Herramientas}

La librería principal que se usará para los elementos de Visión por Computador sigue siendo OpenCV. Se usarán las funcionalidades ofertadas por esta librería para todo el proceso de Visión por Computador. Hay que destacar que para la detección facial se han implementado dos alternativas: Por un lado, se soporta por defecto la detección simple por componentes principales que implementa OpenCV, y por otro, mediante una opción de compilación, se da soporte para la librería desarrollada por Modesto Castrillón Encara2 \cite{Castrillon05}. De esta forma se mantienen los requisitos software del proyecto, especialmente en relación a la licencia del proyecto. Para la detección de colisiones se ha optado por usar el propio motor de juego de Panda3D que ofrece estas capacidades, lo que evitará mantener dos modelos de forma independiente.

\begin{itemize}
\item Visión por Computador: OpenCV / Encara2
\item Detección de Colisiones: Panda3D
\end{itemize}


\section{Diseño}
\subsection{Diseño de la Aplicación}
Se añaden elementos al módulo IPercept para incorporar los nuevos tipos de detectores. También se realizaran ajustes en el núcleo de la interfaz y los módulos Application e IProd.

\begin{itemize}
\item Core: Conjunto de interfaces de la aplicación.
\begin{itemize}
\item IAplicationConfiguration: Se añaden opciones para indicar el uso de reconocimiento y autologin.
\item IPercept: Interfaz para el módulo de Percepción. Se añaden métodos para acceder a las capacidades de los detectores creados.
\begin{itemize}
\item IPerceptVideo: Ajustes en la interfaz para acceder a las funcionalidades ofrecidas por los nuevos detectores añadidos. 
\begin{itemize}
\item IFaceDetection: Interfaz para los detectores faciales que se quieran implementar. Se ofrecen mecanismos para detectar la existencia o no, así como obtener características.
\item IFaceRecognition: Interfaz para los reconocedores faciales que se quieran implementar. Capacidad para añadir nuevos usuarios al catálogo y de identificarlos a partir de una imagen de la cara.
\item IPresenceDetection: Interfaz para los detectores de presencia que se quieran implementar. Métodos para detectar la existencia o no, así como obtener características.
\item IMotionDetection: Interfaz para los detectores de movimiento que se quieran implementar.
\end{itemize}
\end{itemize}
\end{itemize}
\item igui: Ajustes para añadir opciones de configuración de las cámaras.
\begin{itemize}
\item GUIConfiguration: Se añade un panel de opciones generales, cuya funcionalidad se implementará más adelante, y otro panel de Visualización, mediante el cual se podrá ver información de interés del funcionamiendo de los distintos módulos. También se añaden opciones de configuración avanzaad en los paneles de configuración de las Cámaras.
\item GUIUser: Se añade un cuadro de imagen donde se mostrará la cara que esté siendo estudiada.
\end{itemize}
\item iprod: Se realizan cambios para añadir capacidad de detectar colisiones entre objetos de la escena.
\item vox: 
\begin{itemize}
\item ApplicationConfiguration: Ajustes necesarios para guardar las opciones de reconocimiento y autologin.
\item NavigatorController: Controlador que implementa la navegación del usuario dentro de la escena.
\item Application: Ajustes para automatizar el inicio de sesión haciendo uso del módulo de percepción.
\end{itemize}
\end{itemize}

\subsubsection{Breve descripción de los módulos}
Los cambios a realizar afectarán a los módulos de Aplicación, Percepción y Producción.\\

En el primero se introducirá un controlador que permitirá al usuario navegar en la escena presentada. También se añadirá capacidad para, mediante el uso del módulo de percepción, detectar presencia dentro del espacio de estudio y reconocer al usuario facialmente para, de forma opcional, cargar su último escenario creado. \\

En el módulo de percepción, el trabajo se centrará en añadir los primeros detectores al sistema. Se definirán interfaces para poder acceder a las características de interés de los mismos. Se tratará de tres detectores (cara, presencia, movimiento) y de un reconocedor (facial). \\

Finalmente se realizaran los cambios necesarios en el módulo de producción para detectar colisiones entre las entidades que existen dentro de la escena. Se consideran dos tipos de objetos: Interactivos, que a su vez se separan en Activos y Pasivos, y No-Interactivos.

\newpage
\subsubsection{Diseño General en UML}
\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/DiagramadeclasesUML_TerceraDemo.png}
\end{center}
\caption{ \label{F_DiagramadeClasesUML_TerceraDemo} Diagrama de Clases UML, Tercera Demo. Resumen.}
\end{figure}
Se centra el trabajo en el módulo de Percepción. En concreto se diseñan los módulos e interfaces necesarios para la Detección de presencia, movimiento y caras, así como reconocimiento facial. De esta forma, se añaden los componentes IFaceDetection, IFaceRecognition, IPresenceDetection e IMotionDetection a la interfaz y los correspondientes SimpleFaceDetection, Encara2FaceDetection, FaceRecognition, PresenceDetection y MotionDetection al Módulo de Percepción, formando parte de PerceptVideo. También se añade un controlador a la aplicación con el fin de integrar la navegación del usuario dentro de la escena. Finalmente, los cambios relativos al módulo de Producción no son visibles a este nivel. Se estudiará cada apartado en detalle.

\subsubsection{Diseño en UML - Núcleo de la Interfaz}
\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/DiagramadeclasesUML_TerceraDemo_core.png}
\end{center}
\caption{ \label{F_DiagramadeClasesUML_TerceraDemo_core} Diagrama de Clases UML, Tercera Demo. Interfaces.}
\end{figure}
No existen modificaciones de relevancia en las secciones ya definidas del módulo, a excepción de ciertos ajustes necesarios en IPercept e IPerceptVideo para ofrecer acceso a las nuevas características que se integran. El trabajo se centrará principalmente en añadir las nuevas interfaces para los detectores: IFaceDetection, IPresenceDetection, IMotionDetection e IFaceRecognition. Cada uno de ellos ofrecerá mecanismos para consultar información relevante sobre el resultado de su análisis. En especial, si detectan o no su objeto de estudio e información relativa al mismo.

\subsubsection{Diseño en UML - Módulo de Aplicación}
\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/DiagramadeclasesUML_TerceraDemo_vox.png}
\end{center}
\caption{ \label{F_DiagramadeClasesUML_TerceraDemo_vox} Diagrama de Clases UML, Tercera Demo. Aplicación.}
\end{figure}

Se añade un controlador de navegación, NavigationController, que permitirá al usuario moverse por la escena. La interfaz del usuario es eminentemente visual, por lo que accederá a los módulos de percepción para detectar y localizar al sujeto, y al módulo de producción para situarlo en la escena 3D. Se recuerda que también será necesario trabajar en este módulo para añadir las tareas de reconocimiento y autologin. Sin embargo, se usarán mecanismos ya disponibles por lo que no serán necesarios cambios en el diseño del módulo.

\subsubsection{Módulo de Persistencia}
\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/DiagramadeclasesUML_TerceraDemo_ipersistence.png}
\end{center}
\caption{ \label{F_DiagramadeClasesUML_TerceraDemo_ipersistence} Diagrama de Clases UML, Tercera Demo. Persistencia.}
\end{figure}

Como se ha comentado las entidades que existen en la escena se categorizarán en dos tipos: Interactivos y No-Interactivos. Siendo los primeros los que pueden verse involucrados en las acciones que realiza el usuario u otras entidades, y los segundos los que no. Todas las entidades interactivas podrán responder a las acciones que se llevan a cabo. Sin embargo, sólo las activas provocarán acciones, siendo pasivas las que no. Estas características están definidas en el atributo psique. Se añaden los métodos IsInteractive e IsActive a EntityPersistence para facilitar el acceso.

\subsubsection{Diseño en UML - Módulo de GUI}
\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/DiagramadeclasesUML_TerceraDemo_igui.png}
\end{center}
\caption{ \label{F_DiagramadeClasesUML_TerceraDemo_igui} Diagrama de Clases UML, Tercera Demo. GUI.}
\end{figure}

Se recuerda que el módulo de GUI, incluye los paneles de la interfaz gráfica de la aplicación pero no son sólo paneles de interfaces. Se trata de un conjunto de mecanismos para trabajar sobre las interfaces gráficas, por lo que también atiende peticiones sobre las mismas. \\

Se realizan modificaciones en el panel GUIUser para mostrar la cara del sujeto y añadir autologin. Se añaden a MainGUI métodos para rellenar las interfaces y ejecutar acciones bajo demanda, como son SetFace, que establece la cara de estudio actual, o FillLoginUserGUI o FillNewUserGUI rellena los campos nombre-contraseña del usuario reconocido o del nuevo usuario a registrar en correspondencia. MainGUI también ofrece mecanismos para mostrar la interfaz que se solicite, así como realizar acciones sobre la misma.

\subsubsection{Diseño en UML - Módulo de Producción}
\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/DiagramadeclasesUML_TerceraDemo_iprod.png}
\end{center}
\caption{ \label{F_DiagramadeClasesUML_TerceraDemo_iprod} Diagrama de Clases UML, Tercera Demo. Producción.}
\end{figure}

En relación a la navegación, hay que tener en cuenta de que se trata de varios sistemas independientes, que se ejecutan en sendos hilos y que se comunican entre sí. De esta forma se obtendrán posiciones en determinados momentos, que serán frecuentes pero no ocurrirán en cada iteración. Por ello, para conseguir un movimiento suave, se ha optado por realizar una variación del esquema PDU para la localización y movimiento de objetos. Se estudiará en detenimiento en la sección de Implementación \ref{etapa4Implementacion:produccion}. \\

Para resolver el problema de las colisiones será necesario añadir varios componentes. Por motivos de eficiencia se decide usar colisionadores sencillos asociados a las geometrías de las entidades. Se conservará una relación entre éstas y sus colisionadores. Además, será necesario un buscador de colisiones, así como una cola donde mantener las colisiones detectadas en cada frame.

\subsubsection{Diseño en UML - Módulo de Percepción}
\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/DiagramadeclasesUML_TerceraDemo_ipercept.png}
\end{center}
\caption{ \label{F_DiagramadeClasesUML_TerceraDemo_ipercept} Diagrama de Clases UML, Tercera Demo. Percepción.}
\end{figure}

Como puede observarse se añaden los nuevos detectores al componente PerceptVideo, dentro del módulo de Percepción. PerceptVideo será el responsable de su creación y manejo. Así mismo, ofrece métodos para acceder a la información de los distintos detectores. La información consiste en la posición de la característica detectada, la imagen de la misma así como otros datos asociados. Hay que recordar que las implementaciones de los detectores son independientes de la solución tecnológica elegida en cada caso. \\

Otra consideración es que existirá un detector por cada cámara conectada al sistema. Debido a esto, sumado a que se trata de procesos computacionalmente costosos y que no es deseable mantener el sistema de percepción saturado o bloqueado, se plantea el que cada detector se ejecute en un hilo paralelo. Por ello, entre otras consideraciones, se deben mantener copias independientes para cada detector.

\begin{itemize}
\item SimpleFaceDetection: Implementación del Detector de Caras que utilizará los métodos ya proporcionados por la libreria OpenCV. Se basa en un clasificador Haar (cascade), basado en los trabajos de Paul Viola \cite{Viola01cvpr}.
\item Encara2FaceDetection: Implementación del Detector de Caras utilizando la librería desarrollada por Modesto C., Encara2 \cite{Castrillon05}. También necesita un clasificador Haar.
\item PresenceDetection: Implementación del Detector de Presencia, usando la librería OpenCV. Esta libería utiliza un algoritmo de detección de diferencias basado en los trabajos de Liyuan Li et al. \cite{LIYUAN03}. Los atributos que se muestran en la definición son los necesarios para la aplicación del algoritmo.
\item MotionDetection: Implementación del Detector de Movimiento, usando la librería OpenCV que mantiene un historial de imágenes de movimiento.
\end{itemize}
\newpage


\section{Implementación}
Los módulos afectados en la presente fase son los siguiente:

\begin{itemize}
\item core: Añadidas las interfaces en IPercept para los diversos detectores: IFaceDetection, IFaceRecognition, IPresenceDetection, IMotionDetection. \\

También será necesario realizar ajustes de menor relevancia en las interfaces de IApplication, IApplicationConfiguration, IEntityPersistence, IGUI, e IProd.
\item igui: Ajustes en GUIUser para mostrar la cara que está siendo objeto de estudio. Se añade un nuevo panel de Visualización a la interfaz de Configuración de la Aplicación.
\item ipercept: Implementación de los detectores Encara2FaceDetection, SimpleFaceDetection, PresenceDetection, MotionDetection y del reconocedor FaceRecognition.
\item ipersistence: Se añaden métodos para consultar el tipo de entidad: Interactivo, No-Interactivo.
\item iprod: Se realizarán ajustes para modificar el movimiento de la cámara, que será accedido con el controlador de navegación, además de los cambios necesarios para realizar la detección de colisiones entre objetos de la escena.
\item vox: Se añade un Controlador de Navegación y se realizan cambios que permitan utilizar reconocimiento, así como inicio de sesión y carga de escenarios de forma automática.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Núcleo de la Interfaz}
$\backslash$src$\backslash$core$\backslash$IPercept$\backslash$video \\

Es necesario recordar que debe mantenerse una interfaz común para cualquier implementación. Por ello, las imágenes se describen con los parámetros necesarios para construirla a partir de un vector de caracteres. Estos parámetros son dimensiones en ancho y alto, número de canales, profundidad, ancho de línea (calculada como Ancho x Alto x NCanales x Profundidad) y finalmente el bloque de datos en sí. \\

También hay que tener en cuenta que cada detector posiblemente realice trabajo altamente costoso computacionalmente, y que además se desea usar uno en cada cámara que se quiera insertar. Por ello se decide que la mejor opción es que se ejecute cada uno en un hilo en paralelo. Por ello, cada detector necesita una copia propia de la imagen para trabajar, ya que el acceso a la misma zona de memoria no sería adecuado. Además, se decide que sea el componente PerceptVideo (el que captura las imágenes y realiza tareas generales de percepción) el que realice las copias y las envíe en cada iteración a los distintos detectores. En caso contrario, seguramente se realizarían demasiadas peticiones de parte de los detectores a este componente, aumentando en gran medida la posibilidad de bloqueo de los hilos solicitantes y reduciendo el rendimiento tanto del detector como del componente PerceptVideo. \\

De esta forma una vez PerceptVideo haya distribuído las imágenes, cada detector realizará su trabajo en paralelo. Por otro lado, para evitar que un detector realice trabajo innecesario se utilizará un flag para indicar cuándo se ha introducido una imagen nueva, de forma que en caso de que no haya una nueva imagen que tratar el hilo vuelva a dormirse. \\

Sin embargo, llegados a este punto surge una pregunta, ¿por qué no usar el patrón Observador-Observable? El motivo es simple: se trata de ejecución en distintos hilos, por lo que en que el fondo esta solución debería hacer lo mismo que la actual, sólo que introduciendo lógica innecesaria: el hilo del observable debería ejecutar su Notify que activaría un flag en el método Update del observador; por otro lado, el observador debería chequear dicho flag para realizar o no su trabajo. Si el Update del Observador tuviera el código que realiza el trabajo, sería el hilo del Observable el que lo ejecutara al llamar a la fución, y es precisamente esto lo que se desea evitar.

\begin{itemize}
\item IFaceDetection: Define los métodos que deberán tener todos los detectores de caras que se deseen implementar. Contiene los métodos relacionados con los módulos que se ejecutan en paralelo, en concreto el método Init. Sin embargo, los métodos que lo caracterizan son aquéllos usados para introducir imágenes de estudio, y por otro lado los usados para recuperar información: 
\begin{itemize}
\item FaceDetected: Devuelve verdadero o falso dependiendo de si se ha detectado una cara en la última image procesada.
\item SetCurrentImage: Inserta una nueva imagen de estudio. 
\item GetCopyOfCurrentImage: Devuelve una copia de la última imagen que ha sido procesada, con los marcadores del resultado dibujados sobre ella. 
\item GetCopyOfAreaOfInterest: Devuelve una copia del área de interés, que en el caso de este detector será la mayor cara detectada. Como puede querer usarse para otras tareas se devuelve sin marcadores dibujados.
\item GetFaceCenterPos: Para obtener las coordenadas en píxeles del centro de la cara detectada.
\item GetFaceRec: Devuelve el rectángulo que enmarca la cara detectada.
\item GetFeaturePos: Devuelve la posición de la característica que se desea. La característica se pasa como una cadena de texto, puede indicar valores como 'boca', 'nariz', 'ojo izquierdo' y 'ojo derecho'.
\item Apply: Método interno que realiza el trabajo de detección.
\end{itemize}
\begin{lstlisting}[language=C++]
namespace core{
class _COREEXPORT_ IFaceDetection
{ public:
  virtual ~IFaceDetection(){}
  virtual void Delete()=0;
  virtual void Init()=0;
  virtual bool FaceDetected() = 0;
  virtual void SetCurrentImage(
               const int &size_x, const int &size_y, 
               const int &n_channels, const int &depth, 
               const int &width_step, char * data) = 0;
  virtual char * GetCopyOfCurrentImage(
                 int &size_x, int &size_y, ...) = 0;
  virtual char * GetCopyOfAreaOfInterest(
                 int &size_x, int &size_y, ...) = 0;
  virtual void GetFaceCenterPos(corePoint2D<int> &pos) = 0;
  virtual void GetFaceRec(corePoint2D<int> &corner_a, 
                          corePoint2D<int> &corner_b) = 0;
  virtual void GetFeaturePos(const std::string &feature, 
                             corePoint2D<int> &pos) = 0;
 private:
  virtual bool Apply() = 0; };}
\end{lstlisting}

\item FaceRecognition: En este caso no se trata de un detector, sino de reconocedor. Su objetivo es identificar a un usuario a partir de una imagen facial. \\

Como la identidad del objeto de estudio no es probable que cambie con frecuencia entre frames, se considera que su uso está más destino a ser bajo demanda, por lo que no es necesario que se ejecute como un componente en paralelo. Ofrece mecanismos básicos para solicitar el reconocimiento de una persona dada una imagen de su cara, así como métodos para añadir un nuevo usuario al registro y solicitud de entrenamiento.
\begin{lstlisting}[language=C++]
namespace core{	
class _COREEXPORT_ IFaceRecognition
{ public:
   virtual ~IFaceRecognition(){}
   virtual void Train() = 0;
   virtual int RecognizeFromImage(
               char* data, const int &size_x, const int &size_y, 
               const int &n_channels, const int &depth, 
               const int &width_step) = 0;
   virtual void AddUser(std::vector<core::Image> faces, 
                        const int &user_id) = 0; };}
\end{lstlisting}
\item PresenceDetection: Este detector está enfocado a distinguir si existe una presencia dentro del espacio de estudio o no. Por lo general una presencia se considerará como la existencia de algo que no forma parte del fondo. \\

Hay que tener en cuenta que el fondo puede no ser uniforme, aunque como podía verse en la figura \ref{F_Typical_Assumptions_Etapa4}, página \pageref{F_Typical_Assumptions_Etapa4}, se espera que sea regularmente estático. \\

Los métodos necesarios son similares a los ya vistos en el caso de detector de caras. Se añaden:
\begin{itemize}
\item PresenceDetected: Indica si se ha detectado o no una presencia en la última imagen procesada.
\item GetPresenceCenterOfMass: Para obtener el centro de masas del área detectada como presencia.
\item GetPresenceRec: Para obtener las coordenadas en píxeles del bounding box que encuadra la presencia.
\item TrainBackground: Solicita que se comience a entrenar el detector.
\end{itemize}
\begin{lstlisting}[language=C++]
namespace core{
class _COREEXPORT_ IPresenceDetection
{ public:
   virtual ~IPresenceDetection(){}
   virtual void Delete()=0;
   virtual void Init()=0;
   virtual void SetCurrentImage(
                const int &size_x, const int &size_y, 
                const int &n_channels, const int &depth, 
                const int &width_step, char * data) = 0;
   virtual char * GetCopyOfCurrentImage(
                  int &size_x, int &size_y, 
                  int &n_channels, int &depth, 
                  int &width_step, 
                  const bool &switch_rb = false) = 0;	
   virtual void GetPresenceCenterOfMass(corePoint2D<int> &pos)=0;
   virtual void GetPresenceRec(corePoint2D<int> &corner_a, 
                               corePoint2D<int> &corner_b) = 0;
   virtual bool PresenceDetected() = 0;
   virtual void TrainBackground() = 0;
  private:
   virtual bool Apply() = 0;  };}
\end{lstlisting}

\item MotionDetection: En este caso se desea detectar zonas de la imagen donde exista movimiento. Por el momento sólo obtiene y muestra el movimiento detectado. Será en la siguiente fase, en concreto en interacción avanzada cuando se trabaje en mayor profundidad con este detector.
\begin{lstlisting}[language=C++]
namespace core{
class _COREEXPORT_ IMotionDetection
{ public:
   virtual ~IMotionDetection(){}
   virtual void Delete()=0;
   virtual void Init()=0;
   virtual void SetCurrentImage(
                const int &size_x, const int &size_y, 
                const int &n_channels, const int &depth, 
                const int &width_step, char * data) = 0;
virtual char * GetCopyOfCurrentImage(
                int &size_x, int &size_y, 
                int &n_channels, int &depth, int &width_step, 
                const bool &switch_rb = false) = 0;   };}
\end{lstlisting}
\end{itemize}

$\backslash$src$\backslash$core$\backslash$IApplication \\

Se añaden métodos para ofrecer acceso a funcionalidades de los nuevos módulos. En especial para la configuración de los mismos:
\begin{lstlisting}[language=C++]
...
   virtual bool Calibrate()=0;
   virtual bool CalculateHomography()=0;
   virtual bool TrainBackground()=0;
\end{lstlisting}

$\backslash$src$\backslash$core$\backslash$IApplicationConfiguration \\

Se añaden las opciones de configuración necesarias para los nuevos módulos. Por un lado se tendrán las opciones de visualización para poder mostrar datos de interés de cada uno de los detectores. Por otro, se introducen Setters y Getters para las dos opciones de configuración nuevas 'USE RECON', que indica que se desea realizar reconocimiento, y 'AUTO LOGIN' que indica que se desea realizar login en la aplicación de forma automática.
\begin{itemize}
\item ShowCamCapture: Muestra las imágenes que capturan directamente las cámaras conectadas al sistema.
\item ShowHomography: Muestra el resultado de aplicar la matriz de transformación de que sitúa el plano de estudio de forma frontal a la cámara.
\item ShowFaceDetection: Muestra la imagen de entrada con la localización de la cara detectada resaltada sobre ella.
\item ShowForeground: Muestra una imagen binaria que indica la presencia que se ha detectado.
\item ShowMotion: De igual forma, basada en la imagen de entrada muestra el resultado de realizar el proceso. En este caso se verá resaltado las zonas en las que se detectan movimiento siguiendo el histórico de las imágenes.
\item GetUseRecognition: Indica si se desea utilizar el reconocimiento en la aplicación.
\item GetAutoLogin: Indica si se desea que la aplicación inicie la sesión de forma automática cuando ha detectado a una persona.
\end{itemize}
\begin{lstlisting}[language=C++]
...
   virtual bool GetUseRecognition() = 0;
   virtual bool GetAutoLogin() = 0;
...
   virtual void ShowCamCapture(const bool &value) = 0;
   virtual void ShowHomography(const bool &value) = 0;
   virtual void ShowFaceDetection(const bool &value) = 0;
   virtual void ShowForeground(const bool &value) = 0;
   virtual void ShowMotion(const bool &value) = 0;
   virtual void SetUseRecognition(const unsigned bool &value)= 0;
   virtual void SetUseRecognition(const unsigned bool &value)= 0;
\end{lstlisting}

src$\backslash$core$\backslash$IPersistence$\backslash$IEntityPersistence\\

Se añaden dos métodos para acceder fácilmente al tipo de entidad que se trata, en relación a su capacidad de interacción: IsInteractive e IsActive. \\

Como se ha comentado en las secciones previas, se consideran entidades interactivas aquéllas que provoquen o puedan participar en las acciones de otras. Dentro de estas entidades serán activas, aquéllas que puedan provocar acciones sobre las otras. Las entidades consideradas pasivas sólo podrán reaccinar, no podrán provocar acciones sobre ninguna entidad. \\

src$\backslash$core$\backslash$IGUI$\backslash$IGui\\

Recordemos que el módulo de GUI ofrece opciones para ser manipulado, de forma que se puedan ejecutar acciones sobre la interfaz o indicar qué paneles mostrar. De esta forma se añaden los métodos necesarios para rellenar los campos de usuario y contraseña, tanto para login como para la creación de un usuario nuevo. \\

Para el caso del Autologin se añaden los métodos LogIn y LogOut, que desencadenarán las acciones necesarias: es cierto que la aplicación por sí misma puede realizar las operaciones subyacentes al inicio y cierre de sesión, a excepción de los cambios en la interfaz. Por este motivo se opta por encauzar esta acción con la existente del módulo de GUI. La interfaz realizará los cambios necesarios y además, ya que previamente realizaba estas peticiones a la aplicación, cuando era el usuario el que provocaba los eventos al pulsar sobre los botones, solicitará a la aplicación las acciones necesarias para el inicio de la sesión. Se añaden los siguientes métodos a la interfaz de IGui:

\begin{lstlisting}[language=C++]
...
   virtual void LogOut() = 0;
   virtual void LogIn(const std::string &name, 
                      const std::string &passwd) = 0;
   virtual void ShowFaceAtGUI(char* data, 
                const int &size_x, const int &size_y, 
                const int &n_channels, const int &depth, 
                const int &width_step) = 0;
   virtual void FillLoginUserGUI(const std::string username, 
                                 const std::string userpasswd)=0;
   virtual void FillNewUserGUI(const std::string username, 
                               const std::string userpasswd) = 0;
\end{lstlisting}

src$\backslash$core$\backslash$IProd$\backslash$IProd\\

El módulo de producción deberá proveer de un mecanismo para modificar la posición de las cámaras. Será el controlador de la aplicación NavigatorController el que a partir de ahora defina cómo situarlas. Será éste el encargado de definir la forma en la que el usuario se moverá por la escena, y dado que las cámaras representan el punto de vista el usuario lo hará modificando la posición de las mismas. \\ 

Por el momento basta con añadir el método SetCamerasPosition(). Es evidente que el esquema actual de la aplicación introduce elementos que complican el proceso de establecer una nueva posición y actualizar el render. Estos detalles podrán verse en las notas de implementación del módulo de producción.

\begin{lstlisting}[language=C++]
virtual void SetCamerasPosition(const core::corePoint3D<double> &pos)=0;
\end{lstlisting}

En relación a la detección de colisiones por el momento no serán necesarios cambios en la interfaz del módulo.\\

\subsubsection{Módulo de Aplicación}
app$\backslash$vox$\backslash$Application \\

Respondiendo a los cambios introducidos en la interfaz IApplication se implementan los métodos de calibrado, cálculo de homografias y entrenamiento de la detección de fondo. Como puede verse en el cuadro que se muestra a continuación, estos métodos sólo encauzarán la petición al módulo responsable, que en este caso será el módulo de Percepción. \\

\begin{lstlisting}[language=C++]
...
bool Application::Calibrate()
{	if (app_mainpercept)
		app_mainpercept->Calibrate(true);
	return true; }
bool Application::CalculateHomography()
{	if (app_mainpercept)
		app_mainpercept->CalculateHomography();
	return true; }
bool Application::TrainBackground()
{	if (app_mainpercept)
		app_mainpercept->TrainBackground();
	return true; }
\end{lstlisting}

Adicionalmente, se introducen los cambios necesarios para soportar autologin. Esta tarea se ejecuta en la función OnIdle, que se había preparado para capturar los eventos de wxWidgets y destinada a realizar tareas no demasiado exigentes en frecuencia. \\

Lo primero que se realiza es obtener las opciones e configuración de Applicationconfiguration para saber si se debe realizar o no Reconocimiento y Autologin. Si al menos desea utilizarse reconocimiento, el siguiente paso es consultar si se detecta actualmente alguna presencia. \\

En el caso en el que se desee utilizar autologin, pero no se detecte ninguna presencia se cierra la sesión actualmente abierta. \\

\begin{lstlisting}[language=C++]
void Application::OnIdle(wxIdleEvent &event)
{...
 bool use_recon = app_config->GetUseRecognition();
 bool autologin = app_config->GetAutoLogin();
 if(session_controller && app_mainpercept && app_maingui && use_recon)
 { bool session_closed = session_controller->IsSessionClosed();
   bool presence_detected = app_mainpercept->PresenceDetected();
   if(autologin && !session_closed && !presence_detected)
     app_maingui->LogOut();
\end{lstlisting}

Siempre que se desee usar reconocimiento, si la sesión está cerrada y se detecta presencia se consultará si se ha detectado alguna cara en el entorno. Si es así, se mostrará en la interfaz e usuario y se solicitará al módulo de percepción que lo identifique. En caso de éxito en la identificación, se obtendrá la información y rellenarán las interfaces de usuario. En el caso contrario, si no ha sido posible identificar al usuario, se propone la nueva cara detectada como candidata a ser un nuevo usuario. \\

\begin{lstlisting}[language=C++]     
   if(session_closed && presence_detected)
   { bool face_detected = app_mainpercept->FaceDetected();
     if(face_detected)
       img = app_mainpercept->GetCopyOfCurrentFeature("FACE", ...);
     if (app_maingui != NULL)
       app_maingui->ShowFaceAtGUI(img, ...);
     if(face_detected)
     { int user_detected_id = app_mainpercept->RecognizeCurrentFace();
       if (user_detected_id != -1) //known face
       { benefit_of_the_doubt = 0;
         app_mainpersistence->GetUserData(user_detected_id, name, passwd);
         app_maingui->FillLoginUserGUI(name, passwd);
         if (autologin)
         { app_mainpersistence->GetWorldList(name, world_names, world_permissions); 
           app_maingui->LogIn(name, passwd);
           this->RunWorld(world_name);	}	 
        } else { //unknown face
           name = passwd = "anonymous";
\end{lstlisting}

Si durante una cantidad determinada de iteraciones siguientes sigue sin ser posible identificar al sujeto habiéndose detectado una cara, se creará un nuevo usuario en el sistema, asociándole un nuevo escenario vacío por defecto. Adicionalmente, se actualizará la interfaz de usuario. Es necesario mantener esta ventana de incertidumbre debido a que es posible que el reconocedor no sea capaz de detectar al individuo en todas las ocasiones. \\

\begin{lstlisting}[language=C++]            
           if (autologin && app_mainpercept->IsFacePoolReady()
               && (benefit_of_the_doubt > BENEFIT_OF_THE_DOUBT))
           { session_controller->CreateUser(name, passwd);
             session_controller->CreateWorld(wop_world, name, permissions);
             session_controller->CloseSession();
             app_mainpercept->AddNewUserToRecognition(user_id);
             benefit_of_the_doubt = 0;	}
           benefit_of_the_doubt++; 
        }}
...
}
\end{lstlisting}

A partir de ahora, el nuevo usuario ya estaría registrado en el sistema, por lo que formará parte de los usuarios reconocibles. En las siguientes iteraciones el reconocedor debería ser capaz de identificarlo y en tal caso, la aplicación iniciará la sesión y cargará la escena que le fue previamente asociada al nuevo usuario.\\

app$\backslash$vox$\backslash$controllers$\backslash$ConfigurationController \\

Se añaden las nuevas opciones USE RECOGNITION, para indicar que se desea utilizar reconocimineto, y USE AUTOLOGIN para permitir el inicio de sesión automático en la aplicación. Como con el resto de opciones de la aplicación, se obtienen del objeto ApplicationConfiguration, se parsean los datos y añaden al fichero de configuración 'config.ini' al ejecutar el método Save(). En el proceso opuesto, al ejecutar Load(), el fichero es cargado y se extrae la información para asignarlo a ApplicationConfiguration. 

\begin{lstlisting}[language=C++]            
bool ConfigurationController::Load()
{ ...
  else if ( tag == "[USE_RECOGNITION]" )
  { bool value;
    std::stringstream wop;
    config_file.getline(str, size);
    wop << str;
    wop >> value;
    app_config->SetUseRecognition(value);
  }
  else if ( tag == "[AUTO_LOGIN]" )
  { bool value;
    std::stringstream wop;
    config_file.getline(str, size);
    wop << str;
    wop >> value;
    app_config->SetAutoLogin(value);
  }
  ...
\end{lstlisting}

app$\backslash$vox$\backslash$ApplicationConfiguration \\

El siguiente paso es Añadir a AplicationConfigration las nuevas opciones de configuración, para obtenerlas y establecerlas. Se tratan de Getters y Setters comunes, idénticos al resto de las opciones de configuración. Se mostrará a modo de ilustración el caso de ShowCamCapture. Para completar la lista se añadirían ShowHomography, ShowFaceDetection, ShowForeground, ShowMotion, GetUseRecognition, GetAutoLogin. Que se implementarían de la misma manera.

\begin{lstlisting}[language=C++]            
virtual void ShowCamCapture(const bool &value)
{ show_capture = value; }
virtual bool IsShownCamCapture()
{ return show_capture; }
\end{lstlisting}

app$\backslash$vox$\backslash$controllers$\backslash$NavigationController \label{F_Fase4TerceraDemo_ControladorNavegacion}\\

La otra línea de trabajo en Application esta relacionada con la navegación. Se desea implementar una solución simple, a modo de introducción, a la navegación; mediante la cual el usuario pueda moverse por la escena usando exclusivamente una interfaz visual. Es decir, el usuario simplemente se moverá por el espacio de la instalación y a partir de técnicas de Visión por computador, se situarán las cámaras de la escena 3D.\\

Este planteamiento conlleva en realidad tres problemas, que se describirán a continuación:

\begin{itemize}
\item Definición de la mecánica de navegación.
\item Localización del usuario dentro de la escena.
\item Modificación de las cámaras del motor gráfico.
\end{itemize}

Primero: se desea establecer una correspondencia directa, aunque no necesariamente fiel en todo momento, entre la situación del usuario dentro de la instalación con la vista 3D de la escena. Se supondrá el centro del espacio de la instalación identificado con el centro del espacio 3D. Si el usuario de desplaza en algún sentido las cámaras del motor gráfico harán lo mismo. Se usará el centro de masas de la presencia detectada como punto de referencia global del individuo detectado. Se usará además, un vector relativo al centro de masa que nos indique la posición de la cara. En caso de perder la localización de la cara se estimará que se conserva el vector relativo de la última detectada. Se ha optado por esta solución debido a que el centro de masas de la presencia nos ofrece una información más constante del sujeto, mientras que el desplazamiento relativo a la cara nos permitirá colocar las cámaras a la altura de la misma y poder realizar movimientos relativos de cabeceo. Por otro lado, es más probable no poder localizar correctamente la cara del individuo en todo momento. \\

\begin{lstlisting}[language=C++]            
void NavigationController::Iterate()
{ boost::try_mutex::scoped_try_lock lock(m_mutex);
  if ((lock) && (perception) && (production))
  { perception->GetSpaceBoundingBox(space_bounding_box_min, 
                                    space_bounding_box_max);
    presence_detected = perception->PresenceDetected();
    perception->GetHeadPosition(head_pos);
    perception->GetFeaturePosition("CENTER OF MASS", 
                                   presence_center_of_mass);
    com_to_head.x = head_pos.x - presence_center_of_mass.x;
    space_center.x = (space_bounding_box_max.x 
                    + space_bounding_box_min.x)/2;
    ...
\end{lstlisting}

Segundo: Se desea que el usuario pueda desplazarse más allá de los límites espaciales de la instalación. Es decir, es posible que el habitáculo en el que se coloque la instalación tenga unas dimensiones limitadas, pe. 4x4 metros. Sin embargo, las dimensiones del espacio 3D por el que pueda desplazarse son en principio, indefinidas. En este sentido se establece un umbral en las dimensiones del espacio, cercano al límite definido por el tamaño de la imagen, de forma que si el usuario traspasa este umbral se comenzará a aplicar un desplazamiento constante en la dirección del límite al que se ha acercado, en una cantidad proporcional a la distancia entre la situación del usuario y el umbral establecido. De esta forma, cuanto más se acerque a un borde mayor será la velocidad a la que las cámaras se desplazarán en ese sentido, cuando el usuario deje de traspasar el umbral los movimientos volverán ser exclusivamente locales.\\

\begin{lstlisting}[language=C++]  
  ...
  threshold_distance_to_minmax.x = 
    THRESHOLD*(space_bounding_box_max.x - space_bounding_box_min.x);
  double diff=abs(space_bounding_box_max.x - presence_center_of_mass.x);
  if ( diff < abs(threshold_distance_to_minmax.x))
    space_offset.x += OFFSET_STEP*diff;
  else
  { diff =abs(-1*space_bounding_box_min.x + presence_center_of_mass.x);
    if ( diff < abs(threshold_distance_to_minmax.x))
      space_offset.x -= OFFSET_STEP*diff; }
  ...
  final_cam_pos.x = presence_center_of_mass.x + space_offset.x 
                                              + com_to_head.x;
  production->SetCamerasPosition(final_cam_pos);
\end{lstlisting}

Tercero: Se desea que el movimiento sea suave. Hay que tener en cuenta que se trata de diversos componentes que se ejecutan en paralelo. Por un lado, están las cámaras del motor gráfico, que por defecto se actualizarán sesenta veces por segundo (pe. en caso de tener activada la sincronización vertical en un monitor de 60Hz). Sin embargo, por otro lado tenemos las cámaras web y los detectores que ofrecerán resultados son una frecuencia considerable. Desgraciadamente, ya sólo el acceso al dispositivo de captura de imágenes es considerablemente costoso (tanto que no es viable mantener la captura y el render en un mismo hilo sin bajar drásticamente la frecuencia de render). Además el proceso de las imágenes también es costoso. \\

Para resolver este último problema se decide usar el mecanismo conocido como Dead Reckoning, sólo que con algunas modificaciones para adaptarlo a nuestro caso. \\

El procedimiento conocido como Dead Reckoning (deduced reckoning, o navegación por estimación), es usado extensamente en apliaciones en las que no es posible saber en todo momento la localización real de un objeto, como en el caso de videojuegos en red o robótica (un caso concreto el uso de odometria = dead reckoning para calcular trayectoria usando un cuentarrevoluciones). \\

Por poner ejemplos cercanos, nos centraremos en el caso de los juegos en red, donde la posición de los otros jugadores nos llega, con suerte cada 90 ó 200 milisegundos. Si comparamos con una frecuencia de render de referencia 60fps estamos hablando de que disponemos de un dato cada 9 o 20 frames dibujados, con suerte. Si dibujáramos a los jugadores de esta forma no seria posible representar un movimiento continuo, y se apreciarían con toda probabilidad saltos constantes. Para resolver este problema, dead reckoning propone enviar más información que la posición actual, por ejemplo, estimaciones de velocidad, aceleración, etc. (en realidad puede incluirse mucha más información: golpe acertado, puntos de vida sustraídos, etc.). De forma que el otro extremo de la comunicación pueda tendrá una estimación de lo que debería pasar con el objeto entre un dato certero y otro. Cuanto mayor sea la diferencia de tiempo entre estos datos certeros, mayor es la probabilidad de que el objeto siga una ruta errónea. En tal caso se corregiría la posición del objeto. \\

Precisamente esto provoca un efecto común en aplicaciones de este tipo, concretamente cuando existen latencias altas en la red. Un ejemplo común y que no necesita demasiada latencia sucede en los juegos de tipo Shooter, donde es muy probable que cada jugador vea el cuerpo caído del otro en posiciones distintas. Sin embargo, cada jugador está en un ordenador disntinto y la fidelidad de dónde cae cada cuerpo no es realmente relevante (especialmente si nunca ves el monitor de tu contrario). Otro efecto común es ver que los objetos se deplazan de forma constante durante un tiempo prolongado y se repente desaparecen y se colocan en la posición real. Dead reckoning no ofrece información fiel sobre la posición de los objetos. Sin embargo, como los usuarios no tienen la capacidad de contrastar no se considera relevante.

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/dead_reckoning.png}
\end{center}
\caption{ \label{F_Implementacion_TerceraDemo_deadreckoning} Implementación, Tercera Demo. Navegación - Dead Reckoning.}
\end{figure}

Como se ha podido apreciar, este mecanismo nos permite estimar posiciones intermedias entre datos certeros. Sin embargo, también se ha podido ver que existe un problema, especialmente para poder aplicarlo en nuestro sistema: nosotros si veremos directamente que nuestro movimiento y el estimado por el dead reckoning pueden no ser fieles. La estimación se permitirá a nivel del navegador, en relación a atributos a considerar en el futuro pero no en relación a la velocidad y a la aceleración. Estás estimaciones se harán al nivel del motor gráfico. \\

La solución final a adoptar para resolver la navegación consiste en:

\begin{itemize}
\item Navigator controller mantiene diferentes componentes del movimiento de las cámaras: centro de masas, posición relativa de la cabeza, distancia sobepasada al humbral.
\item Inicialmente sólo envía al motor gráfico una única coordenada, pero potencialmente se plantea que puede añadir más información sobre la estimación de su comportamiento.
\item Cada vez que el navegador establece la posición certera, el motor gráfico calcula la velocidad y aceleración correspondiente entre el último frame y el actual. El motor gráfico interpolará entre las posiciones certeras enviadas por el controlador de navegación usando y velocidad y aceleración. A su vez la velocidad sera también interpolada entre frames a partir de la aceleración calculada entre datos certeros.
\end{itemize}

\subsubsection{Módulo de Persistencia}
scr$\backslash$ipersistence$\backslash$EntityPersistence \\

Los cambios ejecutados se reducen a obtener dos atributos de la entidad. Se trata de saber si la entidad es considerada interactiva, es decir si va a producir acciones o responder a las acciones de otros, o por el contrario no se va ver afectada. Por otro lado, averiguar si sólo va a responder a las acciones o si también será capaz de provocarlas (activa). \\

Estas opciones se encuentran codificadas en el atributo llamado psique que ya posee la entidad y que describe el comportamiento de la misma. MEdiante el uso de máscaras binarias es posible extraer de forma rápida información del mismo.

\begin{lstlisting}[language=C++]            
bool EntityPersistence::IsInteractive()
{ return psique & IS_INTERACTIVE;   }
bool EntityPersistence::IsActive()
{ return psique & IS_ACTIVE;   }
\end{lstlisting}

\subsubsection{Módulo de GUI}
scr$\backslash$igui$\backslash$GUIUser \\

Se realizan cambios con el objetivo de mostrar de mostrar la cara detectada que está siendo objeto de estudio actualmente. En caso de que no se detecte ninguna cara o se hayan desactivado las opciones de reconocimiento se mostrará una imagen por defecto.\\

Para hacer esto se mantienen dos bitmaps. El primero será la imagen por defecto, que se mostrará siempre que no se detecte una cara o no se quiera usar reconocimiento. La segunda, se mostrará en caso de que se desee usar reconocimiento y será la última imagen enviada por la aplicación para su reconocimiento.\\

La primera se cargará al construirse la interfaz, junto con el resto de componentes de la misma, en el constructor de la clase, pero aún no será visible.

\begin{lstlisting}[language=C++]            
GUIUser::GUIUser(...)
{ ...
  no_face_bitmap = new wxBitmap( bitmap.str(), wxBITMAP_TYPE_ANY);
  ... }
\end{lstlisting}

La segunda se cargará cada vez que sea enviada. La imagen se escalará para encajar en el recuadro, y tampoco será visible. En realidad sólo se está cargando o creando el objeto imagen, no el componente gráfico que lo dibuja. Esto se realizará manualmente en la función render().\\

En este caso, cuando nos lleva una nueva imagen de una cara, nos interesa forzar que se actualice la interfaz, ya que si no, no se vería el cambios realizado hasta que wxWidgets lo considerara necesario (pe. cuando detecta el área dañada al pasar otra ventana sobre ésta). Se usará el método Update para poner la interfaz en la cola de actualización. Sin embargo, esto tampoco sucederá al instante, sino que puede suceder en un período de tiempo prolongado. Para que la actualización sea inmediata es necesario usar el método Refresh justo antes de Update. De esta forma se forzará que la interfaz se redibuje al momento. 

\begin{lstlisting}[language=C++]            
void GUIUser::SetFace(char* data, const int &size_x, 
                      const int &size_y, 
                      const int &n_channels, const int &depth, 
                      const int &width_step)
{ if (data)
  { wxImage new_image = wxImage( size_x, size_y, (unsigned char*)data, true );
    face_bitmap = wxBitmap( new_image.Scale(100, 100, wxIMAGE_QUALITY_HIGH) );
    face_detected = true;
  } else face_detected = false;
  Refresh();
  Update();  }
\end{lstlisting}

Para terminar con esta interfaz, se añaden métodos para rellenar de forma automática los campos de inicio de sesión o de registro de un nuevo usuario. La aplicación actualizará con cierta frecuencia las caras que ve vayan detectando. En el caso de que el usuario fuera reconocido la aplicación rellenará los campos de la parte del login. Si en caso contrario, el usuario no ha sido reconocido durante un periodo de tiempo adecuado y la aplicación considera que debe añadirse uno nuevo, se rellenarán los campos del nuevo usuario. \\

Hay que tener en cuenta que si además de Reconocimiento, la aplicación tiene activado el autologin, ésta iniciará sesión y cargará la última escena creada, por lo que esta interfaz será sustituía por la interfaz de información del usuario (con la lista de escenas y propiedades).

\begin{lstlisting}[language=C++]            
void GUIUser::FillLoginUserGUI(const std::string username, 
                               const std::string userpasswd)
{ if (user_name)
    user_name->ChangeValue(username);
  if (user_passwd)
    user_passwd->ChangeValue(userpasswd);	    }
    
void GUIUser::FillNewUserGUI(const std::string username, 
                             const std::string userpasswd)
{ if (new_user_name)
    new_user_name->ChangeValue(username);
  if (new_user_passwd)
    new_user_passwd->ChangeValue(userpasswd);	
  if (new_user_passwd2)
    new_user_passwd2->ChangeValue(userpasswd);	}
\end{lstlisting}

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/gui_user_recon.png}
\end{center}
\caption{ \label{F_CapturaFase4TerceraDemo_GUIUser} GUI: Panel de Log in}
\end{figure}

scr$\backslash$igui$\backslash$GUIConfiguration \\

En el desarrollo de las demos anteriores ya se habían introducido la mayor parte de los paneles de interfaces gráficas y se habían conectado con la configuración de la aplicación. A través de estos paneles se podían configurar las cámaras y los displays que usará la aplicación.  En este caso, se se añadirán algunas características nuevas. \\

Se añaden dos nuevos paneles a la colección de paneles de la interfaz gráfica de configuración. El primero de ellos será el de opciones generales, que inicialmente contendrá el idioma de la aplicación, ofreciendo un cuadro desplegable que muestre los idiomas disponibles. Actualmente, no se ha implementado este apartado, aunque ya se ha hecho en otras ocasiones. El segundo, que si se ha implementado en esta fase, se corresponderá con las nuevas opciones de visualización, actualmente sólo relacionadas con el módulo de percepción. Al tratarse de meros indicadores para mostrar si se desea o no visualizar una determinada característica se usarán componentes de tipo checkbox. \\

Hay que tener en cuenta que activar estas opciones exigirán más trabajo para mostrar datos actualizados en cada iteración. Se debe tener en cuenta que no se mostrarán todos los resultados a la frecuencia a la que se obtienen. Aun así, activar estas opciones puede provocar un efecto negativo en el rendimiento de los componentes que se vean involucrados.

\begin{lstlisting}[language=C++]            
void GUIConfiguration::InitVisualizationPanel()
{ int width, height;
  GetClientSize(&width, &height);
  visulization_panel = new wxPanel(panel_book, wxID_ANY);
  if (app_config != NULL)
  { visualization_st = new wxStaticText(visulization_panel, wxID_ANY, 
                       _("These options may slow down your computer. "),...);
    cam_capture      = new wxCheckBox(visulization_panel, wxID_ANY, 
                       _("Cam capture"), wxPoint(col, row+20));                       
    homography       = new wxCheckBox(visulization_panel, wxID_ANY, 
                       _("Homography"), wxPoint(col, row+40));
    face_detection   = new wxCheckBox(visulization_panel, wxID_ANY, 
                       _("Face Detection"), wxPoint(col, row+60));
    foreground       = new wxCheckBox(visulization_panel, wxID_ANY, 
                       _("Foreground"), wxPoint(col, row+80));
    motion           = new wxCheckBox(visulization_panel, wxID_ANY, 
                       _("Motion"), wxPoint(col, row+100));	 } }
\end{lstlisting}

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/gui_configurepanel_visualization.png}
\end{center}
\caption{ \label{F_CapturaFase4TerceraDemo_GUIConfiguration_Viszualization} GUI: Panel de Configuración - Opciones de Visualización}
\end{figure}


scr$\backslash$igui$\backslash$MainGUI \\

EL módulo de GUI atenderá peticiones para alternar entre las interfaces de usuario así como para completar información de forma automática y ejecutar acciones. Anteriormente, la interfaz GUIUser respondía a los eventos que provocaba el usuario sobre los botones de Login y Register, que haciendo uso del controlador de la interfaz terminaba solicitando a la Aplicación las acciones correspondientes. \\

Como se había comentado en la sección de interfaces, la Aplicación dispone de los mecanismos subyacentes necesarios para iniciar una sesión y cargar escenas. Sin embargo, no era capaz de modificar las interfaces en consecuencia. Para ello, y por simplicidad se han añadido método para permitir estas acciones que se han encauzado con el flujo de ejecución ya existente. \\

Por otro lado, se han añadido nuevas opciones de Configuración que el usuario puede activar a través de los paneles gráficos. Estas acciones también deben dirigirse a sus responsables, que en este caso será la aplicación. \\

De cualquier forma, MainGUI sólo encauzará peticiones a los componentes correspondientes. Se muestran a modo de ejemplo Calibrate(), FillLoginUserGUI y ShowFaceAtGUI. El resto de opciones se implementan de la misma forma:

\begin{lstlisting}[language=C++]            
void MainGui::Calibrate()
{ if (app != NULL)
  app->Calibrate(); }

void MainGui::FillLoginUserGUI(const std::string username, 
                               const std::string userpasswd)
{ if (main_frame)
   main_frame->FillLoginUserGUI(username, userpasswd); }
  
void MainGui::ShowFaceAtGUI(char* data, const int &size_x, 
                            const int &size_y, 
                            const int &n_channels, 
                            const int &depth, 
                            const int &width_step)
{ if (main_frame != NULL)
   main_frame->SetFace(data, size_x, size_y, 
                       n_channels, depth, width_step); }  
\end{lstlisting}


scr$\backslash$igui$\backslash$MainFrame \\

De forma similar, MainFrame sólo redirige las peticiones a los componentes correspondientes, en este caso se actualiza cada interfaz de usuario, en caso de ser necesario. Todas las opciones se implementan de forma similar, por lo que se muestran sólo algunas de ellas como ejemplo.

\begin{lstlisting}[language=C++]            
void MainFrame::SetFace(char* data,const int &size_x,const int &size_y, 
                        const int &n_channels, const int &depth, 
                        const int &width_step)
{ if (user_panel && user_panel->IsShown())
   user_panel->SetFace(data, size_x, size_y, n_channels, ...); }

void MainFrame::FillLoginUserGUI(const std::string username, 
                                 const std::string userpasswd)
{ if (user_panel)
   user_panel->FillLoginUserGUI(username, userpasswd); }  
\end{lstlisting}

\subsubsection{Módulo de Producción}\label{etapa4Implementacion:produccion}

scr$\backslash$iprod$\backslash$MainProd \\

Dentro de este módulo se trabajará en dos apartados distintos: Primero, se verán las modificaciones necesarias para permitir la manipulación de las cámaras y que el controlador de navegación pueda utilizarlo. En segundo lugar, se comprobará cómo añadir detección de colisiones entre las entidades de la escena.

\begin{itemize}
\item Navegación: \\

Como se describe en el apartado de Implementación que hace referencia al controlador de navegación, en la página \pageref{F_Fase4TerceraDemo_ControladorNavegacion}, dicho controlador establecerá la posición de las cámaras con frecuencia. También se comenta que como esta frecuencia será con toda probabilidad menor a la frecuencia de render, se ha tomado como solución realizar una interpolación entre los dos valores certeros inmediatamente anteriores, utilizando estimaciones de velocidad y aceleración para conseguir un movimiento suave. \\

En primer lugar, cada vez que el Controlador de Navegación establece una nueva posición se certera se actualizan todos los valores. Se toma el intervalo de tiempo entre dos iteraciones, y a partir del valor actual y los valores previos, se integra para calcular velocidad y aceleración. \\

Los valores de pt0 y pt1 representan la ventana de estudio (o diferencial), pt0 será el instante inicial y pt1 el final (la posición certera actual). El valor de pti representa el último punto interpolado, que pasara a ser el nuevo valor de pt0. \\

A partir de estos dos valores de posición se calcula un nuevo valor de velocidad vel1, siendo vel0 de forma análoga el valor previo más reciente de velocidad. Se establece como velocidad actual la recién calculada. \\

Finalmente, a partir de vel0 y vel1 se calcula el nuevo valor de la aceleración.\\

\begin{lstlisting}[language=C++]            
void MainProd::SetCamerasPosition(const core::corePoint3D<double> &pos)
{ boost::mutex::scoped_lock lock(m_mutex);
  if(initialized)
  { double time = globalClock->get_real_time();
    double interval = time - last_time;
    pt0    = pti;
    pt1.x  = pos.x; 
    vel0   = vel;
    vel1.x = pt1.x - pt0.x;
    vel1.x = vel1.x/interval; 
    vel    = vel1;
    acc.x  = vel1.x - vel0.x; 
    acc.x  = acc.x/interval;
    ...  }}
\end{lstlisting}

Por otro lado, en cada iteración del bucle principal, en cada render, se vuelven a calcular los valores de posición y velocidad, en relación a los valores de velocidad y aceleración actuales respectivamente. Esto es lo que provee el efecto de inercia del movimiento de la cámara. Como detalle, se usa un factor de conversión de unidades espaciales.

\begin{lstlisting}[language=C++]            
void MainProd::DoDoStuff()
{ double time = globalClock->get_real_time();
  double interval = time - last_loop_t; 
  last_loop_t = time;
  vel.x = vel.x + interval*(acc.x);
  pti.x = pti.x + 10*interval*(vel.x);
  for(std::map<int, NodePath>::iterator 
           iter  = windowcamera_array.begin(); 
           iter != windowcamera_array.end(); iter++)
    iter->second.set_pos(pti.x/factor, pti.y/factor, pti.z/factor);  
\end{lstlisting}

Es evidente que esta solución implica un retardo en los valores aplicados a la posición de la cámara, dado que se basan inevitablemente en los valores de instantes anteriores. Sin embargo, la suavidad de movimiento conseguida y el efecto de inercia es interesante y además demuestra ofrecer un comportamiento aceptable. \\

\item Colisiones: \\

A continuación se hablará de la solución adoptada para la detección de colisiones entre entidades de la escena, que igualmente puede ser aplicado para el avatar del usuario. Si bien es cierto que se peuden usar motor de colisiones e incluso de fisicas externos, se ha decidido usar el módulo de colisiones ya incorporado en el propio motor de juego de Panda3D. El motivo es que Panda3D ya ofrece una buena solución integrada y para el propósito que se desea estudiar cubre todas las necesidades. Además al usar un sistema ya integrado no sera necesario mantener dos esquemas a la vez por separado: el de las entidades por un lado y el las colisiones por otro. \\

Existen varias formas de detectar colisiones: \\

\begin{itemize}
\item Las más potentes están basadas en las geometrías de los objetos. Aunque esto puede ofrecer ventajas evidentes en el detalle al que se pueden producir las colisiones, es una opción muy costosa que hay que usar con moderación. No se descarta su uso futuro para el caso del avatar del usuario pero por el momento no se considerará. \\

\item La otra solución es usar geometrías adicionales llamadas colisionadores, que se añaden como hijas al nodo de la entidad. Estas serán geometrías simples, y po defecto invisibles, que envolverán al objeto y simplificará el cálculo de las mismas. Por ejemplo, en el caso de usar esferas como colisionadores, bastará con calcular la distancia entre los centro de ambas geometrías y comprobar que sea mayor que la suma de sus radios. La mayor ventaja de esta solución es la rapidez de cómputo, que nos peude permitir tener muchas más entidades susceptibles de provocar colisiones que por ejemplo en el caso anterior. \\

Recordemos que la escena en los motores gráficos se suele representar como un árbol, en este árbol, los nodos pueden albergar geometrías o matrices de transformación. Por ejemplo, las entidades se pueden ver representadas como un nodo. En este punto, un nodo puede tener descendencia, y es de esta forma que las transformaciones realizadas sobre él se ven reflejadas en todo el subárbol. De esta manera, sólo con añadir la nueva geometría como hija de la entidad, su posición, así como todas las demás transformaciones, se aplicarán sobre la misma (si movemos la entidad, el colisionador se moverá con ella).
\end{itemize}

Por motivos de rendimiento y capacidad potencial de añadir mayor cantidad de entidades susceptibles de colisionar, se ha decidido utilizar la segunda solución como opción por defecto. En concreto se usarán esferas como geometrías de colisión. \\

Llegados a este punto estudiaremos el esquema de colisiones del motor gráfico. Además de las geometrías de colisión, el motor gráfico necesita un objeto que sea capaz de recorrer el grafo de la escena en busca de entidades susceptibles de colisionar. Un objeto que explora el grafo en busca de entidades se conoce como traverser, en este caso será uno para colisiones. También sera necesario tener una cola de colisiones, donde el traverser insertará en cada frame todas las colisiones detectadas y la información asociada. \\

Un detalle a tener en cuenta es que, aunque Panda3D tiene una de las APIs mejor diseñadas que se han podido ver, y ofrece herramientas y resultados por encima de la media en el campo de los motores libres, se ha comprobado que en la versión disponible para C++ sufre de ciertas carencias. Entre estas, ha demostrado que algunos componentes no están blindados para su uso en sistemas multihilos. Una de las recomendaciones es ejecutar la mayor parte del código posible en el hilo del módulo de producción. También se ha comprobado que algunos módulos no funcionan correctamente en los modos de Debug o Release. Por ejemplo, la detección de colisiones dan problemas en modo Debug.\\

Para crear el explorador y la cola de colisiones se ejecutan las siguientes líneas de código:

\begin{lstlisting}[language=C++]            
void MainProd::DoMainLoop()
{ ...
  #ifndef _DEBUG
  if (!collision_handler_queue)
   collision_handler_queue = new CollisionHandlerQueue();
  if (!collision_traverser)
   collision_traverser = new CollisionTraverser();
  #endif
\end{lstlisting}

Hemos hablado de entidades y de dotarlas de capacidad para detectar colisiones. Sin embargo, no todas las entidades serán susceptibles de provocarlas o recibirlas. Es posible que se desee tener entidades en la escena que no colisionarán, u otras que sólo responderán a las colisiones provocadas por determinados objetos, pero que no sean capaces de provocar las colisiones. Llegado aquí, es importante recordar de nuevo que el cálculo de colisiones es computacionalmente elevado y depende geométricamente del número de objetos que se consideran capaces de generar colisiones, ya que es necesario comprobar cada uno con todos los demás. Si todas las entidades provocaran colisiones seria de orden exponencial. \\

La solución que se suele adoptar, y que Panda3D también incorpora es separar los colisionadores en 2 tipos: Por un lado, están los colisionadores capaces de provocar una colisión, lo que que llaman 'fromObjects'; y por otro lado, están todos las demás, incluídos los 'fromObjects'. A esta segunda colección la llaman 'intoObjects'. \\

En cada iteración el traverser consultará cada elemento que exista en 'fromObjects' con todos los elementos que existan en 'intoObjects'. Evidentemente interesa mantener el numero de objetos del primer grupo lo más bajo posible. En su forma más común, sólo existirá un objeto en fromObjects, que será el propio avatar del usuario, pero otros objetos son susceptibles de ser fromObjects, como por ejemplo la munición disparada por un arma. Hay que tener en cuenta que si dos objetos de 'intoObjects' que no pertenecen a 'fromObjects' colisionan, no se detectará. \\

Para añadir una geometría de colisión a una entidad de la escena, basta con crear un objeto de tipo colisionadores y añadirlo como hijo al nodo de la entidad.

\begin{lstlisting}[language=C++]            
 CollisionSphere *coll_solid = new CollisionSphere(0,0,0,radius);
 CollisionNode *coll_node = new CollisionNode(nombre);
 coll_node->add_solid(coll_solid);
 NodePath coll_nodePath = entity->GetNodePath()
                                ->attach_new_node(coll_node);
\end{lstlisting}

Con esto, la geometría de colisión ya formará parte de los objetos en 'intoObjects'. Si además se desea insertarlo en la colección 'fromObjects', bastará con añadirlo como tal en el collision-traverser:

\begin{lstlisting}[language=C++]            
 collision_traverser->add_collider(coll_nodePath, 
                                   coll_handler_queue);
\end{lstlisting}

Dentro de la catalogación de entidades que se adopta en el proyecto 'intoObjects' se corresponderá con las entidades Interactivas, y 'fromObjects' from las entidades Activas. PAra simplificar la creación de geometrias de colisión, y asegurar que se ejecutan siempre en el hilo del módulo de producción, la carga de entidades (que se ejecuta en el hilo principal) añadirá las nuevas entidades a un vector que se comprobará en el método de comprobación de colisiones. Será aquí donde se creen las geometrías y se inserten en fromObjects o intoObjects en caso de necesidad.\\

\begin{lstlisting}[language=C++]            
void MainProd::CheckCollisions()
{ collision_traverser->traverse(pandawin_array[1]->get_render());
  for (int i = 0; i < coll_handler_queue->get_num_entries(); i++)
   cout << "Entry: " << coll_handler_queue->get_entry(i) << "\n";
  for (std::vector< Prod3DEntity * >::iterator iter =
            entity_collidable_array_to_register.begin(); 
            iter != entity_collidable_array_to_register.end();
            iter++)
  { if( coll_traverser && (*iter)->IsInteractive ) //intoObject
    { NodePath* np = (*iter)->GetNodePath();
      PT(BoundingSphere)bs=DCAST(BoundingSphere, np->get_bounds());
      int radius = bs->get_radius();
      CollisionSphere *coll_solid = new CollisionSphere(0,0,0,radius);
      CollisionNode *coll_node = new CollisionNode(nombre);
      coll_node->add_solid(coll_solid);
      NodePath coll_nodePath = (*iter)->GetNodePath()
                                      ->attach_new_node(coll_node);
      coll_nodePath.show(); //shows the collider's geometry
      entity_collider_array[(*iter)] = collision_node;
      if ( (*iter)->IsActive() ) //also a fromObject
       coll_traverser->add_collider(coll_nodePath, coll_handler_queue);
... }
\end{lstlisting}
\end{itemize}

\subsubsection{Módulo de Percepción}\label{Etapa4_Impl:Percepcion}

Llegamos al último módulo en que se han realizado cambios durante esta fase. Derivado de los cambios realizados en el apartado de implementación de las interfaces, se implementarán los  métodos correspondientes tanto a nivel de MainPercept como de PerceptVideo. El objetivo de estos métodos es dar acceso a las nuevas funcionalidades que aportarán los detectores que se desarrollarán. 

scr$\backslash$ipercept$\backslash$MainPercept \\

Se añaden los métodos que permitirán acceder a los datos producidos por los diversos detectores, así como para poder  configurar los mismos. MainPercept no añadirá ninguna lógica especial, sólamente encauzará la petición a los componentes responsables, en este caso PerceptVideo.

\begin{lstlisting}[language=C++]            
namespace core{ namespace ipercept
{ class MainPercept : public core::IPercept
  { public:
    ...
    virtual void Calibrate(const bool &value);
    virtual void CalculateHomography();
    virtual void TrainBackground();
    virtual bool MainPercept::PresenceDetected();
    virtual bool MainPercept::FaceDetected();
    virtual void GetHeadPosition(corePoint3D<double> &result);
    virtual void GetFeaturePosition(const std::string &feature, 
                                    corePoint3D<double> &result);
    virtual void GetSpaceBoundingBox(corePoint3D<double> &min, 
                                     corePoint3D<double> &max);
    virtual char * GetCopyOfCurrentFeature(const std::string &feature, 
                   int &size_x, int &size_y, int &n_channels, 
                   int &depth, int &width_step, 
                   const bool &switch_rb = false);
    virtual int  RecognizeCurrentFace();
    virtual void AddNewUserToRecognition(const int &user_id);
    virtual bool IsFacePoolReady();
\end{lstlisting}

A modo de ilustración se muestra el caso de la implementación del método Calibrate(). El resto se llevan a cabo de forma similar.

\begin{lstlisting}[language=C++]            
void MainPercept::Calibrate(const bool &value)
{ if (perceptVideo_module != NULL)
  perceptVideo_module->Calibrate(value); }
\end{lstlisting}

scr$\backslash$ipercept$\backslash$PerceptVideo \\

Recordemos que PerceptVideo es el componente principal del módulo de percepción, responsable de las tareas relacionadas con el procesado de imágenes. Se encarga de obtener imágenes de los dispositivos de captura, así como de mantener y gestionar los distintos procesos o detectores que se incluyan. Desde este punto de vista, PerceptVideo también encauzará las peticiones a los componentes responsables, pero además también realiza operaciones de forma genérica y necesita mantener cierta gestión. \\

Se separarán los componentes en elementos de proceso general y detectores/reconocedores. Los primeros realizan preproceso o postproceso sobre las imágenes para realizar correcciones o permiter transformar datos sobre ellas, a las que llamaremos Herramientas. Los segundos están destinados a obtener características concretas de las imágenes realizando un estudio sobre las mismas, a las que llamaremos Detectores. \\

En Herramientas localizamos la Captura de Imágenes (ya implementada en fases anteiores), la Calibración y el Cálculo de las matrices de Homografia. Para cada cámara conectada al sistema existirá una instancia de los siguientes componentes: Captura de Imágenes, Calibración, Cálculo de Homografías, Detector de Caras, Presencia y Movimiento. Por otro lado, sólo existirá un único Reconocedor de caras. \\

\begin{itemize}
\item Calibración: \\

Se trata de corregir la deformación que sufre la imagen debido a las características del dispositivo de captura. En la siguiente figura puede verse la expresión de transformación de perspectiva, que convertiría un punto del espacio 3D al espacio 2D.

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/formula_calibrate_intrinsics.png}
\end{center}
\caption{ \label{F_Formula_IPercept_Calibrate_Intrinsics} PerceptVideo: Transformación de Perspectiva. }
\end{figure}

En ella puede verse cómo convertir de un punto (X,Y,Z) en el espacio tridimentional, al correspondiente (u,v) del espacio imagen, 2D. El primer detalle que nos interesa es la matriz A, llamada matriz de parámetros intrínsecos, y que efectivamente guarda parámetros intrínsiecos de la cámara. los valores de fx, fy representan la distancia focal, siendo cx, cy los valores del punto principal del foco (comúnmente, el centro de la imagen). Siempre que la lente sea fija o no se use el zoom, estos parámetros permanecerán constantes. \\

La matriz Rt, representa los parámetros extrínsecos y es la conjugación de transformaciones de Rotación-Traslación, representa el movimiento de un punto entre un sistema y otro.  \\

La expresión anterior puede expresarse también como:

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/formula_calibrate_2.png}
\end{center}
\caption{ \label{F_Formula_IPercept_Calibrate_2} PerceptVideo: Transformación de perspectiva 2. }
\end{figure}

Sin embargo, las lentes reales añaden cierta distorsión que principalmente es radial, pero también existe una componente tangencial. Añadiendo estos coeficientes la expresión quedaría de la siguiente forma:

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/formula_calibrate_intrinsics2.png}
\end{center}
\caption{ \label{F_Formula_IPercept_Calibrate_2} PerceptVideo: Transformación de perspectiva 3 - coeficientes de distorsión }
\end{figure}

Los valores k1,k2,k3 son los coeficientes de distorión radiales, y p1,p2 los coeficientes de distorsión tangenciales. Al igual que al distancia focal, son valores propios de la cámara que no van a cambiar y se incluyen en el conjunto de coeficiente intrínsecos de una cámara, \cite{CALIBCV}. \\

La tarea de la calibración consiste en calcular estos valores y aplicarlos para corregir la distorsión que produce la cámara sobre la imagen. Como son valores estáticos que no cambiarán, pueden guardar en ficheros de configuración que se cargará al inicio de la aplicación.

\begin{lstlisting}[language=C++]            
PerceptVideo::PerceptVideo(IApplicationConfiguration *app_config_)
{ ...
  intrinsics[i+1] = (CvMat*)cvLoad( intrinsics_filename );
  distortion[i+1] = (CvMat*)cvLoad( distortion_filename );
  capture_img[i+1] = cvQueryFrame(capture_cam_array[i+1]);
  undistort_mapx[i+1] = cvCreateImage( cvGetSize(capture_img[i+1]),
                                       IPL_DEPTH_32F, 1);
  undistort_mapy[i+1] = cvCreateImage( cvGetSize(capture_img[i+1]), 
                                       IPL_DEPTH_32F, 1);
  if( (intrinsics[i+1]) && (distortion[i+1]) )
   cvInitUndistortMap( intrinsics[i+1], distortion[i+1], 
                       undistort_mapx[i+1], undistort_mapy[i+1] );  
\end{lstlisting}

Sin embargo, el cálculo de los parámetros es un proceso elaborado que requiere estudiar una sucesión de imágenes capturadas y el uso de un patrón (tablero de ajedrez), para ser capaz de obtener puntos de referencia y estudiar su relación. A continuación se mostrará un resumen simplificado. Si se desea ver en mayor detalle puede consultarse el Anexo ??. \\

En primer lugar se establecer los parámetros del patrón a detectar. Se trata de un tablero de ajedrez de nueve filas por seis columnas. Esta forma nos permitirá no solo poder detectar un patron reconocible sino también poder definir la orientación del mismo, ya que no seria simétrico.\\

Dentro de un bucle se pedirá al usuario que muestre el tablero y se irán añadiendo los puntos del espacio imagen y los puntos del objeto estudiado, por cada tablero detectado con éxito. \\

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/calibrating_chessboard.png}
\end{center}
\caption{ \label{F_IPercept_Calibrate_Chessboard} PerceptVideo: Marcadores Tablero }
\end{figure}

\begin{lstlisting}[language=C++]  
if( corner_count == board_n )
{ step = successes*board_n;
  for( int i=step, j=0; j < board_n; ++i, ++j ){
   CV_MAT_ELEM( *image_points, float, i, 0 ) = corners[j].x;
   CV_MAT_ELEM( *image_points, float, i, 1 ) = corners[j].y;
   CV_MAT_ELEM( *object_points, float, i, 0 ) = j/board_w;
   CV_MAT_ELEM( *object_points, float, i, 1 ) = j%board_w;
   CV_MAT_ELEM( *object_points, float, i, 2 ) = 0.0f; }
  CV_MAT_ELEM( *point_counts, int, successes, 0 ) = board_n;
  successes++; }
\end{lstlisting}

Se inicializan las matrices de valores intrínsecos de forma que se inicializan con una distancia focal de 1. A continuación se calculan los coeficientes y finalmente se almacenan en un fichero de configuración, para que la próxima vez que se inicie la aplicación sean cargados. Finalmente, se crean los mapas de corrección de distorsión que serán necesarios para corregir la misma en las imágenes capturadas.

\begin{lstlisting}[language=C++]            
 CV_MAT_ELEM( *intrinsic_matrix, float, 0, 0 ) = 1.0;
 CV_MAT_ELEM( *intrinsic_matrix, float, 1, 1 ) = 1.0;
 cvCalibrateCamera2( object_points2, image_points2, 
                     point_counts2, cvGetSize( image ), 
                     intrinsic_matrix, distortion_coeffs, 
                     NULL, NULL, CV_CALIB_FIX_ASPECT_RATIO ); 
intrinsics[iter->first] = intrinsic_matrix;
distortion[iter->first] = distortion_coeffs;
intrinsics_filename << app_config->GetRootDirectory() 
                    << "Intrinsics" << iter->first << ".xml";
cvSave( intrinsics_filename.str().c_str(), intrinsic_matrix );
undistort_mapx[iter->first] = cvCreateImage( cvGetSize( image ), 
                                             IPL_DEPTH_32F, 1 );
undistort_mapy[iter->first] = cvCreateImage( cvGetSize( image ), 
                                             IPL_DEPTH_32F, 1 );
cvInitUndistortMap( intrinsic_matrix, distortion_coeffs, 
                    undistort_mapx[iter->first], 
                    undistort_mapy[iter->first] ); 
\end{lstlisting}

Todas las imágenes a estudiar en el módulo de percepción se harán tras haber corregido la distorsión de las mismas. En cada iteración se captura una imagen por cada cámara web conectada. Inmediatamente después se le aplica la trasformación de forma que el procesado futuro se haya con las imágenes corregidas. Para aplicar la transformación, basta con aplicar los mapas de corrección de distorsión, como se muestra a continuación. Siendo 't' la imagen de estudio distorsionada de origen e 'image' la imagen destino. De hecho, PerceptVideo mantiene un vector con la última imagen capturada por la cámara correspondiente. Esta misma imagen es la que se usarán como base en los trabajos posteriores, y como se ha comentado ya tendra la corrección aplicada.

\begin{lstlisting}[language=C++]
cvRemap( t, image, undistort_mapx[iter->first], 
                   undistort_mapy[iter->first] );  
\end{lstlisting}

Antes de concluir este apartado se comenta la forma en la que se visualizarán los datos de interés de los componentes de este módulo de percepción. Todos los componenetes cuyos resultados nos interese visualizar insertarán ventanas en un vector de ventanas de depuración. Cuando el usuario indica que desea mostrar alguna en cuestión, esta venta se crea y se volcarán las imágenes en ellas, y si no se destruyen. Por motivos de eficiencia es en cada iteración, dentro de la función ShowDebugWindows() donde se recorren las ventanas que se desean mostrar, se solicitan las imágenes a los componentes correspondientes y se actualizan en pantalla. Hay que tener en cuenta que es posible que la petición a los componentes no sea antendida, debido a que se hace uso de hilos no bloqueantes para la sincronización, en tal caso la imagen se conservará como la del frame anterior.


\begin{lstlisting}[language=C++]
void PerceptVideo::ShowDebugWindows()
{ for (std::map<int, CvCapture *>::iterator 
       iter = cam_array.begin() ...)
  { IplImage * image = capture_img[iter->first];
    if(show_cam_capture)
    { std::map< std::string, CamWindow* >::iterator cam_iter =
                       camWindow_array.find(window_name.str());
      cam_iter->second->ShowImage(image);  }//already undistorted
\end{lstlisting}

\item Cálculo de Homografías:\\

Una homografia es una matriz de transformación invertible que nos permite convertir coordenadas de un plano de proyección real al plano de proyección de estudio. Establecerá una coalineación entre un punto de un plano en el espacio 3D a un punto en el espacio imagen. La gran ventaja es que la misma matriz puede ser usada en ambos sentidos.

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/homography_diagram.png}
\end{center}
\caption{ \label{F_IPercept_Calibrate_Homography_Diagram} PerceptVideo: Homografía \cite{HOMOGF}}
\end{figure}

Mediante el uso de homografías es posible estudiar la situación de un objeto en planos no frontales a la cámara corrigiendo la deformación de la perspectiva de la vista. Sin embargo, hay que tener en cuenta que deformará la imagen capturada y que debe usarse teniendo en cuenta esta consideración. \\

El análisis se hará siempre en el espacio imagen. Una vez realizado, se convertirán las coordenadas de imagen del punto de estudio en cada caso, con el espacio transformado. \\

Por el momento, la solución adoptada para el alcance actual de este proyecto no contempla visión en estéreo, que será una línea interesante que se propone como trabajo futuro. En cambio a través del panel de configuración de las cámaras es posible asignar qué plano de estudio corresponde a cada cámara. La intención es hacerlo de una forma más genérica, pero actualmente se podrá asignar un plano que corresponderá a cada cara del cubo que envuelve el espacio de la instalación. \\

Se usará la homografía para corregir la perspectiva dado que probablemente el plano de estudio no coincida completamente con el de la imagen (por ejemplo al colocar una cámara frontal a cierta altura). \\

Según las condiciones asumidas en tabla comentada en \ref{F_Typical_Assumptions_Etapa4}, página \pageref{F_Typical_Assumptions_Etapa4}, las cámaras permanecerán estáticas en la instalación. Teniendo en cuenta esta característica, una vez calculas las matrices de homografia de las distintas cámaras será posible almacenarlas en un fichero de configuración que ahorrará tener que volver a calcularlas en próximas ejecuciones. \\

\begin{lstlisting}[language=C++]
 homographies_filename << app_config->GetRootDirectory() 
                       << "Homography" << i+1 << ".xml";
 homography[i+1] = (CvMat*)cvLoad( homographies_filename );
\end{lstlisting}

El cálculo de la homografia se realiza de forma similar al caso anterior. Se entrará en un bucle donde se localizará un patrón de ajedrez, cuyas características han sido definidas. Hay que tener en cuenta que es relevante poder identificar la orientación. \\


En cada iteración se buscará el patrón de tablero de ajedrez y se dibujarán marcadores. Además de los marcadores tradicionales se añaden unas circunferencias para que el usuario sea consciente de la orientación del plano. \\

\begin{lstlisting}[language=C++]
while( successes < n_boards )
{ if( frame++ % board_dt == 0 ) {
 int found = cvFindChessboardCorners( image, board_sz, corners, 
             &corner_count,  CV_CALIB_CB_ADAPTIVE_THRESH | 
             CV_CALIB_CB_FILTER_QUADS );
 cvCvtColor( image, gray_image, CV_BGR2GRAY );
 cvFindCornerSubPix( gray_image, corners, corner_count, 
               cvSize( 11, 11 ), cvSize( -1, -1 ), 
               cvTermCriteria( CV_TERMCRIT_EPS+CV_TERMCRIT_ITER, 
               30, 0.1 ));
 cvDrawChessboardCorners(image,board_sz,corners,corner_count,found);				
 origin.x = corners[0].x; origin.y = corners[0].y;
 xpoint.x = corners[board_w-1].x; xpoint.y = corners[board_w-1].y;
 ypoint.x = corners[(board_h-1)*board_w].x; 
 ypoint.y = corners[(board_h-1)*board_w].y;
 xypoint.x = corners[(board_h-1)*board_w + board_w-1].x; 
 xypoint.y =  corners[(board_h-1)*board_w + board_w-1].y;
 cvCircle(image,origin,20,red,6);
\end{lstlisting}

A continuación se establece la correspondencia entre los puntos del espacio imagen (esquinas del tablero) y espacio transformado (valores definidos internamente). Se crea la matriz que albergará la homografia, y se utiliza la función de OpenCV cvFindHomography(), que calculará los valores de la misma a partir de la relación de puntos entre los dos espacios.

\begin{lstlisting}[language=C++]
 CvPoint2D32f objPts[4], imgPts[4];
 H = cvCreateMat( 3, 3, CV_32F);
 CvMat _pt1, _pt2;
 _pt1 = cvMat(1, 4, CV_32FC2, &objPts[0] );
 _pt2 = cvMat(1, 4, CV_32FC2, &imgPts[0] );
 cvFindHomography(&_pt1, &_pt2, H, CV_RANSAC, 5);
 if(homography[iter->first])
   cvReleaseMat(&homography[iter->first]);
 homography[iter->first] = H; 
\end{lstlisting}

Finalmente, se compondrán las coordenadas obtenidas por las distintas cámaras. Para que esto pueda ser viable, es necesario usar un mecanismo de forma que la coordenada del centro de referencia del patrón sea siempre el centro del espacio de la instalación. \\

Para aplicar la homografia basta con multiplicar las coordenadas por la matriz de transformación. Por lo general el uso será siempre calcular las coordenadas transformadas a partir de las coordenadas del espacio imagen. \\

A continuación, se muestra cómo realizar esta operación, donde imgPts contiene los valores del punto que se tiene en espacio imagen y objPts la variable donde se guardará el resultado.

\begin{lstlisting}[language=C++]
 pt1 = cvMat( 3, 1, CV_32F, &objPts );
 pt2 = cvMat( 3, 1, CV_32F, &imgPts );
 if (homography[index])
 { cvMatMulAdd(homography[index], &pt2, 0, &pt1); }
\end{lstlisting}

Para terminar con esta herramienta se ilustra un ejemplo de cómo aplicar la matriz de homografía al conjunto de la imagen.

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/homography_example.png}
\end{center}
\caption{ \label{F_IPercept_Calibrate_Homography_Diagram} PerceptVideo: Homografía - Ejemplo}
\end{figure}

\end{itemize}

scr$\backslash$ipercept$\backslash$video$\backslash$SimpleFaceDetection \\

A continuación se describirá la solución que vendrá por defecto en el proyecto. Se trata de la implementación del Detector de Caras que utilizará los métodos proporcionados directamente por la librería de OpenCV. El detector de caras de OpenCV se basa en un clasificador Haar (cascade), basado en los trabajos de Paul Viola y mejorado por Rainer Lienhart. \cite{Viola01cvpr} \\

Aunque no es el objetivo analizar ni mejorar estos algoritmos se considera interesante comentar en qué consiste su funcionamiento. \\

Con el fin de mejorar la eficiencia en los procesos de detección de objetos, es común utilizar un proceso de detección de características del objeto de estudio. Esto es, reducir la información de la que se dispone a un conjunto de atributos. Por ejemplo, en el caso de caras humanas, una codificación de los contrastes existentes en la imagen y sus relaciones espaciales. A estas características se las conoce como Haar-Like Features, ya que se procesan de forma similar a los coeficientes que se usan en las transformadas wavelets de Haar. \\

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/haar_features.png}
\end{center}
\caption{ \label{F_IPercept_SimpleFAceDetection_HaarFeatures} PerceptVideo: Haar features}
\end{figure}

Para ello, será necesario disponer de un clasificador. Este clasificador se entrenará con imágenes ciertas, es decir, imágenes que representan al objeto de estudio; y a continuación con imágenes falsas, otras imágenes arbitrarias donde no aparezca el sujeto. Una vez que el clasificador ha sido entrenado será posible contrastar una nueva imagen (del tamaño de la imagen de entrenamiento), de forma que el clasificador sera capaz de distinguir si corresponde con el objeto que se busca. En el caso práctico en el que se dispone de una imagen de una escena, que puede contener un sujeto, el procedimiento consiste en utilizar una ventana que avanza por el espacio de la imagen, utilizando el clasificicador para comprobar si dicha ventana (sub-imagen) contiene al sujeto. De hecho, es el clasificador sobre el que se realiza el escalado, en lugar de la imagen, ya que es más eficiente de esta forma. \\

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/scale_window_classifier.png}
\end{center}
\caption{ \label{F_IPercept_SimpleFaceDetection_Scale} PerceptVideo: SimpleFaceDetection - Escalo convolución}
\end{figure}

Otro detalle a tener en cuenta, es que en realidad el clasificador consiste en una colección de una serie de clasificadores más simples que se aplican a las distintas subregiones de interés. Por este motivo se suele llamar cascade a este tipo de clasificadores (clasificadores en cascada). \\

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/cascade.png}
\end{center}
\caption{ \label{F_IPercept_SimpleFaceDetection_Cascade} PerceptVideo: SimpleFaceDetection - Cascade classifier}
\end{figure}

En el caso que nos ocupa se usarán los clasificadores que ya provee OpenCV, por lo que no sera necesario realizar la fase de entrenamiento. Lo primero que hará un detector será cargar este clasificador.

\begin{lstlisting}[language=C++]
 cascade_name << "./haarcascade_frontalface_alt2.xml";
 cascade = (CvHaarClassifierCascade*)cvLoad(cascade_name,0,0,0);
\end{lstlisting}

El detector se ejecutará en un hilo aparte y permanecerá dormido hasta que el módulo de percepción de video le envíe una nueva imagen para estudiar. Cuando esto sucede se aplicará el clasificador con el objetivo de detectar al sujeto dentro de la imagen. \\

Para ello, el primero paso consiste en inicializar algunos atributos y preparar las imágenes que se van a usar. Entre estas tareas de preprocesado se convierte la imagen de estudio a blanco y negro y se ecualiza para estandarizar el brillo y el contraste, por motivos de simplicidad y eficiencia, además de para reducir el impacto que diferencias en la iluminación pueden introducir.

\begin{lstlisting}[language=C++]
 cvCvtColor(img, gray, CV_BGR2GRAY);
 cvResize(gray, small_img, CV_INTER_LINEAR);
 cvEqualizeHist(small_img, small_img);
\end{lstlisting}

A continuación se aplica el detector y se extraen las ocurrencias detectadas dentro de la imagen. Adicionalmente se dibuja un marco alrededor de la cara detectada.

\begin{lstlisting}[language=C++]
if(cascade)
{ CvSeq * faces = cvHaarDetectObjects(small_img, cascade, 
                       cvCreateMemStorage(0), haar_scale, 
                       min_neighbors, haar_flags, min_size);
  int x, y, w, h, n;
  if (faces)
  for( int i = 0; i < (faces ? faces->total : 0); i++ )
  {  IplImage *aux_img = face_img;
	 face_img = Sub_Image(img, cvRect( faceRec_a.x, faceRec_a.y,
	            faceRec_b.x-faceRec_a.x, faceRec_b.y-faceRec_a.y ));
				if (aux_img) cvReleaseImage(&aux_img);
...
\end{lstlisting}

Se muestra un ejemplo del funcionamiento del detector.

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/simplefacedetection_example.png}
\end{center}
\caption{ \label{F_IPercept_SimpleFaceDetection_example} PerceptVideo: SimpleFaceDetection - Detección de caras}
\end{figure}

De la misma forma que todos los detectores ofrece métodos para la obtención de datos, así como de imágenes. Estos datos son, de forma genérica, las coordenadas del centro del objeto detectado, y las coordenadas de las esquinas que definen el rectángulo que lo enmarca. Estos atributos se obtienen en cada iteración cuando clasificador devuelve los datos del objeto detectado, actualizándose en ese momento. Se muestra a continuación cómo se obtiene el caso de las coordenadas del centro a modo de ilustración. El resto de peticiones se atienden de forma similar.

\begin{lstlisting}[language=C++]
void SimpleFaceDetection::GetFaceCenterPos(corePoint2D<int> &pos)
{ boost::try_mutex::scoped_lock lock(m_mutex);
  pos.x = faceCenterPos.x;
  pos.y = faceCenterPos.y; }
\end{lstlisting}

En concreto, devolverá la imagen completa con la cara resaltada, así como recortes de la cara en cuestión. 

scr$\backslash$ipercept$\backslash$video$\backslash$Encara2FaceDetection \\

En este apartado veremos la aplicación del detector Encara2, desarrollado por el tutor, Modesto Castrillón Santana. Los sistemas de detección de caras pueden clasificarse de varias formas. Una de estas clasificaciones los separan según el tipo de conocimiento que utilizan: implícito o explícito. \\

Los detectores que usan conocimiento implícito se centran exclusivamente en la imagen. Entrenan un clasificador con un conjunto de imágenes, teniendo en cuenta un conjunto de restricciones de escala y orientación. Es una solución de fuerza bruta, lenta pero robusta, que sin embargo, ignora información que podría acelerar la detección. \\

El segundo grupo de detectores se centran en el uso de información explícita acerca de la estructura y apariencia de determinadas características obtenidas a través del conocimiento de la experiencia humana (por ejemplo, proporción ojos-boca). Estos casos pueden ofrecer una detección rápida en entornos con determinadas restricciones. \\

El enfoque de Encara es unificar estos dos tipos de información. Primero, selecciona posibles candidatos utilizando conocimiento explícito, para luego aplicar un análisis basado en conocimiento implícito más rápido. \\

De igual forma que el clasificador estudiado en la implementación anterior, se trata de un clasificador en cascada.

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/cascade_encara.png}
\end{center}
\caption{ \label{F_IPercept_Encara2FaceDetection_Cascade} PerceptVideo: Encara2FaceDetection - Cascade classifier}
\end{figure}

Aunque no es el objetivo analizar ni mejorar esta solución al problema de la detección de caras, se considera interesante comentar brevemente su funcionamiento. El clasificador tomará una hipótesis y avanzará por los clasificadores para confirmarla o rechazarla:

\begin{itemize}
\item Tracking: Si existe una detección previa se buscan caracteristicas en zonas cercanas (ojos y comisuras de la boca). Si las posiciones detectadas son próximas se supera el test.
\item Face Candidate Selecteion: se realiza una búsqueda de zonas de la imagen rectangulares por aproximación de color, que puedan contener piel.
\item Facial Features Detection: Se elimina de forma heurística elementos de la imagen que no sean de interés (pe. cuello) y envuelve la posible imagen de cara dentro de una elipse para ignorar el resto de la imagen. En este módulo también se hace una detección temprana en busca de característica que responden a las interrelaciones de elementos geométricos o de apariencia (pe. ojos).
\item Normalización: Para reducir el priblema de la dimensionalidad al buscar caras a distintas escalas.
\item Pattern Matching Confirmation: A modo de confirmación o descarte, para eliminar posibles falsos positivos se realiza un análisis basado en conocimiento implícito: Se realiza un análisis de componentes principales (PCA) sobre los ojos. Se realizan test sobre el conjunto de la imagen y finalmente se estima la localización de la boca y nariz.
\end{itemize}

La aplicación de este librería muestra una mejora notable tanto en la rapidez como en el caso de detecciones correctas de caras. Modificaciones relaizadas sobre esta librería permite además obtener otras características interesantes como edad o sexo. \\

Su aplicación es similar al caso anterior, aunque más simplificado. Primero se crea una nueva instancia del detector.

\begin{lstlisting}[language=C++]
 ENCARAFaceDetector = new CENCARA2_2Detector(ENCARAdataDir,320,240);
\end{lstlisting}

De igual forma que resto de detectores, el hilo permanece dormido hasta que PerceptVideo le asigna una nueva imagen a estudiar. Esto se lleva a cabo mediante la activación de un flag cada vez que PerceptVideo envía una nueva imagen, que evitará que el hilo vuelva a dormirse y en su lugar procese la nueva imagen enviada. \\

en la siguiente iteración que se ejecute se solicitará al detector que procese la imagen y se obtiene la mayor cara detectada, en caso de haber alguna. A partir de ella, se obtendrá la información que nos interesa: posición del centro de la cara y coordenadas del rectángulo que la enmarca. También es posible obtener otras características ya comentadas, pero que por el momento no se usarán. \\

\begin{lstlisting}[language=C++]
bool Encara2FaceDetection::Apply()
{ ...
  ENCARAFaceDetector->ApplyENCARA2(h,2);
  CFacialDataperImage * facial_data = ENCARAFaceDetector->GetFacialData();
  int face_index = facial_data->GetLargestDetectedFace();
  if (facial_data && (face_index != -1))
  { facial_data->Faces[face_index]->Gender;
    faceRec_a.x = facial_data->Faces[face_index]->x1;
    faceCenterPos.x = (faceRec_a.x + faceRec_b.x) / 2;
    ENCARAFaceDetector->PaintFacialData(h,CV_RGB(0,255,0));
\end{lstlisting}

A continuación se muestra un ejemplo de detección con este componente. \\

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/encara_sample.png}
\end{center}
\caption{ \label{F_IPercept_Encara2FaceDetection_Sample} PerceptVideo: Encara2FaceDetection}
\end{figure}


scr$\backslash$ipercept$\backslash$video$\backslash$FaceRecognition \\

Este componente no será un detector sino un reconocedor. El problema que nos ocupa no es localizar un posible sujeto dentro de una imagen, sino dado un sujeto, ser capaz de iedntificarlo. \\

Para resolver este problema se ha hecho uso de las herramientas proporcionadas por la librería OpenCV. Para ello, se utilizará el algoritmo de Análisis de Componentes Principales (PCA). PCA es la solución más simple y rápida para el reconocimiento de objetos. Sin embargo, tiene ciertas debilidades que es necesario tener en cuenta: variaciones en traslación, escala, fondo, así como cambios en la iluminación peude provocar que el reconocer no sea capaz de identificar una cara ya registrada. También demuestra problemas con cambios como presencia/ausencia de accesorios, como gafas o barba. \\

Sin embargo, es una buena aproximación que cubre las necesidades que se tienen. \\

Al igual que en el detector de caras por defecto, se usará uno de los clasificadores que provee OpenCV.

\begin{lstlisting}[language=C++]
 cascade_name << "./haarcascade_frontalface_alt2.xml";
 cascade = (CvHaarClassifierCascade*)cvLoad(cascade_name,0,0,0);
\end{lstlisting}

Algunos de los atributos más relevantes de este componente son:

\begin{itemize}
\item Vector de caras: Imágenes de las caras de los usuarios a reconocer.
\item Vector de eigen-faces: Imágenes de diferencias usada para reconocer.
\item Cara promedio: media de todas las caras en la colección.
\item Índice: Utilizado para mantener una relación entre una cara y un identificador de usuario.
\end{itemize}

El sistema deberá registrar a cada usuario sobre el que se desee realizar reconocimiento. Si no se dispusiera de información de cada usuario no seria posible realizar identificación. Es un detector que también requiere entrenamiento, pero sólo sera necesario cada vez que se añada un nuevo usuario. \\

Cuando aparece un nuevo usuario que se desea añadir al registro, se introducirá una serie de imágenes del mismo. Estas imágenes habrán sido tomadas por PerceptVideo, que mantiene un pool de caras recientes. El primer paso es preprocesarlas. Como se explicaba en el caso anterior, por simplicidad, velocidad y para disminuir el impacto de efectos de la iluminación, las imágenes son preprocesadas: se convierten a escala de grises, se escalan a un tamaño de estudio por defecto y se ecualizan para estandarizar el brillo y el contraste. Estos procedimiento son comunes y ya han sido descritos. 

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/preprocess.png}
\end{center}
\caption{ \label{F_IPercept_FaceRecognition_Preprocess} FaceRecognition: Preprocess}
\end{figure}

Se comentará un detalle en relación a la escala. Se utilizarán método distintos para agrandar frente a encoger, debido a que cada método ofrece mejores resultados en ese caso, y peores en el contrario. \\

\begin{lstlisting}[language=C++]
 cvCvtColor( zimage, face_gray, CV_BGR2GRAY );
 
 if (face_width > size.width && size.height > size.height)
  cvResize(face_gray, scaled, CV_INTER_LINEAR);
  //CV_INTER_CUBIC or CV_INTER_LINEAR, good for enlarging
 else
  cvResize(face_gray, scaled, CV_INTER_AREA);
  //good for shrinking, but bad at enlarging.

  equalized = cvCreateImage( desired_size, IPL_DEPTH_8U, 1);
  cvEqualizeHist( scaled, equalized );
\end{lstlisting}

PCA implica dos consideraciones: por un lado podrá contrastarse una imagen dada contra el clasificador y obtener la probabilidad de acierto/similitud con alguna de las caras almacenadas. Por otro lado, debe existir esta base de datos para poder aplicar  PCA. \\

Ya se ha comentado que cuando se registra un nuevo usuario se añade una colección de imagenes suyas. El siguiente paso es volver a entrenar el conjunto completo, por lo que se purga a información actual y se vuelve a calcular. \\

Para poder diferenciar entre una cara y otra, el algoritmo calcula una media entre todas las imágenes añadidas a la colección, que se llamará cara-media. A partir de esta imagen se calcularán las eigen-faces, de forma que la primera de ellas mostrará las diferencias más dominantes frente a la cara media, la segunda representa difrencias menos dominantes, y asi sucesivamente. Observado las eigen-faces peude apreciarse que las de primer orden mantienen características reconocibles, mientras que las de orden mayor son cada vez más ruidosas.

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/eigenfaces.png}
\end{center}
\caption{ \label{F_IPercept_FaceRecognition_Eigenfaces} FaceRecognition: eigen-faces}
\end{figure}

De la misma forma que una imagen se puede descomponer en sus eigen-faces, también es posible recomponerla. En este procedimiento es en lo que se basa el algoritmo de PCA para reconocer una imagen, obtener el conjunto de las eigen-faces que en proporción se asemejan más a la cara que se desea reconocer. \\

En resumidas cuentas tenemos dos procesos, un primero donde recolectamos datos y entrenamos el clasificador. Para ello, se calcularán los Eigen Objects (eigen-values y eigen-vectors), y a cotinuación se aplicará la escomposición de cada cada del catálogo para obtener todos los coeficientes de descomposición.

\begin{lstlisting}[language=C++]
 cvCalcEigenObjects( face_vector.size(), (void*)face_vector_pp,
                	(void*)eigen_vector_pp, CV_EIGOBJ_NO_CALLBACK, 
                	0, 0, &term_criteria, average_image, 
                	eigen_values->data.fl);
 ...
 for(int i=0; i < face_vector.size(); i++)
 { float *coeffs = projected_trainning_faces->data.fl;
   coeffs +=  i*offset;
   cvEigenDecomposite( face_vector[i], eigen_vector.size(), 
                       eigen_vector_pp, 0, 0, average_image, coeffs);
\end{lstlisting}

Para cada consulta que se realice, volverá a seguirse algunos de estos pasos. Primero se preprocesará la imagen y a continuación se descompondrá a partir de los valores calculados durante el entrenamiento. Una vez se tenga la imagen proyectada se comparará para obtener la imagen candidata más similar; y si encuentra alguna que supere el umbral de similitud establecido se buscará en el índice que nos identifica una cada con un id de usuario.

\begin{lstlisting}[language=C++]
int FaceRecognition::RecognizeFromImage(char* data, ...)
{ cvEigenDecomposite(equalized, eigen_vector.size(), 
                    eigen_vector_pp, 0, 0, average_image, 
                    projected_test_face);
  candidate = FindBestCandidate(projected_test_face, likeness);
  return (candidate != -1) ? 
                   facepic_to_person_indexes->data.i[candidate] 
                   : -1;
...

int  FaceRecognition::FindBestCandidate(float *projected_test_face_, 
                      float &likeness, const bool &use_mahalanobis)
{ double min_distance = DBL_MAX;
  int candidate = -1;
  for(int trainning_face = 0; 
      trainning_face < face_vector.size(); trainning_face++)
  { double distance = 0;
    for(int eigen = 0; eigen < eigen_vector_size; eigen++)
    { double aux_dis = projected_test_face[eigen] 
                     - projected_trainning_faces->data
                     .fl[trainning_face*eigen_vector_size+eigen];
    if (use_mahalanobis)
         distance += aux_dis * aux_dis / eigen_values->data.fl[eigen];
    else distance += aux_dis * aux_dis;
   }
   if ( distance < min_distance )
   { min_distance = distance;
     candidate = trainning_face; }}

   likeness = 1.0f - sqrt( 
              min_distance / (float)(face_vector.size() 
                                   * eigen_vector_size) ) / 255.0f;
   return ((likeness > 0.75f) ? candidate : -1);
}                   
\end{lstlisting}

Llegado a este punto sólo nos resta guardar todos los datos a un fichero que se pueda recpurar de disco, que cargará al inicio de la aplicación y se que guardará cada vez que s evuelva a entrenar el clasificador. Este fichero estará guardado en formato XML, donde se guardarán todo los datos necesarios como el par etiqueta-valor:

\begin{lstlisting}[language=C++]
 int face_vector_size = 
     cvReadIntByName(file_storage, 0, "face_vector_size", 0);
 int eigen_vector_size= 
     cvReadIntByName(file_storage, 0, "eigen_vector_size", 0);
 facepic_to_person_indexes = 
    (CvMat *)cvReadByName(file_storage,0,"facepic_to_person_indexes",0);
...	
 for(int i = 0;  i < face_vector_size; i++)
 { face_vector.push_back((IplImage *)cvReadByName(file_storage, 0, 
                                      index, 0));	}
\end{lstlisting}

scr$\backslash$ipercept$\backslash$video$\backslash$PresenceDetection \\

El detector de presencia funciona de forma similar al resto de los detectores. Se ejecutará en un hilo aparte y permanecerá dormido mientras no le sean enviadas imágenes nuevas que procesar. Una vez le sea enviada una nueva imagen procederá a su análisis para detectar elementos que no formen parte del fondo. \\

Tras su implementación se ha compribado que es uno de los detectores que más problemas ha dado. Es costoso computacionalmente, tanto en proceso como en memoria. Además también requiere entrenamiento, pero se verá afectado por cambios en las condiciones ambientales más difíciles de controlar. Sin embargo, al tratarse de una inatalación se mantiene que el entorno permanece controlado. Revisando la tabla de condiciones que se asumen \ref{F_Typical_Assumptions_Etapa4}, página \pageref{F_Typical_Assumptions_Etapa4}, sólo habra un individuo cada vez en la instalación y el fondo aunque no sera uniforme si será eminentemente estático. Por otro lado, las condiciones de iluminación también serán controladas. \\

Para poder definir un fonde de imagen es necesario crear un modelo que sea capaz de discriminar si un pixel forma parte del fondo o del sujeto en primer término. Para ello se usa un Modelo de Composición de Gausianas (Gaussian Mixture Model). GMM es un tipo de modelo de densidad probabilístico expresado como una suma ponderada de componentes gausianos de densidad. Se utilizan comunmente para obtener la distribución de probabilidad de medidas continuas de características. Puede verse más fácilmente en un figura.

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/gmm.png}
\end{center}
\caption{ \label{F_IPercept_FaceRecognition_Eigenfaces} FaceRecognition: Gaussian Mixture Model}
\end{figure}

De cualquier forma, utilizaremos este modelo para entregar el detector durante una serie de iteraciones y que sea capaz de clasificar los valores que indicar que un pixel determinado forma parte del fondo. Durante este entrenamiento no debe aparecer ningún sujeto en el espacio de la instalación. \\

El esquema es bastante simple de utilizar. Por una aprte se crea y se le asignaa una serie de parámetros de configuración. A continuación bastará con actualizar el modelo en cada iteración con la imagen capturada. A partir del modelo podemos obtener la imagen de frente.

\begin{lstlisting}[language=C++]
 bg_trainning_model = 
  cvCreateGaussianBGModel(latest_bkg ,background_params);
 ...
bool PresenceDetection::Apply()
{...
 int train_flag = (train) ? -1 : 0;
 cvUpdateBGStatModel( image, bg_trainning_model, train_flag); 
 background_model->foreground;
\end{lstlisting}

Una vez calculado, se comprobará la imagen capturada en cada frame con este modelo y se discriminarán los píxeles que forman parte del fondo o el primer término. La instalación está dirigida a que sólo existe un elemento dentro de ella, el usuario, por lo que se establecerá una relación directa. Finalmente, existe presencia si se detecta una cantidad considerable de píxeles clasificicados como tal. \\

Finalmente, a partir de la imagen de frente pueden obtenerse una cantidad interesante de características mediante el uso de Momentos. 

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/moments.png}
\end{center}
\caption{ \label{F_IPercept_PresenceDetection_Moments} PresenceDetection: Moments}
\end{figure}

Con el momento de orden 0, p = q = 0, (M00) puede verse inmediatamente que se obtiene el área de la imagen: para cada fila y se suman todos los píxeles con valor 1, y luego se acumulan todas estas secciones obteniendo el área. Por otro lado, aunque no se ve de forma tan directa, con los momentos de orden 1, pueden obtenerse las coordenadas del centro de masas del objeto (M10 daría la componente x, y M01 la componente y). Hay que tener en cuenta que aconsejable aplicar erosión antes de calcular los momentos para eliminar ruido que pueda haber en la imagen.

\begin{lstlisting}[language=C++]
 cvErode(background_model->foreground, eroded, 0, 3);
 cvMoments(eroded, &foreground_moments);
 double area = foreground_moments.m00;
 if(area)
 { presenceCenterPos.x = foreground_moments.m10/area;
   presenceCenterPos.y = foreground_moments.m01/area;		}
\end{lstlisting}

A continuación se muestra un ejemplo de la aplicación de este componente.

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/presencedetection_example.png}
\end{center}
\caption{ \label{F_IPercept_PresenceDetection_Example} PresenceDetection: Example}
\end{figure}


scr$\backslash$ipercept$\backslash$video$\backslash$MotionDetection \\

Finalmente, el último de los detectores implementado es un detector de movimiento. De la misma forma que los componentes mostrados anteriormente, existe una instancia por cada cámara que estudie la instalación y por motivos de rendimiento se ejecutará en un hilo paralelo a al ejecución del resto. En el caso de que no tenga nuevas imágenes que procesar, el hilo permanecerá dormido, y cuando PerceptVideo le envíe una nueva imagen procederá a su análisis. \\

En este caso, a diferencia de los anteriores, no será necesario disponer de ningún tipo de datos de entrenamiento ya que el análisis se realizará enun ventana temporal correspondientes a las últimas imágenes más recientes. \\

Para realizar este estudio se seguirá un modelo MHI (Motion History Image), es decir se mantendrá un histórico de las imágenes que representan el movimiento a partir de una silueta. También se calcularán los gradientes de movimientos, que indican la dirección de movimiento de cada pixel.

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/silueta_mhi.png}
\end{center}
\caption{ \label{F_IPercept_MotionDetection_MHI} MotionDetection: Silueta - Motion History Image - Motion History Gradients}
\end{figure}

Para calcular estas siluetas y obtener el histórico, se creará una imagen limpia con valores cero y se aplicará el método UpdateMHI(). Se le pasan como parametros la imagen de estudio y la imagen donde se desea alamacenar el resultado. El tercer parámetro indica el umbral que se desea utilizar para filtrar las direferencias entre frames.

\begin{lstlisting}[language=C++]
void MotionDetection::Process()
{ if( !motion )
  { motion=cvCreateImage(cvSize(width,height),8,3);
    cvZero( motion );
    motion->origin = image->origin;  }
    UpdateMHI( image, motion, 30 ); 
\end{lstlisting}

A continuación se obtendrá un time-stamp (marca de tiempo actual), que se usará cada vez que se desee actualizar el historial. Tambien se mantiene un buffer donde almacenar la secuencia de imágenes. En cada iteración se inserta la nueva imagen capturada al final del historial y se calcula la diferencia con la imagen del frame anterior, se filtra con el umbral, y se actualiza el historial de imágenes y el de gradientes. 

\begin{lstlisting}[language=C++]
void  MotionDetection::UpdateMHI( 
      IplImage* img, IplImage* dst, int diff_threshold )
{ double timestamp = (double)clock()/CLOCKS_PER_SEC;
  cvAbsDiff( buf[idx1], buf[idx2], silh ); //diff between frames
  cvThreshold( silh, silh, diff_threshold, 1, CV_THRESH_BINARY );
  cvUpdateMotionHistory( silh, mhi, timestamp, MHI_DURATION );
  cvCalcMotionGradient( mhi, mask, orient, MAX_TIME_DELTA, 
                        MIN_TIME_DELTA, 3 );
  
\end{lstlisting}

A partir de este análisis es posible recuperar las distintas áreas de interés, de forma que se puede obtener un análisis de movimiento a nivel global o local de cada segmento analizado. Sin embargo, esto entra dentro de la consideración de técnicas avanzadas de detección que se estudiará en el desarrollo de la siguiente fase. \\

Se muestra a continuación ejemplos de resultado del análisis de movimiento utilizando este componente.

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/motion_example.png}
\end{center}
\caption{ \label{F_IPercept_MotionDetection_Example} MotionDetection: Example}
\end{figure}

\section{Validación y Publicidad}
\subsection{Validación}
\begin{itemize}
\item Comprobación de uso de recursos de la máquina mediante las herramientas del sistema. En la máquina en la que se desarrolla la aplicación muestra consumir una media de un 19\% de CPU, con 27 subprocesos asociados y una media de 200Mb de memoria con el escenario por defecto de prueba. Al cargar los distintos escenarios se ven cambios dependiendo de la complejidad de los mismos, lo cual es esperado. También es esperado el aumento de consumo de la CPU debido a los nuevos detectores añadidos. Sin embargo, se aprecia que al carga de trabajo se mantiene balanceada entre las CPUs del equipo. 
\item Respecto a a la comprobración de la ejecución y cierre correctos, así como memory leaks, se han detectando dos bugs de relevancia:\\
\\
El primero consistía un assert espúreo localizado en el módulo de detección, en especial el detector de movimiento, sólo durante de la carga de la aplicación. En concreto durante la creación de un hilo de boost. No se pudo averiguar el motivo ni encontrar documentación relacionada. Sin embargo, durante la depuración se hicieron cambios, incluyendo nuevos fuentes y modificando las opciones de compilación. Tras realizar estos cambios no se ha podido volver a reproducir el error. \\
\\
El segundo se trata un memory leak de tamaño moderado, pero no localizado. Probablemente relacionado con los detectores o el sistema de colisiones. Deben localizarse y resolverse. Sin embargo, con el fin de no retrasar el avance del proyecto y como se ha conseguido alcanzar el reto de objetivos, se propone resolverlo durante el desarrollo de la siguiente fase.
\item Uso de herramientas para medición de frames por segundo para comprobar el rendimiento de la ventana de render. No se aprecian cambios en la tasa de frames por segundo que se mantiene a 60fps para dos ventanas de render. Teniendo en cuenta que 60 es el límite máximo impuesto por la sincronización vertical del monitor. Frente a los primeros esquemas de detectores, la solución finalmente adoptada ofrece una buena tasa de resultados por segundo. (tabla??)

\end{itemize}
Se muestran algunas capturas del estado actual de la aplicación:

\subsection{Publicidad}
Siggraph - Julio 2011 - inscripcion 10 Enero\\
ArtFutura\\
Arco - Febrero 2012, 250euros m2\\
Estampa\\
Sonar\\
CanariasMediaFest
acm multimedia 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Etapa 5: Interacción Avanzada y Creación de Contenidos}

A lo largo de las demos anteriores se perseguía el objetivo de ofrecer las herramientas necesarias para la percepción y creación de elementos tridimensionales, incluyendo un inicio de interacción. Con estos mecanismos ya en marcha se procederá a la creación de contenidos, de forma que el usuario ya pueda crear entornos según sus acciones. Para ello, un usuario podrá crear elementos, destruirlos o provocar reacciones con los mismos, creando de esta forma su propio entorno.
\\

Primero, separaremos el trabajo a realizar en dos campos: Interacción avanzada y Creación de contenidos.
\\

Interacción Avanzada: Se añaden nuevos mecanismos de interacción que vaya más allá de la navegación. Ya no se usará sólo la posición del usuario, sino también información relativa a sus movimientos, de forma que pueda interactuar con los elementos existentes en la escena. Se estudiará la colisión de su cuerpo con los objetos de la escena, así como se recopilará la información obtenida por el módulo de percepción para realizar cálculos estadísticos en relación a su actividad. Excentricidad de su cuerpo (si mantiene una postura muy redondeada o más recta de pie). También se estudia cómo construir una malla 3D de su cuerpo, así como otras representaciones que nos permitan calcular colisiones y obtener datos con ese punto de la colisión como velocidad o dirección de movimiento. Estas acciones afectarán a qué tipo de contenidos se crean en el mundo.
\\

Creación de contenidos: Hasta el momento se han usado escenas de prueba para comprobar el funcionamiento de la aplicación y sus capacidades. Sin embargo, la intención de la aplicación que se desea construir es crear el contenido en tiempo de ejecución, a partir de las acciones del usuario. De esta forma, se habilitarán mecanismos para crear, modificar o destruir elementos del entorno. Además según su comportamiento se creará un perfil, que llamaremos 'Psique' que servirá como punto principal para la composición. Afectará al audio y a los objetos que se crean. 
\\

Un punto de especial interés en la creación de contenidos, es que se desea crear elementos de vida artificial. No todos los objetos creados serán entidades pasivas, esperando la interacción del usuario. Algunos de ellos tendrán un comportamiento propio, siendo capaces de moverse y perseguir sus propios objetivos.
\\

Se incluye también en la creación de contenidos el apartado de la composición musical. La composición procedimental es un campo de gran interés. Aunque no es el objetivo realizar un estudio profundo sobre la misma, se ha decidido añadir ciertas componentes. Basándose en la psique, que refleja el histórico de las acciones realizadas, se establece la estructura armónica de la composición, su ritmo y estilo. Se definen tres elementos que conformarán la composición: una base rítmica, una melodía de fondo y unas decoraciones. 
\\

Con la finalización de esta fase, el SDK y la aplicación se pueden considerar terminada, de cara a los objetivos que se querían conseguir, cerrando el ciclo de las técnicas que se deseaban explorar en el desarrollo del trabajo. Sin duda, tanto el SDK como la aplicación pueden enriquecerse y existen diversas líneas de trabajo futuro muy interesantes. Por ejemplo: crear composiciones musicales procedurales más complejas, hacer uso de análisis y síntesis de audio o extender la interfaz con el motor gráfico para añadir más capacidades. Con la aparición del dispositivo XBOX Kinect, sería interesante implementar un nuevo módulo de percepción que use esta tecnología, y comprobar su integración en el sistema. En cualquier caso, nos detendremos en este apartado al llegar a la sección de trabajo futuro.

\section{Consideraciones Previas}

El proyecto ha sufrido un paréntesis en el que el desarrollo se ha detenido. 
\\

Debido a esto y al reconocimiento de que el alcance ideado inicialmente podía ser demasiado extenso, se han tomado decisiones para acotar la dimensión del mismo. De esta forma, se ha planteado simplificar tanto los mecanismos de interacción avanzada como de creación de contenidos, y las mejoras se añaden como posible trabajo futuro. También se relajará la exigencia y concentración sobre el estudio de múltiples fuentes y salidas para esta propuesta de instalación, centrándonos en el uso de una única cámara y display.

\section{Análisis}
\subsection{Análisis de Requisitos Hardware}
Aunque la arquitectura del SDK soporta un número indeterminado de cámaras y displays, se relajará la exigencia y concentración sobre este apartado para la consecución de la propuesta de instalación, centrándonos en el uso de una única cámara y display.

\subsection{Análisis de Requisitos de Usuario}
A partir de ahora el usuario no sólo se moverá por el entorno, sino que interactuará con el mismo. Para ello, será necesario obtener, además de los datos generales como su posición absoluta o movimiento global, estudiar su cuerpo. El usuario tendrá una representación dentro de la escena, no necesariamente visible, dentro del entorno 3D que reflejará sus acciones. \\

Aunque es realmente interesante el estudio del reconocimiento postural, se ha optado por una solución más simple. Crearemos una representación del usuario basada en una nube de puntos simplificada, nos permitirá representar al usuario así y proyectar su interacción con la escena.\\

Se usará la sustracción de fondo para obtener un modelo del usuario. Añadiendo los datos de detección de movimiento obtendremos la velocidad de movimiento de un punto concreto del avatar. Sin embargo, no habrá distinción explícita de miembros ni postura. Aunque se considera un apartado muy interesante para trabajo futuro. \\

La interacción del usuario se puede clasificar en dos secciones:\\

Interacción basada en colisión: A partir del modelo del usuario podemos proyectar su presencia en profundidad. Cada elemento del modelo del avatar del usuario conserva datos de forma que es posible conocer, por ejemplo, la dirección y velocidad de ese segmento. Si la velocidad de esa colisión tiene una magnitud baja se considerará que el avatar está en contacto con la Entidad que toca y, en nuestra instalación, decidiremos que esa acción provocará una reacción y creará un enjambre de entidades. Sin enbargo, la intensidad del movimiento es elevada lo consideraremos un golpe, la entidad reaccionará de una forma más enérgica, creará otro tipo de enjambre diferente y morirá. 
\\

Interacción basada en comportamiento: Las acciones del usuario provocarán efectos personalizados. Se utilizan para ellos dos conceptos nombrados como "psique" y "Energía". El valor de "psique" puede moverse entre 0-Bondad, 1-Neutral y 2-Maldad. Crear entidades moverá al usuario hacia el valor 0-Bondad, golpearlas para destruirlas la moverá hacia el valor 2-Maldad. Por otro lado, se prepara un esquema para poder obtener valores estadísticos del comportamiento del usuario a partir de la información obtenida por el módulo de percepción. Algunos de estos datos como el centro de masas, volumen, excentricidad de la forma de su cuerpo, o número de segmentos de su cuerpo, pueden permitir clasificar el tipo actividad que realiza el usuario durante una sesión dentro de la instalación. En este caso usaremos la excentricidad y el número de segmentos móviles para calcular un valor de "Energía" que irá entre el valor 0-Calma y 1-Exitado.

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/casosdeuso_cuartademo.png}
\end{center}
\caption{ \label{F_Casos_de_Uso_Etapa5} Diagrama de Casos de Uso. Interacción avanzada y Creación de contenidos.}
\end{figure}


\subsection{Selección de Herramientas}

Un cambio de interés ha sido el de la herramienta de gestión de proyectos. Para esta fase final se ha utilizado la herramienta MOOVIA. MOOVIA es una red social y gestor de proyectos ágiles online que permite la gestión colaborativa de proyectos, muy integrada con el uso de documentos compartidos de Google Drive. Permite crear componentes, etapas, fases o sprints de desarrollo, y crear listas de tareas que planificar y sobre las que realizar su seguimiento. Tiene un acercamiento importante a las metodologías de desarrollo ágiles ofreciendo interfaces de tipo kanban donde ver el estado actual de una tarea (ToDo, Doing, Issue, Done), así como diagramas de BurnDown (gráfica de quemado de trabajo que muestra el trabajo pendiente por realizar) o diagramas de Gantt basados en las etapas.
\\

Como se ha usado sólo en la etapa final del proyecto, su uso se ha centrado en la manipulación de tareas. En cualquier caso se trata de una herramienta muy potente y flexible, por lo que su uso es muy recomendado.
\\

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/moovia_gestionproyectos.png}
\end{center}
\caption{ \label{F_MOOVIA_Etapa5} MOOVIA. Herramienta online para la gestión de proyectos www.moovia.com}
\end{figure}

Otra herramienta que ha sido sustituída ha sido el software de control de versiones. Para la última Etapa se ha creado un repositorio público de Github que puede accederse desde esta url: https://github.com/Adrasl/OX/
\\

Por otro lado, se revisita el módulo de Percepción para extender la detección de movimientos. El análisis que se añade es el cálculo de flujo óptico entre dos imágenes con el que poder estimar la dirección e intensidad de movimiento de un segmento del cuerpo del usuario dado un punto de su cuerpo.
\\

También se visita el módulo de Producción para añadir la capacidad de gestionar colas para añadir y retirar entidades a la escena, así como para llamar a sus métodos de actualización (cuya implementación es desconocida para este módulo), crear un mecanismo de colisiones entre entidades y contra el usuario, así como algunos ejemplos sencillos de postproceso.
\\

En el Módulo de Producción se actualiza la librería de terceros Panda3D a la versión 1.8.1, que aporta la mayor ventaja de haber mejorado la estabilidad, corrigiendo algunos módulos que no eran thread-safe al compilar en Release. Por suerte, los cambios remitidos durante la versión 1.6.1 que evitaban conflictos durante el lincado fueron aceptados en revisión en su momento y forman parte ya de la distribución actual, por lo que no fue necesario volver modificarlos para recompilarlos.
\\

Antes de terminar este apartado se considera de interés recordar algunas herramientas para el análisis y síntesis de audio. Aunque no se usan en este proyecto se proponen al lector como herramientas destacadas para trabajos futuros o proyectos de similar estilo. Estas herramientas destacadas son Marsyas \ref{SUBSUBSEC_MARSYAS}, página \pageref{SUBSUBSEC_MARSYAS} y Chuck \ref{SUBSUBSEC_CHUCK}, página \pageref{SUBSUBSEC_CHUCK}.
\\

En resumen:
\\

\begin{itemize}
\item Gestión de Proyectos: MOOVIA
\item Control de versiones: Github (https://github.com/Adrasl/OX/)
\item Visión por Computador - Flujo Óptico: OpenCV
\item Motor gráfico - Gestión de entidades: Colas de gestión.
\item Motor gráfico - Gestión de colisiones: Panda3D
\item Motor gráfico - Postproceso: Panda3D
\item Creación de una malla 3D a partir de una nube de puntos: Marching Cubes.
\item Acumuladores estadísticos: Boost accumulators
\end{itemize}


\section{Diseño}
\subsection{Diseño de la Aplicación}

Se añaden elementos al módulo iPercept, IProd, ICog y se hacen modificaciones en IApplicacion, IPersistence. También se añaden elementos a la aplicación.

\begin{itemize}
\item Core: Conjunto de interfaces de la aplicación.
\begin{itemize}
\item IPercept: Interfaz para el módulo de Percepción. Se añaden métodos para acceder al análisis de la forma de la presencia del usuario.
\begin{itemize}
\item IPerceptVideo: Ajustes en la interfaz para acceder a las funcionalidades de grabación y carga de videos, así como funcionalidades nuevas ofrecidas por los detectores añadidos. 
\item IPresenceDetection: Interfaz para ofrecer más información sobre la detección de presencia de área, orientación y excentricidad.
\item IMotionDetection: Interfaz para añadir información de flujo óptico a la detección de movimiento.
\end{itemize}
\item IProd: Interfaz para el módulo de Poducción. Se añaden métodos para añadir y retirar entidades a la escena actual. Se añaden métodos para crear y actualizar la presencia del usuario en la escena. Se añaden métodos para detectar colisiones entre entidades y de entidades contra el usuario. Se añaden métodos para añadir o quitar audios a la escena (absolutos) o a las entidades (relativos), y modificar su tono y amplitud. Se añaden métodos para cambiar el color de fondo y la intensidad y color de la niebla de la escena. Se añaden métodos para añadir ejemplos de postproceso tras el render de la imagen.
\begin{itemize}
\item IEntity: Interfaz abstracta para poder manipular entidades sin necesidad de conocer su implementación.
\end{itemize}
\end{itemize}
\item igui: Ajustes para añadir opciones de configuración de las cámaras que permitan grabar y cargar videos grabados, así como cambios estéticos de las interfaces.
\item ipercept: Se realizan cambios para implementar las capacidades comentadas en IPercept. 
\begin{itemize}
\item PerceptVideo: Grabación y carga sincronizada de múltiples cámaras. Acceso a las funcionalidades añadidas en PresenceDetection y MotionDetection. Creación y simplificación de una nube de puntos que represente al usuario, donde cada punto guarda además información sobre el mismo como posición, tamaño y velocidad de movimiento.
\item PresenceDetection: Uso del cálculo de momentos de orden 1 y 2 para obtener área, orientación y excentricidad.
\item MotionDetection: Uso de flujo óptico para calcular intensidad y dirección de movimiento sucedido en la imagen.
\end{itemize}
\item iprod: Se realizan cambios para implementar las capacidades comentadas en IProd. Para mantener un pipeline coherente y consistente se hace uso de colas para, sus casos más destacados son las colas de actualización de entidades, detección de colisiones, y las usadas para añadir y borrar entidades a la escena.
\begin{itemize}
\item MeshFactory: Creación de un modelo3D a partir de una nube de puntos basado en Marching Cubes (Metaballs).
\item Prod3DEntity: Se extiende para heredar de IEntity y se añaden elementos como fuentes de Audio, animaciones del modelo, así como los métodos OnStart, OnUpdate y OnCollision para que el pipeline de producción permita, de forma agnóstica, que las entidades actualicen su estado.
\end{itemize}
\item vox: 
\begin{itemize}
\item ApplicationConfiguration: Ajustes necesarios para guardar las opciones de reconocimiento y autologin.
\item NavigatorController: Extendemos su responsabilidad y lo renombarmos a RunningSceneController. Mantiene la responsabilidad de navegar dentro de la escena y añade la responsabilidad de crear y mantener la representación del usuario así como la de ejecutar la creación de contenidos.
\item ContentCreationController: Controlador que implementa la creación de contenidos durante una sesión en curso teniendo en cuenta información sobre la interacción del usuario.
\item OXStandAloneEntity: Extensión de IEntity que implementa un tipo de entidad que se muestra por sí misma e independiente. Representa un tipo de entidad básica.
\item OXBoidsEntity: Extensión de IEntity que implementa un tipo de entidad que pertenece a una especie gregaria y cuyo comportamiento se define por su conocimiento del entorno y del de los integrantes de su propia especie.
\item Application: Ajustes para automatizar el inicio de sesión haciendo uso del módulo de percepción.
\end{itemize}
\item icog:
\begin{itemize}
\item CommonSwarmIndividual: Se ofrece en el módulo de icog una implementación de una IA genérica que puede servir para crear comportamientos de indivuos de una especie, que contiene los componentes básicos de Boids (separación, cohesión y alineamiento) y extiende algunos componentes adicionales (límites, aleatoriedad, evasión y atracción). Con esta IA puede representarse comportamientos como el de una nube de moscas, como un banco de peces o bandada de aves.
\end{itemize}
\end{itemize}

\subsubsection{Breve descripción de los módulos}

Durante esta Etapa se trabajará principalmente en los módulos de Producción y Percepción de SDK, así como en la Aplicación en sí para crear la propuesta de instalación. 
\\

Se añade la capacidad de crear un modelo 3D a partir de la sustracción del usuario (Presencia) obtenida del módulo de Percepción. También se añadirá un esquema de colisiones Entidad-Entidad y Avatar-Entidad, donde para el usuario además se añade información sobre la velocidad de movimiento del jugador en ese punto de su cuerpo.
\\

El propio motor de juego nos da herramientas para la detección de colisiones, y el módulo de percepción es capaz de darnos la segmentación del usuario y la velocidad en cada pixel de la imagen, como ya se ha descrito en los apartados anteriores. De esta forma, el apartado más interesante de esta sección es la contrucción de una malla 3D que represente al usuario, a partir de una nube de puntos. Para ello se ha usado el algoritmo de Marching Cubes que, dado un punto y una vecindad crea primitivas geométricas predefinidas o las descarta. Este problema es muy costoso en cómputo y muy sensible a la resolución, por ello se propone una simplificación. 
\\

El primer paso es definir un grid, una rejilla tridimensional que definirá el tamaño y posiciones de los posibles puntos en el espacio del modelo 3D. El siguiente punto a acotar son los datos a estudiar, la nube de puntos, con el fin de acelerar y obtener una cantidad mayor de mallas por segundo. Para ello se ha usado un clasificador espacial R-Tree y un algoritmo de deconstrucción, de forma que dado un punto de la nube se sustraerá un volumen del mismo, como si diéramos bocados. De esta forma es posible reducir la nube de puntos original a una colección de elementos más reducida, sin afectar en exceso a la resolución de la forma o pérdida de detalles (dependiendo siempre de la configuración de los parámetros y la configuración del grid en el Marching Cubes). Una de las debilidades de este algoritmo de deconstrucción es que los puntos internos de la forma se siguen teniendo en cuenta. Se conoce la posibilidad de mejorar este algoritmo usando uno de esculpido en su lugar, de forma que sólo se tenga en cuenta los puntos de la superficie. Se estima que esto aceleraría sensiblemente la simplificación de la nube de puntos. Esta alternativa se plantea como mejora futura.

Por último, nos queda el apartado de creación de contenidos:
\\

La aplicación contendrá colecciones de audios y primitivas geométricas, clasificadas de forma que sea posible asignarlas a la Ambientación y a los Elementos que se crearán, y que de esta forma sean acordes a la Psique definida para este mundo.
\\

Ambientación y Creación de Contenidos: Se trata de apartados de la escena 3D que no está formado por elementos: sonido ambiental, color de fondo, niebla y efectos de post-producción. Se elegirán configuraciones de forma aleatoria dentro de un pool según la Psique actual del Mundo. El Mundo creará de forma aleatoria elementos interactivos.
\\

Interacción con los Elementos: Para simplificar sólo existirá un tipo de interacción, la colisión usuario-elemento. Se realizará una colisión contra la malla 3D del usuario. Al estar creando continuamente una malla con la forma del usuario esta colisión no será de tipo Curpo-Rígido, sino que las geometrías serán capaces de atravesarse y lo que se desea detecctar es si una geometría se superpone a otra. A partir de esta colisión se calculará la velocidad de movimiento de ese segmento y, según esa velocidad, concluiremos si el usuario está en contacto estático, moviendo suavemente o golpeando un Elemento. Con este lenguaje de tres elementos (contacto, mover, golpear) y el valor de la velocidad cada Elemento definirá un comportamiento propio. Por defecto, golpear destruirá el Elemento, contacto y mover podrán disparar una acción del elemento o crear otros nuevos.

\vspace{100 mm}

\subsubsection{Diseño en UML - Resumen}

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/DiagramadeclasesUML_CuartaDemo.png}
\end{center}
\caption{ \label{F_DiagramadeClasesUML_CuartaDemo} Diagrama de Clases UML, Cuarta Demo. Resumen.}
\end{figure}

Se revisan algunas secciones generales para extender las interfaces con nuevos métodos. En IProd se crea la interfaz IEntity que servirá para manipular entidades a alto nivel sin necesidad de conocer su implementación. En la aplicación se renombra NavigationController a RunningSceneController, ya que no se dedicará sólo a la navegación sino al control de la escena actualmente en curso. También se crea ContentCreationController, encargado de crear el contenido de la escena según la interacción observada del usuario y las Entidades OXStandAloneEntity y OXBoidEntity. Se detallan algunos de los tipos generales creados de uso común para la herramienta, como Observer y Subject para ofrecer el patrón Observador-Observable, corePoint3D, coreSound o Image, representaciones de estos datos de uso común entre módulos pero cuyos detalles de implementación queremos mantener ajenos.
\\

A este nivel no se observan más cambios, la mayor parte del trabajo se ha centrado en el módulo de Producción y en la aplicación principal, detallados en las secciones siguientes. 

\subsubsection{Diseño en UML - Núcleo de la Interfaz}
\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/DiagramadeclasesUML_CuartaDemo_core.png}
\end{center}
\caption{ \label{F_DiagramadeClasesUML_CuartaDemo_core} Diagrama de Clases UML, Cuarta Demo. Core.}
\end{figure}
En IPerceptVideo se añaden los métodos SetCameraRecording que activa o desactiva la grabación de forma sincronizada de todas las cámaras y SetUseRecording, que activa o desactiva el uso de de los ficheros de video grabados para alimentar al sistema. También se añaden métodos para obtener información sobre área, orientación, excentricidad de la forma observada, o cantidad de segmentos, dirección o intensidad de velocidad en la detección de movimiento. Se crea la interfaz IEntity para poder manipular diferentes implementaciones de entidades, destacando los métodos OnStart y OnUpdate para la actualización de estado durante su ejecución en el pipeline de producción. 

\subsubsection{Diseño en UML - Módulo de Persistencia}

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/DiagramadeclasesUML_CuartaDemo_ipersistence.png}
\end{center}
\caption{ \label{F_DiagramadeClasesUML_CuartaDemo_ipersistence} Diagrama de Clases UML, Cuarta Demo. Persistencia.}
\end{figure}

Por considerarse de interés general se añaden atributos a la clase IEntityPersistence. Algunos de estos datos son el tiempo de vida de la entidad, si es una entidad que hace uso de colisiones o por el contrario no interviene en ellas. 
\\

También se añaden junto con las coordeanas de su posición, los atributos de velocidad y aceleración. Por último se añaden métodos que permitan cambiar varios atributos a la vez, que típicamente son actualizados en conjunto, lo que ahorran accesos e interrupciones entre hilos al hacer las asignaciones juntas en un único bloque. 
\\

Antes de pasar al siguiente apartado, comentar que aunque la mayor parte de la aplicación usa el propio puntero a una entidad como identificador único, también se ha considerado añadir un método para obtener el ID que identifica a cada instancia en todas las clases. Éste es un mecanismo más explícito y puede evitar confusiones al trabajar a diferentes niveles.
\\
 
\vspace{140 mm}
 
\subsubsection{Diseño en UML - Módulo de GUI}

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/DiagramadeclasesUML_CuartaDemo_igui.png}
\end{center}
\caption{ \label{F_DiagramadeClasesUML_CuartaDemo_igui} Diagrama de Clases UML, Cuarta Demo. GUI.}
\end{figure}

En este sección, los cambios en la interfaz se reducen a añadir los métodos SetCameraRecording y SetUseRecording en GUIGenericController, para indicar que se desea grabar de las cámaras o utilizar los videos grabados.

\subsubsection{Diseño en UML - Módulo de Aplicación}
\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/DiagramadeclasesUML_CuartaDemo_vox.png}
\end{center}
\caption{ \label{F_DiagramadeClasesUML_CuartaDemo_vox} Diagrama de Clases UML, Cuarta Demo. Aplicación.}
\end{figure}

Éste es el componente donde más elementos se han añadido. Teniendo gran parte de las herramientas disponibles este componente hará uso de ellas para crear una propuesta de instalación que sirva como ejemplo.
\\

\begin{itemize}
\item Application: Añadimos los métodos SetCameraRecording y SetUseREcording para indicar que deseamos grabar de forma sincronizada de todas las cámaras o por lo contrario alimentar al sistema con imágenes obtenidas de ficheros de video en lugar de usar las imágenes en vivo de las cámaras. También añadimos los métodos AddNewentityIntoCurrentWorld y RemoveEntityFromCurrentWorld para indicar que deseamos añadir una nueva entidad a la escena actual o que queremos eliminarla.
\item NavigatorController: Extendemos su responsabilidad y lo renombarmos a RunningSceneController. Mantiene la responsabilidad de navegar dentro de la escena y añade la responsabilidad de crear y mantener la representación del usuario, extrayendo esta tarea del hilo de ejecución del render de la escena mejorando así el rendimiento gráfico, así como es el encargado de ejecutar y actualizar la creación de contenidos haciendo uso del controlador ContentCreationController.
\item ContentCreationController: Controlador que implementa la creación de contenidos durante una sesión en curso teniendo en cuenta información sobre la interacción del usuario. Mantiene un registro de las entidades que existen actualmente en la escena, tiene acceso a la información obtenida por percepción y decide qué contenidos crear en consecuencia. Cabe destacar que como debe estar actualizado con los cambios sucedidos en la escena se trata de un observador que observa dos objetos: El controllador de sesión que indicará cuándo hay un cambio de escena, y las Entidades que le indicarán cuando mueren o son destruídas. Como Observador tiene un método Notified donde recibe un punto a la entidad notificadora, así como varios algunos atributos indicadores, para que los Observables puedan comunicar mensajes usando esta misma interfaz.
\item OXStandAloneEntity: Esta clase es una implementación de Prod3dEntity (que a su vez lo es de IEntity). Implementa un tipo de entidad que se muestra por sí misma y de forma independiente. Representa un tipo de entidad básica. Cabe destacar sus métodos OnStart y OnUpdate llamados al comienzo de cada ciclo del pipeline de producción y que permite que cada entidad sea capaz de actualizar su estado según indique su implementación. También tiene los métodos OnCollisionCall y OnUserCollisionCall, llamadas cuando se detecta una colisión de esta entidad contra otra entidad o contra la representación del usuario. Por último mencionar que conserva datos como tiempo de vida, métodos para cambiar el tono o volúmen de los audios que reproduce, etc.
\item OXBoidsEntity: Igualmente es una implementación de Prod3DEntity (que a su vez lo es de IEntity). Implementa un tipo de entidad que pertenece a una especie gregaria y cuyo comportamiento se define por su conocimiento del entorno y del de los integrantes de su propia especie. Hereda también de CommonSwarmIndividual, una implementación básica que se ofrece en el módulo iCog del SDK.
\end{itemize}

\subsubsection{Diseño en UML - Módulo de Percepción}
\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/DiagramadeclasesUML_CuartaDemo_ipercept.png}
\end{center}
\caption{ \label{F_DiagramadeClasesUML_CuartaDemo_perception} Diagrama de Clases UML, Cuarta Demo. Percepción.}
\end{figure}
\begin{itemize}
\item PerceptVideo: Se añaden los métodos SetCameraRecording y SetUseRecording para grabar o usar video y métodos para obtener datos como dominancia lateral (en qué vista tiene mayor área), orientación (inclinación) o excentricidad (si la forma es más lineal o redondeada).
\item PresenceDetection: GetPresenceArea, GetPresenceOrientation y GetPresenceEccentricity.
\item MotionDetection: Destacan GetMotionElements que devuelve los segmentos de imagen donde se ha detectado movimiento, y GetMotionAtCoords que dada unas coordenadas ofrece una velocidad de movimiento.
\end{itemize}

\subsubsection{Diseño en UML - Módulo de Producción}
\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/DiagramadeclasesUML_CuartaDemo_iprod.png}
\end{center}
\caption{ \label{F_DiagramadeClasesUML_CuartaDemo_production} Diagrama de Clases UML, Cuarta Demo. Producción.}
\end{figure}

Este es otro apartado donde más trabajo se ha realizado durante esta fase, aunque su reflejo en el diseño de la interfaz es menor en comparación.
\\

Se destacan los siguientes apartados:
\\

\begin{itemize}
\item MeshFactory: Esta clase se ha creado con el fin de crear una malla3D a partir de una nube de puntos. Para ello se ha hecho uso del algoritmo de Marching cubes que predefine un espacio discretizado en vóxeles (unidad espacial con forma de cubo) y un catálogo de primitivas geométricas. Dada una configuración de vecindad de los puntos de la nube de puntos se crea una malla 3D combinando estas primitivas. 
\\

Destacar que este algoritmos es uno de los clásicos y más extensamente utilizados para reconstrucción de superficies a partir de datos volumétricos, usado especialmente para representaciones 3D de metabolas o metasuperficies. En este caso se hacen modificaciones en el cálculo de la distancia para conseguir un acabado más destacado o exagerado de metabola, de forma que si dos puntos alejados están lo suficientemente cerca, teniendo en cuenta sus tamaños, se crea una superficie más suavizada que los conecta, en lugar de dejar el hueco que existe entre ellos.
\\

También comentar que la creación de esta malla no se realiza en el hilo del render, sino que es el hilo del controllador RunningSceneController el encargado de crear la representación gráfica del usuario para evitar interrupciones y pérdida de rendimiento en el render.

\item Prod3DEntity: Destacar que extiende de la clase Subject, ya que será observada por ContentCreationController a quién notificará de sus cambios. Se añaden también atributos como fuentes de audio, tiempo de vida, controladores de animación y se extiende la interfaz para añadir más métodos para obtener o asignar valores a sus atributos. 
\\

Se añaden especialmente métodos para obtener o asignar atributos en bloque que suelen ser accedidos juntos, de forma que pueden hacerse en una sóla llamada, lo que permite reducir el número de peticiones y bloqueos de la entidad.

\item MainProd: Finalmente se añade a MainProd una serie de atributos y métodos para dar cabida a las nuevas funcionalidades que se van a desarrollar. Los más destacados son los controladores de animación de la escena para permitir que los objetos3D que tengan asociados una animación sean capaces de reproducirlo.
\\

También se añade acceso para cambiar el color de fondo de la escena o el color e intensidad de la niebla, un contenedor de audios no relativos para reproducir sonidos de fondo que siempre se escuchen a igual intensidad, independientemente de la posición del usuario, y, finalmente, una serie de efectos de postproceso predefinidos como Invert, Bloom, Blur o Cartoon para añadir de forma simple un acabado diferente al render.
\end{itemize}

\subsubsection{Diseño en UML - Módulo de IA (Cog)}
\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/DiagramadeclasesUML_CuartaDemo_icog.png}
\end{center}
\caption{ \label{F_DiagramadeClasesUML_CuartaDemo_icog} Diagrama de Clases UML, Cuarta Demo. IA (Cog).}
\end{figure}

Este módulo tiene la intención de ofrecer componentes de IA básicos que el desarrollador pueda usar directamente o extender. En este caso añadimos una implementación básica del comportamiento de un individuo que forma parte de un enjambre. Está inspirado en la implementación clásica del Boids, un programa creado por Craig Reynolds \cite{CRAIGREYN}.

\begin{itemize}
\item CommonSwarmIndividual: Se ofrece en el módulo de icog una implementación de una IA genérica que puede servir para crear comportamientos de indivuos de una especie, que contiene los componentes básicos de Boids (separación, cohesión y alineamiento) y extiende algunos componentes adicionales (límites, aleatoriedad, evasión y atracción). 
\\

Cada individuo tiene conocimiento de su entorno, aunque para evaluar su estado actual usará sólo información que esté dentro de campo de visión. Teniendo en cuenta todas sus necesidades (separación, cohesión, alineación, límites, evasión, atracción y randomicidad) y el factor de peso de cada una de ellas, el individuo modificará su comportamiento expresado como posición, velocidad y aceleración para cumplir esos objetivos. 
\\

Con esta IA pueden representarse comportamientos como el de una nube de moscas, un banco de peces o una bandada de aves. También puede extenderse o combinarse para crear otros tipos de comportamientos o ejemplos de inteligencias articiales más complejas como los agentes inteligentes.

\end{itemize}

\section{Implementación}

Los módulos afectados en la presente fase son los siguiente:

\begin{itemize}
\item core: Añadida la clase IEntity, modificadas las interfaces de IPerceptVideo.
\item igui: añadidas interfaces gráficas para grabar la captura de imagen en video o cargar video en lugar d e usar imagen en vivo. Cambios estético y ocultación de interfaces de elementos que se dejan como trabajo futuro.
\item ipercept: Se implementa la captura sincronizada de múltiples pantalla o la arga de múltiples videos. Se añade flujo óptico al detector de movimiento, y se añade cálculo de área, orientación y excentricidad al detector de presencia.
\item ipersistence: Se añaden campos a los objetos usuario, mundo y entidad y se añaden algunos métodos para mejorar su uso.
\item iprod: Se añade capacidad para crear una malla 3d a partir de una nube de puntos, se plantea un esquema de proyección de la representación del usuario para interactuar con objetos de la escena. Se prepara esquema para la carga, destrucción y actualiazación de entidades en la escena. Se añade algunos elementos de personalización de la escena como música de fondo, color de fondo, color e intensidad de niebla y efectos de postproceso.
\item vox: Se convierte el NavigatorController en RunningSceneController. Se crea el ContentCreationController. Se crean dos implementaciones de entidades y se crea de esta forma una propuesta de instalación a partir del kit de desarrollo creado.
\item icog: implementación de un modelo de IA que simula el comportamiento de enjambres.
\end{itemize}

\subsubsection{Núcleo de la Interfaz}

En esta ocación los cambios realizados en el núcleo de la Interfaz son menores, añadiendo principalmente algunos métodos a clases ya existentes para acceder a más información. Al tratarse sólo de una interfaz podrá verse información de mayor interés en los apartados correspondientes, donde se implementan estas funcionalidades. Se listan los cambios a continuación:
\\

src$\backslash$core$\backslash$IPercept$\backslash$IPerceptVideo\\

Como se ha comentado en los apartados anteriores, se añaden los métodos SetCameraRecording destinada a activar o desactivar la grabación de forma sincronizada de todas las cámaras y SetUseRecording, para activar o desactivar el uso de de los ficheros de video grabados para alimentar al sistema. Así como métodos para obtener información sobre dominancia, orientación, excentricidad, número de elementos móviles, velocidad, o la nube de puntos que representa al usuario.

\begin{lstlisting}[language=C++]
...
 virtual bool SetCameraRecording(const bool &value)
 virtual bool SetUseRecording(const bool &value, const std::string &url)
 virtual void GetMainLateralDominance(corePoint3D<double> &result)
 virtual void GetMainOrientation(corePoint3D<double> &result)
 virtual void GetMainEccentricity(corePoint3D<double> &zero_means_round)
 virtual void GetFeatureWeightedPositions(const std::string &feature, 
              std::map<int, std::vector<corePDU3D<double>>> &result, 
              const float &scale=1)
 virtual std::vector<MotionElement> GetMotionElements()
...
\end{lstlisting}

src$\backslash$core$\backslash$IPercept$\backslash$video$\backslash$IPresenceDetection\\

Se añaden métodos para obtener información sobre área, orientación, excentricidad de la forma observada a partir de la imagen de sustracción de fondo. El área está medido en píxeles, la orientación en radianes (partiendo del eje de ordenadas, en sentido contrario a las agujas del reloj) y la excentricidad es un valor que variará entre 0.0, que indica una forma circular, y 1.0m que indica una forma de línea.

\begin{lstlisting}[language=C++]
...
 virtual void GetPresenceArea(double &area_inpixel_units)
 virtual void GetPresenceOrientation(double &radians_counterclockwise)
 virtual void GetPresenceEccentricity(double &cero_means_round)
...
\end{lstlisting}

src$\backslash$core$\backslash$IPercept$\backslash$video$\backslash$IMotionDetection\\
Más información añadida es la cantidad de segmentos en movimiento, y la dirección e intensidad de movimiento dadas unas coordenadas.

\begin{lstlisting}[language=C++]
...
 virtual std::vector<MotionElement> GetMotionElements()
 virtual corePoint3D<float> GetMotionAtCoords(corePoint2D<int> coords)
...
\end{lstlisting}

src$\backslash$core$\backslash$IProd$\backslash$IEntity\\

Se crea la interfaz IEntity para poder manipular diferentes implementaciones de entidades. A partir de ahora Prod3DEntity implementará esta interfaz. Igualmente, cuando la aplicación desee crear entidades más complejas que Prod3DEntity (que contiene sólo elementos básicos para su representación), heredarán e esta última para extender sus capacidades. Destacan los métodos OnStart y OnUpdate, para la actualización de estado durante su ejecución en el pipeline de producción. También se añaden métodos que permiten acceder a varios atributos en bloque, lo que reduce el número de interrupciones necesarias en la sincronización de hilos. Se comentan algunos de los métodos:

\begin{itemize}
\item OnStart(): Llamado una única vez tras cargar la entidad en la escena.
\item OnUpdate(): Llamado cada vez, al comienzo de la ejecución de un nuevo render de imagen. Es el lugar adecuado para evaluar el comportamiento que debe realizar como la gestión de sus animaciones, trasladarla, o aplicar los cambios que una IA asociada quisiera hacer.
\item OnCollisionCall: Llamado cada vez, al comienzo de la ejecución de un nuevo render, antes de que se llame a ningún Update de ninguna entidad, sólo si el motor gráfico indica que dicha entidad ha incurrido en una colisión con otra entidad.
\item OnUserCollisionCall: Actúa de igual manera que el caso anterior, pero en la situación especial de que se trate de la representación del usuario, indicando en este caso la posición y velocidad del elemento del avatar con el que colisionó.
\item IsReadyToDie: Las entidades no pueden retirarse en cualquier momento ya que el motor gráfico puede seguir usándola y provocar fallos si la entidad es destruída de imprevisto. Todas las tareas que añadan o eliminen objetos en la escena se realizan a través de unas colas de carga y borrado, que añaden o borran las entidades en el momento adecuado. Con este método una entidad puede indicar que debe evitarse su uso y que ya puede ser retirada del grafo de la escena, así ésta será retirada de forma segura al comienzo del próximo render.
\end{itemize}

\begin{lstlisting}[language=C++]
class _COREEXPORT_ IEntity
{public:
  virtual ~IEntity(){}
  virtual void DeletePersistence()
  ...
  virtual void SetPositionVelocityAcceleration(...)
  virtual void GetPositionVelocityAcceleration(...)
  virtual void OnStart()
  virtual void OnUpdate();
  virtual void OnCollisionCall(IEntity *otherEntity) 
  virtual void OnUserCollisionCall(core::corePDU3D<double>collisionInfo)
  virtual bool IsReadyToDie()
  virtual bool IsCollidable()
  virtual void SetCollidable(const bool &value)
};
\end{lstlisting}

src$\backslash$core$\backslash$types\\

Lo más destacado de los cambios realizados en esta sección son las clases Subject y Observer, que aportan una implementación muy básica del patrón de diseño Observador-Observable. El problema que resuelve este patrón es el de mantener notificada a una entidad de los cambios sufridos por otra entidad, sin necesidad de que consulte continuamente por ella.

\begin{itemize}
\item Subject: Se trata de la entidad observable. Contiene un lista de Observadores. Cuando esta entidad es actualizada basta con llamar a su método Notify(), donde recorrerá su listado de observadores para notificarles de que ha sufrido un cambio. Como extensión al patrón de diseño básico, se añaden algunos campos de información que sirvan como intercambio de mensajes.

\begin{lstlisting}[language=C++]
class Subject {
 std::vector< class Observer* > observers;
 public:
 int ObserversCount() 
 void attach(Observer *observer) 
 void detach(Observer *observer) 
 void detach_all() 
 virtual void Notify(void* callinginstance=NULL, 
                     const std::string &tag="", 
                     const int &flag=0) 
 { ... (*iter)->Notified(callinginstance, tag, flag); }
}
\end{lstlisting}

\item Observer: Se trata de la entidad Observadora. Debe implementar el método Notified(), y una vez conectada a la entidad Observable será notificada cada vez que la Entidad Observable lo indique. Qué código implementa la clase Observadora es completamente transparente para la Observable. En este caso, la clase base no ejecuta ningún código al ser Notificada.

\begin{lstlisting}[language=C++]
class Observer {
 public:
  virtual void Notified(void* callinginstance=NULL, 
                        const std::string &tag="", 
                        const int &flag=0) {}
};
\end{lstlisting}


\end{itemize}

\subsubsection{Módulo de Persistencia}
Se han añadido modificaciones que añaden nuevos campos a los objetos. También se han añadido métodos que permiten leer o escribir varios campos en una sola llamada. Finalmente, se añade un método GetID() que nos devuelve el ID asignado al objeto en la base de datos. Estos cambios son muy simples y no aporta mayor detalle que lo visto en secciones anteriores. En cualquier caso, a modo de ilustración sobre este apartado, puede consultarse la sección equivalente en la página \pageref{Etapa3detallesImp:ModuloPersistencia}.

\subsubsection{Módulo de GUI}

Como añadido en la vista de Configuración, sección Cámaras, se añaden un botón que permite iniciar o detener la grabación sincronizada de múltiples cámaras. 
\\

Igualmente se añade un campo de texto donde podemos escribir el nombre de un fichero de video que será usado como prefijo, al que el sistema añadirá el texto "cam" y el número que indica un índice de cámara comenzando a partir de 1. Este "basename" es usado a la hora de cargar ficheros de video, uno por cada cámara para alimentar al sistema, que deben estar en el mismo directorio de la aplicación donde se generan. 
\\

Finalmente, hay un radio button que permite alternar entre el uso de la captura en vivo desde las cámaras o la carga de dichos ficheros para usar la imagen de video en su lugar.

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/etapa5_grabarousarvideo_gui.png}
\end{center}
\caption{ \label{F_GUIEsteticos_Etapa5} Cambios en la sección de configuración que permiten grabar la captura de imagen en vivo en un video por cada cámara o usar videos ya grabados en lugar de la captura en vivo}
\end{figure}

Aparte de lo comentado, los cambios realizados en esta sección están limitados a la sustitución de elementos estéticos como son las imágenes de fondo de las diferentes vistas. Para ello ha bastado con sustituir la imagen de fondo en el directorio de recursos gráficos de GUI. 
\\

También se ocultan algunos elementos de la interfaz que inicialmente fueron añadidos pero que pretendían dar acceso a funcionalidades que han quedado fuera del alcance del proyecto. 

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/etapa5_captura_gui.png}
\end{center}
\caption{ \label{F_GUIEsteticos_Etapa5} Retoques gráficos en las vistas de la aplicación}
\end{figure}

\subsubsection{Módulo de Percepción}
Durante esta fase se extienden las capacidades de Percepción definidas durante el desarrollo de la Etapa4 "Detección, Navegación e Interacción", que puede consultarse a partir de la página \pageref{Etapa4_Impl:Percepcion}. 
\\

De esta forma, se revisitan los componentes PresenceDetection y MotionDetection para añadir más elementos a su estudio.
\\

src$\backslash$ipercept$\backslash$video$\backslash$PresenceDetection\\

Este componente obtenía, mediante sustracción de fondo una imagen que representaba la presencia de un usuario en el interior de la zona de estudio. Como puede verse en la etapa anterior, hacíamos uso del cálculo de los momentos de orden cero y uno para obtener el área y centro de masas de la imagen obtenida por el módulo de detección de presencia. 
\\

Siguiendo la misma línea, en esta ocasión, usaremos además los momentos de orden dos, para obtener la orientación y la excentricidad de la forma detectada como presencia. La orientación está medida en radianes (tomando el sentido como contrarreloj y resolviendo la ambigüedad de la cabeza o cola de la elipse, partiendo del eje de ordenadas y tomando el semieje mayor de la elipse). Para un mejor entendimiento del uso de los momentos para obtener este tipo de información se recomienda leer el artículo "Simple Image Analysis by Moments" de Johannes Kilian \cite{JOHANNESK}, del que se han obtenido los cálculos necesarios para obtener los cálculos que se muestran.
\\

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/etapa5_momentosorientacion_percepcion.png}
\end{center}
\caption{ \label{F_GUIEsteticos_Etapa5} Cálculo de la orientación de un blob a partir de los momentos}
\end{figure}


\begin{lstlisting}[language=C++]
presence_orientation = 0.0;
double mu20_minus_mu02 = (*foreground_moments).mu20 - (*foreground_moments).mu02;
double mu11 =(*foreground_moments).mu11;
double base_presence_orientation=0.5*atan(2*mu11/mu20_minus_mu02);
if (mu20_minus_mu02==0.0)
{ if (mu11>0) presence_orientation = M_PI * 0.25;
  if (mu11<0) presence_orientation = M_PI * -0.25;			
} else if (mu20_minus_mu02>0.0)
{ if ((mu11>0)||(mu11<0))presence_orientation=base_presence_orientation;
} else
{ if (mu11>0) presence_orientation=base_presence_orientation+(M_PI*0.5);
  if (mu11<0) presence_orientation=base_presence_orientation-(M_PI*0.5);
}
\end{lstlisting}

La excentricidad es un valor que variará entre 0.0 y 1.0. Un valor cercano a cero indica que la forma estudiada es similar a una circunferencia. Por el contrario un valor cercano a 1.0 indica una forma de línea. En  nuestro caso, un ejemplo claro que da un valor cercano a 1.0 es cuando la presencia se encuentra en una pose recta de pie o acostada. 

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/etapa5_momentosexcentricidad_percepcion.png}
\end{center}
\caption{ \label{F_GUIEsteticos_Etapa5} Cálculo de la excentricidad de un blob a partir de los momentos}
\end{figure}

\begin{lstlisting}[language=C++]
presence_eccentricity = 1.0;
double mA=pow((*foreground_moments).mu20-(*foreground_moments).mu02,2);
double mB=pow((*foreground_moments).mu11, 2)*4.0;
double mC=pow((*foreground_moments).mu20+(*foreground_moments).mu02,2);
if (mC>0.0) presence_eccentricity=(mA-mB)/mC ;
\end{lstlisting}

\vspace{5 mm}

src$\backslash$ipercept$\backslash$video$\backslash$MotionDetection\\

El cambio realizado en este detector es que se añade cálculo de flujo óptico para ofrecer más información de movimiento. 
\\

El Flujo Óptico es una técnica que nos indica el movimiento aparente de un objeto en una imagen. Como puede verse en la figura \ref{F_FlujoOptico_Etapa5} se trata de encontrar la distancia h que hace que F(x)=G(x)=F(x+h), o que lleva a una diferencia mínima entre G(x) y F(x+h). A la izquierda se muestra este problema de forma esquemática, a la derecha se ilustra el cálculo de un caso simple de una muestra monodimensional (si nos centramos en el estudio del eje x). En nuestro caso utilizamos una implementación que ofrece  OpenCV basada en el algoritmo de Lucas Kanade \cite{LUKASK81}. 

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/etapa5_problemofopticalflow_perception.png}
\end{center}
\caption{ \label{F_FlujoOptico_Etapa5} Ilustración del problema del flujo óptico, Lukas Kaneda \cite{LUKASK81}}
\end{figure}

Una de las mejoras que aportaba el algoritmo de Lukas Kanade era que permitía realizar menos comparaciones de imágenes para ofrecer una correspondencia, utilizando información extrínseca, en este caso discriminar por la proximidad lineal en las transformaciones del patrón que se busca, que frecuentemente son cercanas y permiten acelerar el cálculo.

\begin{lstlisting}[language=C++]
...
 velx = cvCreateImage(size,IPL_DEPTH_32F,1);
 vely = cvCreateImage(size,IPL_DEPTH_32F,1);	
if (previous_frame == NULL)
{ previous_frame = cvCreateImage(size,IPL_DEPTH_8U,1);
  cvCvtColor(img,previous_frame,CV_RGB2GRAY);
} else
{ cvReleaseImage(&previous_frame);
  previous_frame = current_frame;
}
current_frame = cvCreateImage(size,IPL_DEPTH_8U,1);
cvCvtColor(capture, current_frame, CV_RGB2GRAY);		
...
 velx_Mat = cvCreateMat(size.height, size.width, CV_32FC1); 
 vely_Mat = cvCreateMat(size.height, size.width, CV_32FC1);

//Calculating dense Optical Flow (Lucas Kanade)
block_size.height = block_size.width = 3;
if (previous_frame && current_frame)
 cvCalcOpticalFlowLK(previous_frame,current_frame,block_size,velx,vely);
cvConvert(velx, velx_Mat);
cvConvert(vely, vely_Mat);
\end{lstlisting}
\vspace{5 mm}

De esta forma, dadas dos imágenes (actual y previa) y un tamaño de ventana para agrupar píxeles para la búsqueda de patrones similares, esta función nos ofrece, por cada píxel dos matrices, una dando el desplazamiento estimado en el eje de ordenadas y otra para el eje de abscisas.

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/etapa5_opticalflow_perception.png}
\end{center}
\caption{ \label{F_FlujoOptico_Etapa5} Detección de Movimiento: mostrando la imagen de histórico de movimientos a la derecha, y una muestra de flujo óptico tomada cada 50 píxeles a la izquierda, donde un punto implica que no hay movimiento y una línea refleja el movimiento detectado.}
\end{figure}

\vspace{7 mm}
src$\backslash$ipercept$\backslash$PerceptVideo\\

Llegados a este punto tenemos una colección de detectores que estudian imágenes y son capaces de ofrecer una información enriquecida. El principal objetivo que queremos conseguir con ella, es que una aplicación pueda obtener, además de esa información de forma puntual, una representación de la presencia del usuario con sus datos. Para terminar este apartado, veremos cómo creamos un modelo del usuario a partir del cual poder crear una malla3D para su visualización en producción, o una representación a modo de proyección para colisionar con objetos que se encuentren delante.
\\

Desde este punto de vista, y aunque es realmente interesante el estudio del reconocimiento postural, se ha optado por una solución más simple que cubran los objetivos del presente proyecto y que entren dentro de su alcance.En cualquier caso, me gustaría hacer referencia a algunos artículos sobre esta materia muy interesantes. Sería muy interesante poder continuar estas líneas de estudio como posible trabajo futuro.
\\

Algunos de los trabajos analizados se basan en el estudio de un modelo de voxels del usuario obtenido a partir de mútiples cámaras como puede leerse en el trabajo "Articulated Body Posture Estimation from Multi-Camera Voxel Data" de Ivana Mikic \cite{IVANAM}. Otros en o la detección de partes del cuerpo centrando el estudio en las articulaciones del mismo para estimar la pose del esqueleto del usuario como puede leerse en " Estimating Human Body configurations using Shape Context Matching" de Greg Mori \cite{GREGMJM}. 

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/postural.png}
\end{center}
\caption{ \label{F_ReconocimientoPostural_Etapa5} Ilustraciones de los trabajos mencionados sobre reconocimiento postural de Ivana Mikic et al.\cite{IVANAM} (izquierda) y Greg Mori et al.\cite{GREGMJM} (derecha)}
\end{figure}

En la misma línea, no quisiera dejar atrás hacer referencia al trabajo de Juergen Gall, Carsten Stoll, Edilson de Aguiar et al. que puede leerse en el artículo "Motion Capture Using Joint Skeleton Tracking and Surface Estimation" donde proponen un método para adaptar una plantilla de una malla3D y un esqueleto predefinidos a la captura en vivo de una imagen y con la que parecen conseguir resultados excelentes incluso en acciones que suceden a gran velocidad \cite{JUERGEN}.
\\


\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/etapa5_postural_percepcion.png}
\end{center}
\caption{ \label{F_ReconocimientoPosturalJuergen_Etapa5} Ajuste de una malla3D dada y esqueleto a modo de plantilla sobre una captura de imagen \cite{JUERGEN}}
\end{figure}

En nuestro caso crearemos una representación del usuario basada en una nube de puntos que, simplificada, nos permitirá representar al usuario así como proyectar su interacción con la escena y obtener información del usuarios sobre esas interacciones.\\

Se usará la sustracción de fondo para obtener un modelo del usuario. Añadiendo los datos de detección de movimiento obtendremos la velocidad de movimiento de un punto concreto del avatar. Sin embargo, no habrá distinción explícita de miembros ni postura. Aunque se considera un apartado muy interesante para trabajo futuro. 
\\

Uno de los principales puntos a resolver es conseguir una representación lo bastante ligera como para generar una alta tasa de resultados por segundo. Para ello obtenenos la intersección proyectada entre los blobs de las diferentes fuentes de video. Aún así, el mayor problema de trabajar con nubes de puntos es la gran candidad de datos que hay que manejar. A modo ilustrativo si tuviéramos un blob de 10x10 píxeles tendríamos 100 elementos para estudiar, si llevamos esto a 500x500 píxeles la cuenta sube a 250.000 elementos.
\\

Por este motivo, dada una nube de puntos en bruto a partir del blob de presencia obtenido por el detector de presencia, la simplificación de la nube de puntos sigue los siguientes pasos:

\begin{itemize}
\item Bajar la resolución: El primer paso consiste en escalar la imagen. No necesitamos una representación a resolución completa.
\item Reducción de la nube de puntos original: Se ha planteado una posible solución que ofrecería una representación en forma de nube de puntos con pesos asignados, similar a la original, donde cada punto puede tener un tamaño mayor a los otros. 
\\

Para conseguir esto se siguen los siguientes pasos:
\begin{itemize}
\item Clasificación espacial de la nubes de puntos mediante un árbol de búsqueda R-Tree. 
\\

Este árbol balanceado da especial importancia a la agrupación de sus nodos en bounding boxes (envolturas rectangulares) mínimos que engloben las dispersiones locales de objetos. Permite realizar búsquedas rápidas dentro de una colección de elementos a los que se les puede asignar un tamaño, y sobre la que se pueden realizar consultas mediante vecindades usando radios de distancia. 
\\

R-Tree no garantiza buenos rendimientos en los peores casos, pero demuestra ser muy eficiente en situaciones reales y es muy usado actualmente en las extensiones para búsquedas eficientes de entidades geolocalizadas dada una localización y una vecindad.

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/etapa5_rtree.png}
\end{center}
\caption{ \label{F_RTree_Etapa5} Representaciones en forma de árbol de nodos, 2D y 3D de un R-Tree \cite{RTREE}.}
\end{figure}

\item Simplificación mediante escarbado: Dada esta representación se toma un nodo aleatorio y se calcula un porcentaje de llenado para vecindades, empezando por un tamaño mayor y escalando hacia tamaños más pequeños hasta encontrar uno que satisfaga la condición de llenado. Si no satisface la condición con un tamaño mínimo el nodo es descartado. De esta forma podemos pasar de una representación del usuario que, como en el caso de la fingura \ref{F_Escarbado_Etapa5} puede contener contiene miles de puntos a una representación simplificada que para una forma similar a la mostrada puede estar alrededor de 30 puntos de diferentes tamaños.

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/etapa5_simplificacionnubepuntos_perception.png}
\end{center}
\caption{ \label{F_Escarbado_Etapa5} Simplificación de una nube de puntos mediante escarbado.}
\end{figure}

\end{itemize}
\end{itemize}

\begin{lstlisting}[language=C++]
void PerceptVideo::ObtainPresenceVolumeAsWeightPoints(std::map<int, 
  std::vector<corePDU3D<double>>>&weighted_points, 
  const float &scale)
...
for (int i = n_steps; i >= 0; )
{ bool candidate_step_found = false;
  Rect3F search_rect(new_point.position.x-search_delta_window,
    new_point.position.y-search_delta_window,
    new_point.position.z-search_delta_window, 
    new_point.position.x+search_delta_window,
    new_point.position.y+search_delta_window,
    new_point.position.z+search_delta_window);
  search_results.clear();
  int overlapping_size=spatial_index.Search(search_rect.min,
    search_rect.max,RegisterPointIDIntoSearchResults,NULL);
  float n_deltas=search_delta/delta;
  third_delta=(is3D)?n_deltas:1.0;
  float success_criteria = 0.95;
  
  if((overlapping_size>(success_criteria*pow(n_deltas,((is3D)?3:2))))&&
    (overlapping_size>1)) 
  { new_point = relative_points[iter_isp->first];
    image_space_points[iter_isp->first].y;
    if (weighted_points.find(search_delta)!= weighted_points.end())
      weighted_points[search_delta].push_back(new_point);
    else
    { std::vector<corePDU3D<double>> new_vector;
      new_vector.push_back(new_point);
      weighted_points[search_delta] = new_vector;
    }
    for (std::vector<int>::iterator diter = search_results.begin();
         diter != search_results.end(); diter++)
    { spatial_index.Remove(search_rect.min, search_rect.max, *diter);
      image_space_points.erase(*diter);
      candidate_step_found = true;
    }
  }
  else if (( i==0 ) || overlapping_size == 1 )
  { spatial_index.Remove(search_rect.min,search_rect.max,iter_isp_id);
    image_space_points.erase(iter_isp_id); 
  } ...
\end{lstlisting}

Recordar que PerceptVideo actúa como controlador principal del análisis de video. También recordar que, aunque la arquitectura está preparada para trabajar con múltiples fuentes (creando los detectores necesarios que funcionarán en paralelo por cada fuente), en el alcance de este proyecto nos centraremos en el uso de una única fuente.\\

Se recuerda también que la solución adoptada para la orientación y combinación de múltiples cámaras consiste en una orientación predefinida de tipo Cave (un cubo, con sus seis lados: derecha, izquierda, frontal, trasero, superior, inferior). Para coordinar los datos se alinean los centros de cada imagen y se proyecta la información en profundidad, utilizando (por normal general) la intersección, unión o promedio de ambos. Igualmente se plantea revisitar esta sección como trabajo futuro, tanto para mejorar la eficiencia de la solución actual como para extenderla y permitir combinar fuentes de video en cualquier orientación.
\vspace{20 mm}

\subsubsection{Módulo de Producción}

Este es una de las secciones donde más trabajo se ha realizado durante esta fase. Podemos distinguir tres apartados: Representación del Avatar, Extensión de funcionalidades asociadas a la gestión de entidades en la escena y extensión de capacidades del controlador principal de producción para personalizar el entorno.
\\

Se destacan los cambios realizados en las siguientes clases:
\\

\begin{itemize}
\item MeshFactory: Como comentábamos en la sección de diseño, esta clase se ha creado con el fin de crear una malla3D a partir de una nube de puntos. Para ello se ha hecho uso del algoritmo de Marching cubes que predefine un espacio discretizado en vóxeles (unidad espacial con forma de cubo) y un catálogo de primitivas geométricas. 
\\

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/etapa5_marchingcubesprimitives_production.png}
\end{center}
\caption{ \label{F_MarchingCubesPrimitives_Etapa5} Marching Cubes: Primitivas geométricas (facets) \cite{MCUBESW}}
\end{figure}

Dada una configuración de vecindad de los puntos de la nube de puntos se crea una malla 3D combinando estas primitivas. 
\\

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/etapa5_marchingcubessamples_production.png}
\end{center}
\caption{ \label{F_MarchingCubesSampless_Etapa5} Marching Cubes, construcción de metasuperficies y Voxelización del espacio \cite{PAULB94}}
\end{figure}

En la figura \ref{F_MarchingCubesSampless_Etapa5} puede verse en gis, la construcción de un mismo modelo, con mayor o menor resolución según el número de pasos en los que se subdivide el espacio. El modelo de datos es exactamente el mismo en los cinco casos, es la misma nube de puntos. Subdividiendo el espacio en más o menos vóxeles se puede conseguir una representación con mayor o menor resolución. Notar también que en estas muestras de render no se usan técnicas de shading como Phong para interpolar la iluminación dentro de una cara. Dicho de otra forma, el color en estas imágenes es exactamente igual para todos los píxeles que caen dentro de una misma cara, en todos los casos. En esta figura, las formas más suavizadas se ven así porque la geometría del mallado 3D tiene una cantidad muy elevada de caras.


Destacar que este algoritmos es uno de los clásicos y más extensamente utilizados para reconstrucción de superficies a partir de datos volumétricos, usado especialmente para representaciones 3D de metabolas o metasuperficies. En este caso se hacen modificaciones en el cálculo de la distancia para conseguir un acabado más destacado o exagerado de metabola, de forma que si dos puntos alejados están lo suficientemente cerca, teniendo en cuenta sus tamaños, se crea una superficie más suavizada que los conecta, en lugar de dejar el hueco que existe entre ellos.
\\

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/etapa5_marchingcubesavatar_production.png}
\end{center}
\caption{ \label{F_MarchingCubesAvatar_Etapa5} Marching Cubes para construir una representación 3D del avatar a partir de una nube de puntos simplificada con aproximadamente 30 puntos con pesos asignados.}
\end{figure}


También comentar que la creación de esta malla no se realiza en el hilo del render, sino que es el hilo del controllador RunningSceneController el encargado de crear la representación gráfica del usuario para evitar interrupciones y pérdida de rendimiento en el render.

\item Prod3DEntity: Destacar que extiende de la clase Subject, ya que será observada por ContentCreationController a quién notificará de sus cambios. Se añaden también atributos como fuentes de audio, tiempo de vida, controladores de animación y se extiende la interfaz para añadir más métodos para obtener o asignar valores a sus atributos. 
\\

Se añaden especialmente métodos para obtener o asignar atributos en bloque que suelen ser accedidos juntos, de forma que pueden hacerse en una sóla llamada, lo que permite reducir el número de peticiones y bloqueos de la entidad.

\item MainProd: Finalmente se añade a MainProd una serie de atributos y métodos para dar cabida a las nuevas funcionalidades que se van a desarrollar. Los más destacados son los controladores de animación de la escena para permitir que los objetos3D que tengan asociados una animación sean capaces de reproducirlo.
\\

También se añade acceso para cambiar el color de fondo de la escena o el color e intensidad de la niebla, un contenedor de audios no relativos para reproducir sonidos de fondo que siempre se escuchen a igual intensidad, independientemente de la posición del usuario, y, finalmente, una serie de efectos de postproceso predefinidos como Invert, Bloom, Blur o Cartoon para añadir de forma simple un acabado diferente al render.
\end{itemize}


\subsubsection{Módulo de Inteligenia Artificial}
\subsubsection{Módulo de Aplicación}
Pool de contenidos: audios, imágenes, primitivas 3D, Elementos.
ContentCreationController



\section{Validación y Publicidad}
\subsection{Validación}
\subsection{Publicidad}

\chapter{Etapa i: Funcionalidades Añadidas}
\section{Análisis}
\subsection{Análisis de Requisitos de Usuario}
\subsection{Selección de Herramientas}
\section{Diseño}
\subsection{Diseño de la Aplicación}
\subsubsection{Breve descripción de los módulos}
\subsubsection{Diseño en UML - Resumen}
\subsubsection{Diseño en UML - Núcleo de la Interfaz}
\subsubsection{Diseño en UML - Módulo de Aplicación}
\subsubsection{Diseño en UML - Módulo de Persistencia}
\subsubsection{Diseño en UML - Módulo de GUI}
\subsubsection{Diseño en UML - Módulo de Producción}
\subsubsection{Diseño en UML - Módulo de Percepción}
\section{Implementación}
\subsubsection{Núcleo de la Interfaz}
\subsubsection{Módulo de Aplicación}
\subsubsection{Módulo de Persistencia}
\subsubsection{Módulo de GUI}
\subsubsection{Módulo de Producción}
\subsubsection{Módulo de Percepción}
\section{Validación y Publicidad}
\subsection{Validación}
\subsection{Publicidad}

\chapter{Etapa n: Revisión Final}
\section{Implementación}
\section{Validación y Publicidad}

\chapter{Etapa n+1: Publicidad}
\section{Análisis}
\section{Diseño}
\section{Implementación}
\section{Validación y Publicidad}

\chapter{Etapa n+2: Implantación Final}
\section{Análisis}
\section{Diseño}
\section{Implementación}
\section{Validación y Publicidad}

\chapter{Resultados y conclusiones}
 Desarrollo de los resultados y conclusiones del proyecto. Se debe intentar resaltar el interés del proyecto
 y la calidad del trabajo realizado. Ha llegado el momento de "vender" nuestro trabajo. Se deben incluir aspectos como:

\begin{itemize}
\item Calidad, dificultad y amplitud del trabajo desarrollado que justifique el tiempo de dedicación al proyecto.
\item Aspectos integradores de las disciplinas de la titulación de Ingeniero en Informática.
\item Impacto social. Utilidad del proyecto en el ámbito social
\item Facilidad de utilización de los resultados del proyecto por terceras personas.
\item Publicidad de los resultados del proyecto a través de páginas web, etc.. Cuando de los resultados del proyecto 
 se derive un prototipo o programa de utilización se debe poner a disposición del público en general una versión 
 de demostración de dicho prototipo.
\item Cualquier otro mérito.
\end{itemize}

\chapter{Trabajo Futuro}
 Continuidad del trabajo realizado a través de una implementación, o utilidad real del proyecto,
 o a través de otros proyectos fin de carrera.
 \\
 -Esquema multiusuario online.
 -Usar dispositivo XBox Kinect.
 -Reconocimiento postural.
 -Base de Datos Orientada a Objetos, Tiempo Real.
 


% EMPIEZAN LOS APENDICES DEL PROYECTO
\appendix

\chapter{Manuales de usuario}

 En el caso de que el desarrollo (y/o naturaleza del proyecto haya dado lugar a la creación de manuales
 de usuario, habrá que ponerlo aquí).

\chapter[Detalles Implementación]{Detalles técnicos sobre la implementación del proyecto}

En este apéndice se detallan los aspectos técnicos relacionados con la implementación del proyecto de forma más concreta. 
\\

\section{Incompatibilidades}\label{detallesImp:Incompatibilidades}
Uno de los aspectos a tener en cuenta es que al usar una cantidad considerable de librerías externas aparecen con frecuencia incompatibilidades que deben ser resueltas. Algunas de estas incompatibilidades pueden afectar sólo a los proyectos de los que dependa un módulo concreto o a la totalidad del proyecto.
\\
\subsection{winsock}
Uno de los problemas más comunes fue la incompatibilidad entre librerías relacionado con librerías del sistema operativo, como fue el caso con winsock. Varias de las librerías usan módulos relacionados incompatibles, por ejemplo Panda3D utiliza winsock, mientras OpenCV usa ws2def, ambas son incompatibles entre sí. Para resolver estos problemas se añaden al inicio de la cabecera de la aplicación las siguientes líneas:
\begin{lstlisting}[language=C++]
#ifdef _WINDOWS
#include <winsock2.h>
#endif
\end{lstlisting}
Estos problemas están ligados a las librerías del sistema operativo que utilizan las librerías externas y la inclusión sólo es necesaria para el sistema operativo Windows y se realizaría la inclusión si la macro está definida.
\\
\subsection{CLR}
Otra de las incompatibilidades más comunes fue la necesidad de desactivar el CLR, Common language runtime support. Debe ser puesto a  No Common language runtime support.

\subsection{NDEBUG}
Panda3D presenta graves problemas si se utiliza la macro \_NDEBUG en Release, por lo que debe quitarse de todo el proyecto.

\section{Third Parties}\label{detallesImp:ThirdParties}

Todas las librerías externas utilizadas se encuentran dentro de la rama $backslash$extern. 

\subsection{wxWidgets}\label{detallesImp:wxWidgets}
Las instrucciones para descargar y compilar wxWidgets pueden encontrarse en la web oficial \cite{wx09}. La distribución viene proyectos preparados para Visual Studio 2008. Sólo es necesario descargar la distribución, abrir la solución y compilarlo en Debug y Release, para obtener las librerías, sin necesidad de ningún ajuste en la compilación.

\subsection{Boost}\label{detallesImp:Boost}

La integración de Boost en el proyecto dio algunos problemas. Se obtuvo la distribución a partir del svn y se siguieron los pasos descritos en la documentación. Sin embargo, para poder añadir correctamente la librería a la solución son necesarias algunas opciones extras al compilarla. Estas opciones son la especificación de multi-threading y el uso de librerías estáticas. El comando final para compilar la librería se muestra a continuación:

\begin{listing}[style=consola, numbers=none]
>.\bjam link=static threading=multi threading=single variant=release variant=debug runtime-link=static
\end{listing}

Hay que tener en cuenta que es necesario desactivar el CLR para evitar incompatibilidades.

\subsection{SFML}\label{detallesImp:SFML}

Puede obtenerse desde svn o descargando algunos de los paquetes que existen, con proyectos preparados para distintos IDEs. La compilación es sencilla y directa.
\\

Sin embargo, es importante destacar que la documentación acerca del paquete de sonido es confusa y en ocasiones errónea. Sin embargo, es útil para conocer las herramientas que dispone. 
\\

Se describen los detalles a tener en cuenta, especialmente en relación a la localización del espacio 3D. En concreto:

\begin{itemize}
\item El eje X es creciente hacia la derecha.
\item El eje Y es creciente hacia arriba.
\item El eje Z es creciente y apunta hacia afuera de la pantalla, hacia el usuario.
\item El método SetTarget(), para orientar el oyente en una dirección, usa un vector normalizado relativo al oyente, no el punto del objeto al que escucha, como puede malinterpretarse por la documentación.
\item La librería NO permite la correcta espacialización del oyente en un entorno 3D tal y como se distribuye. Esto es debido a que enmascara la creación del oyente de OpenAL, ignorando una componente crucial: el vector de verticalidad. Establecen este vector siempre como vertical en el espacio absoluto. Como puede apreciarse en la figura \ref{F_SFML_listener_orientation}, esto impide la correcta orientación del oyente e impide que se pueda distinguir la orientación derecha-izquierda de la 'cabeza'. Sólo sería válido para posicionamiento 2D o 3D donde el vector de dirección del usuario se moviera en el plano horizontal y la verticalidad del oyente no cambiara.
\end{itemize}

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/SFML_listener_orientation.png}
\end{center}
\caption{ \label{F_SFML_listener_orientation} Necesidad del vector de verticalidad.}
\end{figure}

Esto puede verse claramente en la Definición del método de la clase Listener:

\begin{lstlisting}[language=C++]
/// Change the orientation of the listener (the point he must look at) (take 3 values)
void Listener::SetTarget(float X, float Y, float Z)
{   float Orientation[] = {X, Y, Z, 0.f, 1.f, 0.f};
    ALCheck(alListenerfv(AL_ORIENTATION, Orientation)); }
\end{lstlisting}
Sin embargo, el arreglo es sencillo. Basta con añadir a la clase Listener la siguiente función:
\begin{lstlisting}[language=C++]
void Listener::SetTarget(float X, float Y, float Z, float u, float v, float p)
{   float Orientation[] = {X, Y, Z, u, v, p};
    ALCheck(alListenerfv(AL_ORIENTATION, Orientation)); }
\end{lstlisting}

Tras recompilar se obtiene una librería que, ahora sí, es capaz de posicionar correctamente sonido y oyente en 3D.

\subsection{OpenCV, Open Source Computer Vision}\label{detallesImp:OpenCV}
(RETOMAR!!!)
2. Due to many technical problems the installation package does
   not include pre-compiled OpenCV libraries for Visual Studio users.

   Instead, it includes libraries built with MinGW 4.3.3 TDM-SJLJ.

   They are good enough to run the C/C++ and Python samples and tests,
   but for developing your OpenCV-based applications using
   Visual Studio, Borland IDE etc., or even a different version of MinGW,
   you need to build the libraries with your compiler using CMake,
   as explained here:
      http://opencv.willowgarage.com/wiki/InstallGuide.
   
   Here is the procedure at glance:
   --------------------------------
   1. Download CMake from http://www.cmake.org/cmake/resources/software.html
      and install it.

   2. Run CMake GUI tool and configure OpenCV there:
      2.1. select C:/OpenCV2.0 (or the installation directory you chose)
           as the source directory;
      2.2. choose some other directory name for the generated project files, e.g. C:/OpenCV2
      2.3. press Configure button, select your preferrable build environment
      2.4. adjust any options at your choice
      2.5. press Configure again, then press Generate.
   3a. In the case of Visual Studio or any other IDE, open the generated
      solution/workspace/project ..., e.g. C:/OpenCV2.0/vs2008/OpenCV.sln,
      build it in Release and Debug configurations.
   3b. In the case of command-line Makefiles, enter the destination directory
       and type "make" (or "nmake" etc.) 
      
   4. Add the output directories to the system path, e.g.:
      C:/OpenCV2.0/vs2008/bin/Debug;C:/OpenCV2.0/vs2008/bin/Release:\%PATH\%
      It is safe to add both directories, since the Debug
      OpenCV DLLs have the "d" suffix, which the Release DLLs do not have.
      
   5. Optionally, add C:/OpenCV2.0/include/opencv to the list of
      include directories in your IDE settings,
      and the output library directories
      (e.g. C:/OpenCV2.0/vs2008/lib/(Debug,Release))
      to the list of library paths.

   It is important to build both release and debug configurations, and link
   you code with the proper libraries in each configuration,
   otherwise various compile-time or run-time errors are possible.


ADEMAS VS 2008 EE no tiene soporte para OPENMP, así que es necesario desactivarlo en CMAKE, tambien PYTHON SUPPORT

Dependencias cv200d.lib cxcore200d.lib highgui200d.lib
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Panda3D}\label{detallesImp:Panda3D}
(REVISAR!!!!!)
www.panda3d.org/wiki/index.php/Tutorial:\_Compiling\_the\_Panda\_3D\_Source\_on\_Windows
www.panda3d.org/wiki/index.php/How\_to\_build\_a\_CXX\_Panda3D\_game\_using\_Microsoft\_Visual\_Studio\_2008
www.panda3d.org/wiki/index.php/Configuring\_Panda

scr/framework/config\_famework.cxx  (debug)
ConfigVariableBool show\_frame\_rate\_meter("show-frame-rate-meter", true);

panda3d usa winsock mientras que openCV usa ws2def....... sonincompatibles, debería usarse winsock2.h..

Se añade winsock2.h al comienzo de Application.h para resolver el conflicto (este problema sólo aparecería en las versiones de las librerías compliladas en Windows)

Excluir librería standar (norecuerdonombre)

Modificar fuentes de panda para exportar Geom::CDataCache y GeomVertexData::CDataCache, mediate macro EXPCL\_PANDA\_GOBJ : 
src/gobj/geom.h

  // The pipelined data with each CacheEntry.
  class EXPCL\_PANDA\_GOBJ CDataCache : public CycleData {
  public:
    INLINE CDataCache();
    INLINE CDataCache(const CDataCache \&copy);
    virtual ~CDataCache();
    ALLOC\_DELETED\_CHAIN(CDataCache);
    virtual CycleData *make\_copy() const;
    virtual TypeHandle get\_parent\_type() const {
      return Geom::get\_class\_type();
    }

src/gobj/geomvertexdata

  class EXPCL\_PANDA\_GOBJ CDataCache : public CycleData {
  public:
    INLINE CDataCache();
    INLINE CDataCache(const CDataCache \&copy);
    ALLOC\_DELETED\_CHAIN(CDataCache);
    virtual CycleData *make\_copy() const;
    virtual TypeHandle get\_parent\_type() const {
      return GeomVertexData::get\_class\_type();
    }

cambios aceptados en revisión y subidos al repositorio


\subsection{PostgreSQL}\label{detallesImp:PostgreSQL}
Inslatador de windows. Descargar instalador de Windows, ejecutar, seguir los pasos del tutorial. Instalar al mismo nivel que el proyecto. en carpeta BBDD. puerto por defecto 5432. Ejecutar pgAdmin para crear la base de datos. nombre voxDB, codificacion UTF8, tablespace pg\_default, colacióntipo de caracter spanish\_spain.1252. Nuevo rol de login, nombre vox, contraseña vox. Asignar usuario a base de datos.

\subsection{Debea}\label{detallesImp:Debea}
set DEVEL = directorio
nmake -f makefile.vc DEBUG=0 DEBUG=1 PGSQL=1
nmake -f makefile.vc DEBUG=0 DEBUG=1 PGSQL=1 install
copiar directorio resultatnte en extern\_debea
(REVISAR!!!!!)

\subsection{DynDNS}\label{detallesImp:DynDNS}
https://www.dyndns.com
dns para IPs dinámicas, para montar servidor propio de svn y acceso a bbdd.

\subsection{VisualSVN}\label{detallesImp:VisualSVN}
http://www.visualsvn.com/server/licensing/
Servidor svn.

\subsection{Tortoise SVN}\label{detallesImp:Torroise}
http://www.visualsvn.com/server/licensing/
Servidor svn.

Se monta servidor en https: 192.168.0.192 svn ProjectOX


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Interfaces core}\label{detallesImp:InterfacesCore}
Para preparar el módulo ha sido necesario configurar el proyecto igui de la siguiente forma:
\begin{itemize}
\item Additional Include Directories:
\begin{lstlisting}[language=C++]
.
..\..\src
..\..\extern\include
\end{lstlisting}
\item Additional Library Directories:
\begin{lstlisting}[language=C++]
\"\$(OutDir)\"
\end{lstlisting}
\item Preprocessor Definitions (Debug): WIN32; \_DEBUG; \_WINDOWS; 
\item Preprocessor Definitions (Release): WIN32; \_WINDOWS;
\end{itemize}

Además por asuntos de compatibilidad es necesario asegurar las siguientes opciones:
\begin{itemize}
\item No usar la macro \_NDEBUG en release.
\item Configuration Type: Static Library (.lib). (actualemnte dll cambiar)
\item Use of ATL: Not Using ATL.
\item Common Language Runtime Support: No Common Language Runtime Support.
\item Runtime Library: Multi-threaded [Debug] DLL ($\backslash$MTD $\backslash$MTDd).
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{IApplicationConfiguration}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{lstlisting}[language=C++]
class CameraData
{ public:
   CameraData() : x(0),y(0),z(0),flip_h(false),flip_v(false){}
   ~CameraData(){}
   double x, y, z;
   bool flip_h, flip_v; };
   
class DisplayData
{ public:
   DisplayData() : x(0),y(0),z(0),flip_h(false),flip_v(false), resolution_x(800),resolution_y(600){}
   ~DisplayData() {}
   double x, y, z;
   bool flip_h, flip_v;
   unsigned int resolution_x, resolution_y; };
   
class IApplicationConfiguration
{ public:
   virtual ~IApplicationConfiguration(){}
   virtual core::CameraData  GetCameraData(const int &id)=0;
   virtual core::DisplayData GetDisplayData(const int &id)=0;	
   virtual unsigned int GetNumCams()     = 0;
   virtual unsigned int GetNumDisplays() = 0;   
   virtual std::string GetDBHost() = 0;
   virtual std::string GetDBPort() = 0;
   virtual std::string GetDBName() = 0;
   virtual std::string GetDBUser() = 0;
   virtual std::string GetLanguage() = 0;   
   virtual std::string GetDBPassword()      = 0;
   virtual std::string GetRootDirectory()   = 0;
   virtual std::string GetDataDirectory()   = 0;
   virtual std::string GetModelDirectory()  = 0;
   virtual std::string GetSoundDirectory()  = 0;
   virtual std::string GetImageDirectory()	= 0;
   virtual std::string GetUIResourceDirectory() = 0;			
   
   virtual void SetRootDirectory(const std::string &value) = 0;
   virtual void SetDataDirectory(const std::string &value) = 0;
   virtual void SetModelDirectory(const std::string &value) = 0;
   virtual void SetSoundDirectory(const std::string &value) = 0;
   virtual void SetImageDirectory(const std::string &value) = 0;
   virtual void SetUIResourceDirectory(const std::string &value) = 0;
   virtual void SetDBHost(const std::string &value) = 0;
   virtual void SetDBPort(const std::string &value) = 0;
   virtual void SetDBName(const std::string &value) = 0;
   virtual void SetDBUser(const std::string &value) = 0;
   virtual void SetDBPassword(const std::string &value) = 0;
   virtual void SetLanguage(const std::string &value) = 0;		
   virtual void SetNumCams(const unsigned int &value) = 0;
   virtual void SetNumDisplays(const unsigned int &value) = 0;
   virtual void SetCameraData(const int &id, const CameraData &value)=0;
   virtual void SetDisplayData(const int &id, const DisplayData &value)=0;
};   
\end{lstlisting}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Módulo de GUI}\label{detallesImp:ModuloGUI}
Para preparar el módulo ha sido necesario configurar el proyecto igui de la siguiente forma:
\begin{itemize}
\item Additional Include Directories:
\begin{lstlisting}[language=C++]
.
..\..\src
..\..\extern\include
..\..\extern\include\wxwidgets
..\..\extern\include\wxwidgets\msvc
..\..\extern\boost
\end{lstlisting}
\item Additional Library Directories:
\begin{lstlisting}[language=C++]
..\..\bin
..\..\extern\lib
..\..\extern\boost\lib
\"\$(OutDir)\"
\end{lstlisting}
\item Preprocessor Definitions (Debug): WIN32; \_DEBUG; \_WINDOWS; \_USRDLL; \_MSVC; \_IGUIEXPORT\_; \_\_WXMSW\_\_; \_\_WXDEBUG\_\_; \_UNICODE; NOPCH
\item Preprocessor Definitions (Release): WIN32; \_WINDOWS; \_USRDLL; \_MSVC;\_IGUIEXPORT\_; \_\_WXMSW\_\_; \_UNICODE; NOPCH
\item Additional Dependencies (Debug): 
wxmsw29ud\_core.lib wxbase29ud.lib wxtiffd.lib wxjpegd.lib wxpngd.lib wxzlibd.lib wxregexud.lib wxexpatd.lib kernel32.lib user32.lib gdi32.lib comdlg32.lib
winspool.lib winmm.lib shell32.lib comctl32.lib ole32.lib oleaut32.lib uuid.lib rpcrt4.lib advapi32.lib wsock32.lib wininet.lib 
\item Additional Dependencies (Release): wxmsw29u\_core.lib wxbase29u.lib wxtiff.lib wxjpeg.lib wxpng.lib wxzlib.lib wxregexu.lib wxexpat.lib kernel32.lib user32.lib gdi32.lib comdlg32.lib winspool.lib winmm.lib shell32.lib comctl32.lib ole32.lib oleaut32.lib rpcrt4.lib advapi32.lib wsock32.lib wininet.lib
\end{itemize}

Además por asuntos de compatibilidad es necesario asegurar las siguientes opciones:
\begin{itemize}
\item No usar la macro \_NDEBUG en release.
\item Configuration Type: Static Library (.lib).
\item Use of ATL: Not Using ATL.
\item Common Language Runtime Support: No Common Language Runtime Support.
\item Runtime Library: Multi-threaded [Debug] DLL ($\backslash$MTD $\backslash$MTDd).
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{MainGui}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{lstlisting}[language=C++]
#define IMPLEMENT_MIIAPP(name) IMPLEMENT_APP(name)
namespace core {namespace igui{
class MainFrame;
class MainGui : public core::IGui
{public:
 	virtual ~MainGui();
 	virtual void Delete();
 	virtual void Update();

 	void RegisterWindow(IGuiWindow *window);
 	void AddWindowToFullscreenList(IGuiWindow *window);
 	void ShowAll(bool value);
 	void SetAllWindowsFullScreen(bool value);
 	static MainGui* GetInstance(const std::wstring &title = L"");
 private:		
 	MainGui(const std::wstring &title = L"");
 	static MainGui* instance; 
 	static MainFrame *main_frame;
 	static std::map<IGuiWindow*, int> registered_windows;
 	static std::map<IGuiWindow*, int> fullscreenable_windows;
}; 
}}

MainGui* MainGui::GetInstance(const std::string &title)
{	boost::mutex::scoped_lock lock(m_mutex);
	if (instance == NULL) 
		instance = new MainGui(title);
	return instance; }
\end{lstlisting}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{MainFrame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{lstlisting}[language=C++]
namespace core { namespace igui	{
class MainGui;
class MainFrame : public wxFrame
{ public:
	MainFrame(wxWindow *parent, wxWindowID id, const wxString &title, const wxPoint &pos=wxDefaultPosition, const wxSize &size=wxDefaultSize, long style=wxDEFAULT_FRAME_STYLE, const wxString &name=wxFrameNameStr);
	virtual ~MainFrame();
	void Delete();
 private:
	wxMenu *file_menu, *view_menu, *tools_menu, *help_menu;
	wxMenuItem *item_file_close, *item_view_fullscreen, *item_view_start, *item_tools_configure, *item_help_about;
	wxWindow *dummy_panel;
	GUIHelp  *help_panel;
	GUIStart *start_panel;
	DECLARE_EVENT_TABLE()
	void InitShortCuts();
	void OnClose(wxCommandEvent& WXUNUSED(event));
	void OnViewStart(wxCommandEvent& WXUNUSED(event));
	void OnHelpAbout(wxCommandEvent& WXUNUSED(event));
	void DismissPanels();
};	}}
#endif
\end{lstlisting}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{GUIStart}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{lstlisting}[language=C++]
namespace core { namespace igui { 
class GUIStart : public wxPanel 
{ public:
	GUIStart(wxWindow *parent, wxWindowID id,  const wxPoint &pos=wxDefaultPosition, const wxSize &size=wxDefaultSize, long style=wxTAB_TRAVERSAL, const wxString &name = "panel");
	virtual ~GUIStart();
	void Delete();
	void OnPaint(wxPaintEvent &evt);
	void paintNow();	        
	void render(wxDC& dc);
 private:
	wxBitmap background_image;
	DECLARE_EVENT_TABLE()		
	wxButton *login_button, *start_button, *configure_button;
};}}
#endif
\end{lstlisting}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{GUIHelp}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{lstlisting}[language=C++]
BEGIN_EVENT_TABLE(GUIHelp, wxPanel)
	EVT_PAINT (GUIHelp::OnPaint)
END_EVENT_TABLE()

GUIHelp::GUIHelp(wxWindow *parent, wxWindowID id, const wxPoint &pos, const wxSize &size, long style, const wxString &name) 
: wxPanel(parent, id, pos, size, style, name)
{ background_image = wxBitmap("c:/etc/help.png", wxBITMAP_TYPE_ANY); }

GUIHelp::~GUIHelp()
{}

void GUIHelp::Delete()
{ wxPanel::Destroy(); }

void GUIHelp::OnPaint(wxPaintEvent & evt)
{ wxPaintDC dc(this);
  render(dc); }

void GUIHelp::render(wxDC& dc)
{ dc.DrawBitmap(background_image, 0, -20, false );
  dc.DrawRotatedText("ProjectVOX", 250, 30, 0);
  dc.DrawRotatedText("Code Name ::TheElectricGOAT::", 250, 50, 0);
  dc.DrawRotatedText("Version 0.1", 250, 70, 0); }
\end{lstlisting}
 
\section{Módulo de Percepción}\label{detallesImp:ModuloPercepcion}

Para implementar el módulo IPercept se ha configurado el proyecto de la siguiente forma:

\begin{itemize}
\item Additional Include Directories:
\begin{lstlisting}[language=C++]
.
..\..\src
..\..\extern\include
..\..\extern\include\opencv
..\..\extern\boost
..\..\extern\include\wxwidgets\msvc
..\..\extern\include\wxwidgets
..\..\extern\sfml\include
\end{lstlisting}
\item Additional Library Directories:
\begin{lstlisting}[language=C++]
..\..\bin
..\..\extern\lib
..\..\extern\lib\opencv
..\..\extern\boost\lib
..\..\extern\sfml\lib\vc2008
\"\$(OutDir)\"
\end{lstlisting}
\item Preprocessor Definitions (Debug): WIN32;\_DEBUG
\item Preprocessor Definitions (Release): WIN32;\_WINDOWS;\_MSVC;
\item Additional Dependencies (Debug): cv200d.lib cxcore200d.lib highgui200d.lib sfml-audio-s-d.lib sfml-system-s-d.lib
\item Additional Dependencies (Release): cv200.lib cxcore200.lib highgui200.lib sfml-audio-s.lib sfml-system-s.lib
\end{itemize}

Además por asuntos de compatibilidad es necesario asegurar las siguientes opciones:
\begin{itemize}
\item No usar la macro \_NDEBUG en release.
\item Configuration Type: Static Library (.lib).
\item Use of ATL: Not Using ATL.
\item Common Language Runtime Support: No Common Language Runtime Support.
\item Runtime Library: Multi-threaded [Debug] DLL ($\backslash$MTD $\backslash$MTDd).
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{MainPercept}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{lstlisting}[language=C++]
#include <core/IPercept/IPercept.h>
#include <ipercept/PerceptAudio.h>
#include <ipercept/PerceptVideo.h>
#include <string>
#include <vector>
namespace core { namespace ipercept	{
 class MainPercept : public core::IPercept
 { public:
    MainPercept();
	virtual ~MainPercept();
	virtual void Delete();
	virtual void Init();
   private:
	static int num_cam;
	static PerceptAudio* perceptAudio_module;
	static PerceptVideo* perceptVideo_module;
 };} }

MainPercept::MainPercept()
{	perceptAudio_module = new PerceptAudio();
	perceptVideo_module = new PerceptVideo(); }

MainPercept::~MainPercept()
{	delete perceptAudio_module;
	delete perceptVideo_module; }

void MainPercept::Delete()
{	perceptAudio_module->Delete();
	perceptVideo_module->Delete(); }

void MainPercept::Init()
{	perceptAudio_module->Init();
	perceptVideo_module->Init(); }
\end{lstlisting}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{PerceptAudio}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{lstlisting}[language=C++]
#include <core/IPercept/IPerceptAudio.h>
#include <string>
#include <vector>
#include <boost/thread.hpp>
#include <SFML/Audio.hpp>
#include <SFML/Audio/SoundRecorder.hpp>
namespace core { namespace ipercept {
class PerceptAudio : public core::IPerceptAudio 
{ public:
   PerceptAudio();
   virtual ~PerceptAudio();
   virtual void Delete();
   virtual void Init();
  private:
   static void DoInit();
   static void DoMainLoop();
   static void Iterate();
   static void Capture();
   
   static boost::shared_ptr<boost::thread> m_thread;
   static boost::try_mutex m_mutex;
   static bool initialized, stop_requested;
   static sf::SoundBufferRecorder Recorder;
   static sf::SoundBuffer recordingBuffer;
};}}

void PerceptAudio::Delete()
{	stop_requested = true;
	assert(m_thread);
	m_thread->join(); }

void PerceptAudio::Init()
{	PerceptAudio::DoInit(); }

void PerceptAudio::DoInit()
{	if (!initialized)
	{	assert(!m_thread);
		m_thread = boost::shared_ptr<boost::thread>(new boost::thread(boost::function0<void>(&PerceptAudio::DoMainLoop))); } }

void PerceptAudio::DoMainLoop()
{	initialized = true;
	Recorder.Start();
	while(!stop_requested)
	{	Iterate();
		m_thread->sleep(boost::get_system_time()+boost::posix_time::milliseconds(1000)); }}

void PerceptAudio::Iterate()
{	boost::try_mutex::scoped_try_lock lock(m_mutex);
	if (lock) { Capture(); } 
}

void PerceptAudio::Capture()
{	Recorder.Stop();
	recordingBuffer = Recorder.GetBuffer();
	Recorder.Start(); }
\end{lstlisting}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{PerceptVideo}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{lstlisting}[language=C++]
#include <core/IPercept/IPerceptVideo.h>
#include <ipercept/CamWindow.h>
#include <string>
#include <vector>
#include <cv.h>
#include <cxcore.h>
#include <highgui.h>
#include <boost/thread.hpp>
namespace core { namespace ipercept {
class PerceptVideo : public core::IPerceptVideo
{ public:
   PerceptVideo();
   virtual ~PerceptVideo();
   virtual void Delete();
   virtual void Init();
   
  private:
   static void DoInit();
   static void DoMainLoop();
   static void Iterate();
   static void Capture();

   static int num_cam;
   static std::map< int, CvCapture* > capture_cam_array;
   static std::map< std::string, CamWindow* > camWindow_array;
   static IplImage *capture_img;

   static boost::shared_ptr<boost::thread> m_thread;
   static boost::try_mutex m_mutex;
   static bool initialized, stop_requested;
};}}

PerceptVideo::PerceptVideo()
{ if (!initialized)
  { num_cam = NUM_CAM;
    for (int i=0; i<num_cam; i++)
    { std::stringstream window_name;
      window_name << "Cam" << i << ":";
      capture_cam_array[i] = cvCaptureFromCAM( i );
      camWindow_array[window_name.str()] = new CamWindow(window_name.str());
} } }

PerceptVideo::~PerceptVideo()
{ boost::try_mutex::scoped_try_lock lock(m_mutex);
  if (lock)
  { for (std::map< int, CvCapture * >::iterator iter = capture_cam_array.begin(); iter!=capture_cam_array.end(); iter++)
    { std::stringstream window_name;
      window_name << "Cam" << iter->first << ":";
      cvReleaseCapture(&(iter->second)); }
    capture_cam_array.erase(capture_cam_array.begin(),capture_cam_array.begin());

    for (std::map< std::string, CamWindow* >::iterator iter = camWindow_array.begin(); iter!=camWindow_array.end(); iter++)
     delete iter->second;
    camWindow_array.erase(camWindow_array.begin(),camWindow_array.begin());
}}

void PerceptVideo::Delete()
{ stop_requested = true;
  assert(m_thread);
  m_thread->join(); }

void PerceptVideo::Init()
{ PerceptVideo::DoInit(); }

void PerceptVideo::DoInit()
{ if (!initialized)
  { assert(!m_thread);
    m_thread = boost::shared_ptr<boost::thread>(new boost::thread(boost::function0<void>(&PerceptVideo::DoMainLoop))); 
} }

void PerceptVideo::DoMainLoop()
{ initialized = true;
  while(!stop_requested)
  { Iterate();
    m_thread->sleep(boost::get_system_time()+boost::posix_time::milliseconds(10));
} }

void PerceptVideo::Iterate()
{  boost::try_mutex::scoped_try_lock lock(m_mutex);
   if (lock) { Capture(); } }

void PerceptVideo::Capture()
{ for (std::map<int, CvCapture *>::iterator iter = capture_cam_array.begin(); iter!=capture_cam_array.end(); iter++)
  { std::stringstream window_name;
    window_name << "Cam" << iter->first << ":";
    capture_img = cvQueryFrame(iter->second);
    std::map< std::string, CamWindow* >::iterator cam_iter = camWindow_array.find(window_name.str());
    if (cam_iter != camWindow_array.end())
      cam_iter->second->ShowImage(capture_img);
} }
\end{lstlisting}


\section{Módulo de Producción}\label{detallesImp:ModuloProduccion}

Para implementar el módulo IProd se ha configurado el proyecto de la siguiente forma:

\begin{itemize}
\item Additional Include Directories:
\begin{lstlisting}[language=C++]
.
..\..\src
..\..\extern\include
..\..\extern\panda3d\built\include
..\..\extern\panda3d\built\python\include
..\..\extern\boost
..\..\extern\include\wxwidgets\msvc
..\..\extern\include\wxwidgets
..\..\extern\sfml\include
\end{lstlisting}
\item Additional Library Directories (Debug):
\begin{lstlisting}[language=C++]
..\..\bin
..\..\extern\lib
..\..\extern\panda3d\debug\lib
..\..\extern\panda3d\debug\python
..\..\extern\panda3d\debug\python\Lib
..\..\extern\panda3d\debug\python\libs
..\..\extern\boost\lib
..\..\extern\sfml\lib\vc2008
\$(OutDir)
\end{lstlisting}
\item Additional Library Directories (Release):
\begin{lstlisting}[language=C++]
..\..\bin
..\..\extern\lib
..\..\extern\panda3d\built\lib
..\..\extern\panda3d\built\python
..\..\extern\panda3d\built\python\Lib
..\..\extern\panda3d\built\python\libs
..\..\extern\boost\lib
..\..\extern\sfml\lib\vc2008
\$(OutDir)
\end{lstlisting}
\item Preprocessor Definitions (Debug): WIN32;\_DEBUG
\item Preprocessor Definitions (Release): WIN32;
\item Additional Dependencies (Debug): libpandaexpress.lib libp3framework.lib libpanda.lib libpandafx.lib libp3dtool.lib libp3dtoolconfig.lib libp3pystub.lib libp3direct.lib sfml-audio-s-d.lib sfml-system-s-d.lib
\item Additional Dependencies (Release): libpandaexpress.lib libp3framework.lib libpanda.lib libpandafx.lib libp3dtool.lib libp3dtoolconfig.lib libp3pystub.lib libp3direct.lib sfml-audio-s.lib sfml-system-s.lib
\end{itemize}

Además por asuntos de compatibilidad es necesario asegurar las siguientes opciones:
\begin{itemize}
\item No usar la macro \_NDEBUG en release.
\item Configuration Type: Static Library (.lib).
\item Use of ATL: Not Using ATL.
\item Common Language Runtime Support: No Common Language Runtime Support.
\item Runtime Library: Multi-threaded [Debug] DLL ($\backslash$MTD $\backslash$MTDd).
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Prod3dWindow}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{lstlisting}[language=C++]
#include <igui/maingui.h>
#include <string>
#include <vector>
#include <pandaFramework.h>
namespace core { namespace iprod {
class Prod3DWindow : public core::IGuiWindow
{ public:
   Prod3DWindow(PandaFramework *framework, const WindowProperties &props, bool registrable = false, bool fullscreenable = false);
   virtual ~Prod3DWindow();
   WindowFramework *GetWindowFrameWork() {return m_windowFramework;}
   void Show(const bool &value = true);
   void FullScreen( const bool &value = true );
   bool IsShown() {return isShown;}
 private:
   bool isShown;
   WindowFramework *m_windowFramework;
}; }}
\end{lstlisting}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{MainProd}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{lstlisting}[language=C++]
#include <core/IProd/IProd.h>
#include <iprod/Prod3DWindow.h>
#include <string>
#include <pandaFramework.h>
#include <pandaSystem.h>
#include <modelPool.h>
#include <filename.h>
#include <genericAsyncTask.h>
#include <asyncTaskManager.h>
#include "cIntervalManager.h"
#include "cLerpNodePathInterval.h"
#include "cMetaInterval.h"
#include <boost/thread.hpp>
#include <SFML/Audio.hpp>

namespace core { namespace iprod {
class MainProd : public core::IProd
{ public:
   MainProd(int argc, char *argv[]);
   virtual ~MainProd();
   virtual void Delete();
   virtual void Init();
   virtual void DoStuff();
   virtual void OpenWindow();
  private:
   static void DoInit();
   static void DoDoStuff();
   static void PlaySoundCapture();
   static void DoMainLoop();
   static void Iterate();
   static void CreateDefaultWindows(int numWindows);
   static void LoadDefaultScene();

   static AsyncTask::DoneStatus SpinCameraTask(GenericAsyncTask* task, void* data);
   static boost::shared_ptr<boost::thread> m_thread;
   static boost::try_mutex m_mutex;
   static PandaFramework framework;
   static Thread *graphic_thread;
   static PT(AsyncTaskManager) taskMgr;
   static PT(ClockObject) globalClock;
   static bool initialized, stop_requested;
   static std::map<int, Prod3DWindow*>		prod3Dwindow_array;
   static std::map<int, WindowFramework*>	pandawindows_array;
   static std::map<int, NodePath>			windowcamera_array;
   static int last_window_id, m_argc;
   static char **m_argv;
   static NodePath cam_viewpoint;
   static NodePath origin, up;

   static sf::SoundBuffer sound_Buffer;
   static double listener_position[];
   static double listener_target[];
   static double sound_pos[];
   static sf::Sound Sound;
}; } }

MainProd::MainProd(int argc, char *argv[])
{ m_argc = argc;
  m_argv = argv; }

MainProd::~MainProd()
{ m_thread->join();
  for (std::map<int, Prod3DWindow*>::iterator iter=prod3Dwindow_array.begin(); iter != prod3Dwindow_array.end(); iter++)
   delete iter->second;
  prod3Dwindow_array.erase(prod3Dwindow_array.begin(), prod3Dwindow_array.end());
}

void MainProd::Delete()
{ stop_requested = true;
  assert(m_thread);
  m_thread->join(); }

void MainProd::DoMainLoop()
{ framework.open_framework( m_argc, m_argv);
  taskMgr = AsyncTaskManager::get_global_ptr();
  globalClock = ClockObject::get_global_clock();
  graphic_thread = Thread::get_current_thread();	

  CreateDefaultWindows(DEFAULT_NUM_WINDOWS);

  LoadDefaultScene();

  sf::SoundBuffer Buffer;
  if (!Buffer.LoadFromFile("c://etc//motor.wav"))
   int error_loadfromfile = 1;		
  unsigned int chan = Buffer.GetChannelsCount();
  sf::Listener::SetGlobalVolume(100.f);
  Sound.SetBuffer(Buffer);
  Sound.SetLoop(true);
  Sound.SetPosition(sound_pos[0], sound_pos[1], sound_pos[2]);
  Sound.SetMinDistance(10.f);
  Sound.SetAttenuation(0.75f);

  initialized = true;
  
  Sound.Play();	

  while(!stop_requested) 
  { Iterate();
    m_thread->sleep(boost::get_system_time()+boost::posix_time::milliseconds(10)); }

  framework.close_all_windows();
  framework.close_framework();
}

void MainProd::Iterate()
{ boost::try_mutex::scoped_try_lock lock(m_mutex);
  if (lock)
  { framework.do_frame(graphic_thread);
    CIntervalManager::get_global_ptr()->step(); }
}

void MainProd::Init()
{ MainProd::DoInit(); }

void MainProd::DoStuff()
{ MainProd::DoDoStuff(); }

void MainProd::DoInit()
{ if (!initialized)
	{ assert(!m_thread);
      m_thread = boost::shared_ptr<boost::thread>(new boost::thread(boost::function0<void>(&MainProd::DoMainLoop))); }
}

void MainProd::DoDoStuff()
{ boost::try_mutex::scoped_try_lock lock(m_mutex);
  if (lock && initialized)
  { double time = globalClock->get_real_time();
    double angledegrees = time * 2.0;
    double angleradians = angledegrees * (3.14 / 180.0);
    windowcamera_array[1].set_pos(20*sin(angleradians),-20.0*cos(angleradians),3);
    windowcamera_array[1].set_hpr(angledegrees, 0, 0);

    LPoint3f cam_pos = windowcamera_array[2].get_pos();
    LVector3f abs_vec_at, abs_vec_up;
    abs_vec_at = cam_viewpoint.get_pos(pandawindows_array[2]->get_render());
    abs_vec_up = up.get_pos(pandawindows_array[2]->get_render());
    LPoint3f new_pos(cam_pos.get_x()  ,    cam_pos.get_z(),    -cam_pos.get_y());
    LPoint3f new_at(abs_vec_at.get_x(), abs_vec_at.get_z(), -abs_vec_at.get_y());
    LPoint3f new_up(abs_vec_up.get_x(), abs_vec_up.get_z(), -abs_vec_up.get_y());
    new_at = new_at - new_pos;
    new_up = new_up - new_pos;
    new_at.normalize();
    new_up.normalize();
    sf::Listener::SetPosition(new_pos.get_x(), new_pos.get_y(), new_pos.get_z());
    sf::Listener::SetTarget(new_at.get_x(), new_at.get_y(), new_at.get_z() , new_up.get_x(), new_up.get_y(), new_up.get_z());
   }
   else 
   { //más suerte la próxima }
}

void MainProd::CreateDefaultWindows(int num_windows)
{ WindowProperties win_props = WindowProperties(); 
  win_props.set_size(800, 600); 
  for (int i=last_window_id+1; i <= last_window_id + num_windows; i++)
  { prod3Dwindow_array[i] = new Prod3DWindow(&framework, win_props, true, true);
    pandawindows_array[i] = prod3Dwindow_array[i]->GetWindowFrameWork();
    pandawindows_array[i]->set_background_type(WindowFramework::BackgroundType::BT_white);
    pandawindows_array[i]->set_lighting(true);
    pandawindows_array[i]->set_perpixel(true);
    windowcamera_array[i] = pandawindows_array[i]->get_camera_group(); 
  }
  origin = pandawindows_array[1]->load_model(framework.get_models(), "panda-model");
  origin.set_pos(0,0,0);
  origin.set_scale(0.001);
  origin.reparent_to(pandawindows_array[2]->get_render());
  up = pandawindows_array[1]->load_model(framework.get_models(), "panda-model");
  up.set_pos(0,0,10);
  up.set_scale(0.002);
  up.reparent_to(pandawindows_array[2]->get_camera_group());
  cam_viewpoint = pandawindows_array[1]->load_model(framework.get_models(), "panda-model");
  cam_viewpoint.set_pos(0,10,0);
  cam_viewpoint.set_scale(0.002);
  cam_viewpoint.reparent_to(pandawindows_array[2]->get_camera_group());
  last_window_id += num_windows;
}

void MainProd::LoadDefaultScene()
{ if (pandawindows_array.begin() != pandawindows_array.end())
  { NodePath environment = pandawindows_array[1]->load_model(framework.get_models(),"environment");
    environment.set_scale(0.25,0.25,0.25);
    environment.set_pos(-8,42,0);
    environment.reparent_to(pandawindows_array[1]->get_render());
    NodePath pandaActor = pandawindows_array[1]->load_model(framework.get_models(), "panda-model");
    pandaActor.set_scale(0.005);
    LPoint3f pandapos = pandaActor.get_pos();
    pandaActor.reparent_to(pandawindows_array[1]->get_render());
    NodePath pandaActor2 = pandawindows_array[1]->load_model(framework.get_models(), "panda-model");
    pandaActor2.set_scale(0.004);
    pandaActor2.set_pos(10.0,0.0,0.0);
    LPoint3f pandapos2 = pandaActor2.get_pos();
    pandaActor2.reparent_to(pandawindows_array[1]->get_render());
	...

    pandawindows_array[1]->load_model(pandaActor, "panda-walk4");
    pandawindows_array[1]->loop_animations(0);

    std::map<int, WindowFramework*>::iterator iter = pandawindows_array.begin();
    iter++;
    while(iter != pandawindows_array.end())
    { environment.instance_to(iter->second->get_render());
      pandaActor.instance_to(iter->second->get_render());
      pandaActor2.instance_to(iter->second->get_render());
      pandaActor3.instance_to(iter->second->get_render());
      pandaActor4.instance_to(iter->second->get_render());
      iter->second->setup_trackball();
      iter++;
} } }
\end{lstlisting}

\section{Aplicación Principal}\label{detallesImp:Application}

El proyecto que conforma la aplicación princial se ha configurado de la siguiente forma:

\begin{itemize}
\item Additional Include Directories:
\begin{lstlisting}[language=C++]
.
..\..\src
..\..\extern\include
..\..\extern\panda3d\built\include
..\..\extern\panda3d\built\python\include
..\..\extern\include\opencv
..\..\extern\include\wxwidgets
..\..\extern\include\wxwidgets\msvc
..\..\extern\boost
..\..\extern\sfml\include
\end{lstlisting}
\item Additional Library Directories (Debug):
\begin{lstlisting}[language=C++]
..\..\bin
..\..\extern\lib
..\..\extern\bin
..\..\extern\panda3d\debug\python\libs
..\..\extern\boost\lib
\$(OutDir)
\end{lstlisting}
\item Additional Library Directories (Release):
\begin{lstlisting}[language=C++]
..\..\bin
..\..\extern\lib
..\..\extern\bin
..\..\extern\panda3d\built\python\libs
..\..\extern\boost\lib
\$(OutDir)
\end{lstlisting}
\item Preprocessor Definitions (Debug): WIN32;\_DEBUG;\_WINDOWS;\_MSVC;
\item Preprocessor Definitions (Release): WIN32;\_WINDOWS;\_MSVC;
\item Additional Dependencies (Debug): icogd.lib iguid.lib iperceptd.lib ipersistenced.lib iprodd.lib
\item Additional Dependencies (Release): icog.lib igui.lib ipercept.lib ipersistence.lib iprod.lib
\item Environment (Debug): 
\begin{listing}[style=consola, numbers=none]
PATH=%PATH%;..\..\extern\bin;..\..\extern\bin\opencv;..\..\extern\panda3d\debug\bin;..\..\extern\panda3d\debug\python;..\..\extern\panda3d\debug\python\DLLs;..\..\extern\sfml\extlibs\bin;
\end{listing}
\item Environment (Release): 
\begin{listing}[style=consola, numbers=none]
PATH=%PATH%;..\..\extern\bin;..\..\extern\bin\opencv;..\..\extern\panda3d\built\bin;..\..\extern\panda3d\built\python;..\..\extern\panda3d\built\python\DLLs;..\..\extern\sfml\extlibs\bin;
\end{listing}
\end{itemize}

Además por asuntos de compatibilidad es necesario asegurar las siguientes opciones:
\begin{itemize}
\item No usar la macro \_NDEBUG en release.
\item Configuration Type: Application (.exe).
\item Use of ATL: Not Using ATL.
\item Common Language Runtime Support: No Common Language Runtime Support.
\item Runtime Library: Multi-threaded [Debug] DLL ($\backslash$MD $\backslash$MDd).
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Application}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{lstlisting}[language=C++]
#ifdef _WINDOWS
#include <winsock2.h>
#endif
#include <core/IApplication.h>
#include <core/IGUI/IGui.h>
#include <core/ICog/ICog.h>
#include <core/IProd/IProd.h>
#include <core/IPercept/IPercept.h>
#include <core/IPersistence/IPersistence.h>
#include <igui/maingui.h>
#include <icog/maincog.h>
#include <iprod/mainprod.h>
#include <ipercept/mainpercept.h>
#include <ipersistence/mainpersistence.h>
class Application : public wxApp, public core::IApplication
{ public:
   Application(void);
   ~Application(void);
   bool OnInit();
   void ExitApplication();
  private:
   DECLARE_EVENT_TABLE();
   wxTimer app_timer;
   core::IGui		*app_maingui;
   core::IPercept	*app_mainpercept;
   core::IProd		*app_mainprod;
   void	OnIdle(wxIdleEvent &event);
   int		OnRun();
   void	DoMainLoopStuff(wxTimerEvent& event);   
};

#define MAINLOOP_EVT 12345

BEGIN_EVENT_TABLE(Application, wxApp)
	EVT_IDLE	(Application::OnIdle)
	EVT_TIMER	(MAINLOOP_EVT, Application::DoMainLoopStuff)
END_EVENT_TABLE()

Application::Application(void) : app_maingui(NULL), app_mainpercept(NULL), app_mainprod(NULL)
{ app_timer.SetOwner(this, MAINLOOP_EVT); }

Application::~Application(void)
{	if (app_mainpercept!=NULL)
	{	app_mainpercept->Delete();
		delete app_mainpercept;	}

	if (app_mainprod!=NULL)
	{	app_mainprod->Delete();
		delete app_mainprod;	}

	if (app_maingui!=NULL)
	{	app_maingui->Delete();
		delete app_maingui;	} }

bool Application::OnInit()
{ app_maingui     = core::igui::MainGui::GetInstance("VOX");
  app_mainpercept = (core::IPercept *) new core::ipercept::MainPercept(); 
  app_mainprod    = (core::IProd *) new core::iprod::MainProd(argc, argv); 
  app_mainpercept->Init();
  app_mainprod->Init();
  return true; }

void Application::OnIdle(wxIdleEvent &event)
{ //To Do less important/frequent stuff }

int Application::OnRun()
{ app_timer.Start(10);
  return wxApp::OnRun(); }

void Application::ExitApplication()
{	wxApp::Exit(); }

void Application::DoMainLoopStuff(wxTimerEvent& event)
{ //To Do important/frequent stuff
  app_mainprod->DoStuff(); }
\end{lstlisting}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{main}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{lstlisting}[language=C++]
#define _WINSOCKAPI_
#include "Application.h"
IMPLEMENT_MIIAPP(Application)
\end{lstlisting}

\chapter[Redacción de Proyectos]{Comentarios Generales sobre la redacción de los proyectos}


Por donde se empieza? Habitualmente se empieza a redactar en orden inverso al que se lee el documento, primero 
se puede empezar a rellenar la bibliografía utilizada que se irá completando según avance el proyecto. A continuación, 
a partir de un esqueleto inicial del proyecto, que puede estar redactado, manuscrito, o simplemente en la cabeza del estudiante, 
se empieza a redactar las partes más concretas del proyecto, que tengamos más claras, y que sean lo más independiente posible 
de la redacción de otras partes del proyecto. Por ejemplo, se puede empezar por anexos donde se resuma las características 
de una herramienta que utilizamos, etc.., a continuación empezamos a redactar de manera individual los detalles de cada 
una de las etapas en las que se constituye el proyecto, no tienen que redactarse ordenadas según aparecen en el texto, 
sobre la elección sobre cual empezar, siempre primará que sea una parte que tengamos bien clara, y que hayamos delimitado 
su contenido para que sea independiente de la redacción de las otras etapas. Como verán, según vayan avanzando en la 
redacción, cada vez verán las cosas más claras, y de forma natural verán la forma de ir redactando las otras partes del 
proyecto, hasta llegar a las secciones de introducción y conclusiones y resultados que son las más delicadas de desarrollar, 
pues son las más importantes y las que previsiblemente se van a leer en mayor detalle las personas que lean el proyecto. 
Una deficiente redacción de la introducción (que es donde se atrae al lector sobre la importancia de lo que se va a hacer) 
o una mala presentación de las conclusiones y resultados (que es donde se transmite el mensaje de todo lo bueno que hay 
en proyecto) pone en entredicho la calidad global del proyecto. Una buena estrategia consiste en según se van redactando 
las diferentes secciones del proyecto, ir haciendo un borrador de las secciones de introducción y conclusiones y resultados, 
poniendo las ideas sueltas, y en principio desordenadas, que nos vayan surgiendo  y que puedan ser de utilidad en la redacción 
de estas secciones. Redactar bien tiene su dificultad y no todos los días tenemos la inspiración adecuada, para esos días 
negros, que no nos viene nada a la cabeza, lo mejor es dedicarse a cosas más mecánicas que no requieren tanta concentración, 
como puede ser completar la bibliografía, ir haciendo un manual de usuario o un anexo técnico, etc..

Cuando empezamos a redactar, siempre es necesario tener en cuenta algunos criterios básicos como son:

1. Escribir de cada cosa su esencia. Que es lo que es realmente relevante en la sección que estoy redactando y esforzarme 
en que ello quede claro

2. Ponerse en el lugar del potencial lector. El orden y la forma en la redactamos no es sólo para que nosotros tengamos 
claro lo que hacemos, es sobre todo para que una tercera persona que lea el texto lo pueda tener, si cabe, más claro 
que nosotros. Para ello hay que respetar un orden lógico en la forma en que presentamos las cosas y no presuponer que 
el lector conoce los entresijos de lo que estamos haciendo, hay que evitar dar saltos en el vacío, por ejemplo dando 
por supuesto conocimientos que el lector no tiene o alterando el orden natural en que deben aparecer las cosas.

Algunas ideas sueltas sacadas del libro "Como elaborar y presentar un trabajo escrito" cuyo autor es el profesor Santos Pérez:

El proyecto fin de carrera es un trabajo personal en el que el estudiante debe demostrar que domina el tema, sabe organizarlo, 
estructurarlo y elaborar en profundidad, y presentarlo en la forma normalizada de un trabajo técnico o científico. Es la 
ocasión que tiene el estudiante de demostrar que sabe analizar un problema, sabe seleccionar la metodología y técnicas 
apropiadas para reunir los datos, y alcanzar conclusiones razonables. Un proyecto de esta naturaleza permite la evaluación 
de la capacidad del estudiante para aplicar su conocimiento a un tema concreto.

El proyecto fin de carrera debe ser
\begin{itemize}
\item Proyecto Personal: debe ser un producto de la reflexión, investigación y esfuerzo del estudiante. Si se hace con 
 reflexión, con investigación y esfuerzo personal, el rendimiento que obtiene el estudiante es muy productivo y beneficioso para él y de una duración permanente.
\item  Es un trabajo documentado: es decir, serio, científico, hay que sustentar las afirmaciones con datos comprobables 
 y lógicamente fundados.
\item  Planificado: La elaboración de un proyecto fin de carrera es un proceso complicado. Un trabajo de esta naturaleza 
 requiere una planificación cuidadosa del tiempo: tiempo para investigar y documentarse, tiempo para reflexionar, 
 tiempo para corregir posibles desviaciones y finalmente tiempo para redactarlo y presentarlo de forma adecuada.
\end{itemize}

El esquema final de un proyecto fin de carrera debe llenar las siguientes características:

\begin{itemize}
\item Claridad: la claridad se consigue sobre todo con una nítida división y distribución del esquema. Y a su vez esta 
 claridad deriva también de la compresión en profundidad del material recogido.
\item Convergencia hacia el objetivo: El secreto de la claridad está en saber ordenar las partes del trabajo hacia el 
 objetivo buscado; es decir, en lograr que cada punto del esquema nos vaya encaminando con naturalidad hacia la meta 
 enunciada en el título del trabajo.
\item Coherencia: las distintas partes, puntos o párrafos deben estar trabados entre sí, concatenados, de forma que se 
 vayan preparando y completandose recíprocamente para conseguir el efectode que cada punto sea consecuencia del otro, 
 formando un todo orgánico y no una mera yuxtaposición de partes; por el contrario que se vaya mostrando la conexión 
 y la coherencia lógica de los distintos aspectos tratados
\item Conformidad con el objetivo: La estructura del esquema final debe resaltar lo más importante y debe dejar en la 
 penumbra los accesorios.
\item Elegancia: en la distribución del esquema, debe guardarse una cierta simetría y proporción. La elegancia no debe 
 subordinarse a la claridad y a la verdad; pero hay una elegancia no sólo formal, sino de concepción y elegancia 
 que contribuye significativamente a conseguir la armonía y transparencia en la transmisión del contenido principal 
 del tema. Conviene resaltar esta elegancia sobre todo ahora que nos encontramos en un mundo de zafiedad y donde se 
 hace gala del caos mental como norma de actuación.
\item El descanso inteligente: Una vez que se ha acabado la primera redacción del proyecto se sugiere tomarse unos 
 días de descanso suficientes para que la cabeza descanse del tema. Con este descanso se adquiere perspectiva, 
 y aumenta la objetividad y sentido crítico del autor.
\end{itemize}


% Aqui va la Bibliografía utilizada por el proyecto.

\begin{thebibliography}{1}

\bibitem{La86} Leslie Lamport {\em LaTex : A document Preparation System}. Addison"=Wesley, 1986.

\bibitem{Ro93} Christian Rolland {\em LaTex guide pratique}. Addison"=Wesley, 1993.

\bibitem{Castrillon05} M. Castrillón Santana, C. Guerra Artal and M. Hernández Tejera {\em Real"=time Detection of Faces in Video.} Face Processing in Video 2005, Victoria, Canada.

\bibitem{CastrillonSchmidt} M. Castrillón Santana, Joachim Schmidt {\em AUTOMATIC INITIALIZATION FOR BODY TRACKING: Using Appearance to Learn a Model for Tracking Human Upper Body Motions}


\bibitem{Chellappa95} R. Chellappa et al. {\em Human and machine recognition of faces: A survey.} Proceedings IEEE, vol. 83(5), 705~-740, 1995.

\bibitem{Cielniak03} G. Cielniak, M. Miladinovic, D. Hammarin, L. Göransson, A. Lilienthal and T. Duckett {\em Appearance-based Tracking of Persons with an Omnidirectional Vision Sensor} Proceedings of the Fourth IEEE Workshop on Omnidirectional Vision (Omnivis 2003)", Madison, Wisconsin", 2003

\bibitem{Deniz04} O. Déniz, A. Falcón, J. Méndez, M. Castrillón {\em Useful Computer Vision Techniques for Human-Robot Interaction.} International Conference on Image Analysis and Recognition, September 2004, Porto, Portugal.

\bibitem{Hjelmas01a} E. Hjelmas y B. K. Low {\em Face Detection: A Survey.} Computer Vision and Image Understanding, vol. 83(3), 2001.

\bibitem{Iges99} José Iges {\em El espacio. El tiempo en la mirada del sonido.} Catálogo de exposición. Kulturanea. España, 1999.

\bibitem{Krueger85} Myron W. Krueger, Thomas Gionfriddo y Katrin Hinrichsen {\em VIDEOPLACE: An Artificial Reality} Proceedings of the SIGCHI conference on Human factors in computing systems, 35~-40, 1985.

\bibitem{Levin04} Golan Levin y Zachary Lieberman {\em In-Situ Speech Visualization in Real-Time Interactive Installation and Performance.} The 3rd International Symposium on Non-Photorealistic Animation and Rendering (NPAR) June 7~-9 2004, Annecy, France

\bibitem{Samal92} A. Samal and P. A. Iyengar {\em Automatic Recognition and Analysis of Human Faces and Facial Expressions: A Survey.} Pattern Recognition, vol. 25(1), 1992.

\bibitem{Spalter99} Anne Morgan Spalter {\em The Computer in The Visual Arts.} Addison-Wesley Professional. 1st edition, 1999.

\bibitem{Viola01cvpr} P. Viola and M. J. Jones {\em Rapid Object Detection using a Boosted Cascade of Simple Features.} In Computer Vision and Pattern Recognition, 2001a.

\bibitem{Yang02} M. H. Yang et al. {\em Detecting Faces in Images: A Survey.} Transactions on Pattern Analysis and Machine Intelligence, vol. 24(1), 34~-58, 2002.

\bibitem{OpenCV09} Willow Garage: S. Hassan, S. Cousins, B. Gerkey et al. {\em OpenCV, Open Source Computer Vision} Official Page and Documentation: http://opencv.willowgarage.com/documentation/index.html 2009.

\bibitem{Chuck09} G. Wang, P. Cook et al. {\em Chuck, Strongly-timed, Concurrent, and On-the-fly Audio Programming Language} http://chuck.cs.princeton.edu/ 2009.

\bibitem{DME09} Devmasters.net {\em 3D Engines Data Base} http://www.devmaster.net/engines/ 2009

\bibitem{UDK09} Epic Games {\em Unreal Development Kit} http://www.udk.com/ 2009

\bibitem{UDKS09} Epic Games {\em Unreal Script Language Reference, Example Program Structure}  http://udn.epicgames.com/Three/UnrealScriptReference.html/ 2009

\bibitem{UDKFL09} Epic Games {\em Unreal Engine 3: Rendering Feature List}  http://www.unrealtechnology.com/features.php?ref=rendering 2009

\bibitem{CryE309} CryTek {\em CryENGINE 3 Educational SDK License} http://mycryengine.com/index.php?conid=42 2009

\bibitem{CryE3FL09} CryTek {\em CryENGINE 3 - Specifications}  http://www.crytek.com/technology/cryengine-3/specifications/ 2009

\bibitem{CryMod09} Crytek {\em CryTek Official Modding Portal} Educational community area for the CryENGINE 3 Software Development Kit: http://www.crymod.com/ 2009

\bibitem{wx09} wxWidgets {\em wxWidgets, Cross~platform GUI library} http://www.wxwidgets.org/ 2009

\bibitem{wxFL09} wxWidgets {\em wxWidgets Features} http://www.wxwidgets.org/about/feature2.htm 2009

\bibitem{KenB01} K. Beck, J. Sutherland et al. {\em Manifesto for Agile Software Development} http://www.agilemanifesto.org 2001

\bibitem{RCol09} R. Colusso {\em Desarrollo ágil de software} http://knol.google.com/k/desarrollo-ágil-de-software 2009

\bibitem{IJacobson09} I. Jacobson {Introducing the Essential Unified Process} 2009

\bibitem{BOOST10} D. Abrahams et al. {Boost C++ Liraries} http://www.boost.org/ 2010

\bibitem{SFML10} L. Gomila {Simple and Fast Multimedia Library} http://www.sfml-dev.org 2010

\bibitem{PGSQL10} PostGreSQL {\em PostGreSQL, The world most advanced open source database} http://www.postgresql.org/about/ 2010

\bibitem{DEBEA10} Debea {\em Debea Database Access Library} http://www.debea.net/ 2010

\bibitem{THOMASB00} Thomas B. y Erick Granum {\em Application Areas, Surveys and Taxonomies } A Survey of Computer Vision-Based Human Motion Capture 2000

\bibitem{LIYUAN03} Liyuan Li, Weimin Huang et al. {\em Foreground Object Segmentation } Foreground Object Detection from Videos Containing Complex Background 2003

\bibitem{HOMOGF} Homography at the Wikipedia http://en.wikipedia.org/wiki/Homography\_(computer\_vision)

\bibitem{CALIBCV} OpenCV Documentation {\em Camera Calibration and 3D Reconstruction } http://opencv.willowgarage.com/documentation/camera\_calibration\_and\_3d\_reconstruction.html/ 2010

\bibitem{JKAGG} J.K. Aggarwal, Q. Cai {\em Motion Analysis of Human Body Parts } Human Motion Analysis: A Review 

\bibitem{GAVRILA98} D.M. Gavrila {\em Action Recognition } The Visual Analysis of Human Movement: A Survey 1998

\bibitem{GREGMXIAOF} Greg Mori, Xiaofeng Ren, et al. {\em Finding Body Parts } Recovering Human Body Configurations: Combining Segmentation and Recognition

\bibitem{YASERY97} Yaser Yacoob {\em Parameterized modeling and recognition of activities } Parameterized Modeling and Recognition of Activities 1997


\bibitem{IVANAM} Ivana Mikic, Edward Hunter, Pamela Cosman. {\em  } Articulated Body Posture Estimation from Multi-Camera Voxel Data 

\bibitem{GREGMJM} Greg Mori, Jitendra Malik {\em  } Estimating Human Body configurations using Shape Context Matching 

\bibitem{JUERGEN} Juergen Gall, Carsten Stoll et al {\em Motion Capture Using Joint Skeleton Tracking and Surface Estimation}

\bibitem{CRAIGREYN} Craig Reynolds, {\em Boids Background and Update} http://www.red3d.com/cwr/boids/

\bibitem{JOHANNESK} Johannes Kilian, {\em Simple Image Analysis By Moments} March 15, 2001

\bibitem{LUKASK81} Bruce D. Lukas, Takeo KAnade,  {\em An Iterative Image Registration Technique with an Application to Stereo Vision} 1981

\bibitem{RTREE} R-Tree description at the Wikipedia http://en.wikipedia.org/wiki/R-tree

\bibitem{STEVES00} Steve Seitz, {\em From Images to Voxels} SIGGRAPH 2000 Course on 3D Photography

\bibitem{CHUCKD} Chuck Dyer, {\em Volumetric Scene Reconstruction from Multiple Views}

\bibitem{PAULB94} Paul Bourke, {\em Polygonising a scalar field: Also known as 3D Contouring, Marching Cubes, Surface Reconstruction} 1994

\bibitem{MCUBESW} Marching Cubes at the Wikipedia http://en.wikipedia.org/wiki/Marching\_cubes

\bibitem{BELKIN99} Alan Belkin, {\em A Practical Guide to Musical Composition} 1995-1999

añadir sdk de windows, sdk de directx


\end{thebibliography}


% Termina el documento
\end{document}
