% Definimos el estilo del documento
\documentclass[12pt,A4,spanish]{book}

% Definimos los márgenes de la página
\usepackage[lmargin=2.5cm,rmargin=1.5cm,tmargin=3.0cm,bmargin=3.0cm]{geometry}

% Utilizamos el paquete para utilizar español
\usepackage[spanish]{babel}

% Utilizamos el paquete para gestionar acentos
\usepackage[latin1]{inputenc}

%Utilizamos el paquete para inluir imágenes jpg
\usepackage{graphicx}

%Para evitar problemas de imágenes flotantes
\usepackage{float}


%Utilizamos el paquete para incorporar graficos postcript
%\usepackage[dvips,final]{epsfig}

%utilizamos paquete para introducir cuadros de código
%\usepackage[utf8]{inputenc}
%\usepackage[T1]{fontenc}
\usepackage{listings}
%\usepackage{times}
\usepackage{color}

\definecolor{gray97}{gray}{.97}
\definecolor{gray75}{gray}{.75}
\definecolor{gray45}{gray}{.45}
 

\lstset{ frame=Ltb,
     framerule=0pt,
     aboveskip=0.5cm,
     framextopmargin=3pt,
     framexbottommargin=3pt,
     framexleftmargin=0.4cm,
     framesep=0pt,
     rulesep=.4pt,
     backgroundcolor=\color{gray97},
     rulesepcolor=\color{black},
     %
     stringstyle=\ttfamily,
     showstringspaces = false,
     %basicstyle=\small\ttfamily,
     basicstyle=\small\ttfamily,
     commentstyle=\color{gray45},
     keywordstyle=\bfseries,
     %
     numbers=left,
     numbersep=15pt,
     numberstyle=\tiny,
     numberfirstline = false,
     breaklines=true,
   }
 
% minimizar fragmentado de listados
\lstnewenvironment{listing}[1][]
   {\lstset{#1}\pagebreak[0]}{\pagebreak[0]}
 
\lstdefinestyle{consola}
   {basicstyle=\scriptsize\bf\ttfamily,
    backgroundcolor=\color{gray75},
   }
 
\lstdefinestyle{C++}
   {language=C++,
   } 
   
%Para ajustar el tamaño de la fuente de la cabecera
%\usepackage{sectsty}
%\allsectionsfont{\tiny}
%\usepackage{titlesec}
%\titleformat{\chapter}{\bf\small}

\usepackage{fancyhdr, blindtext}
\newcommand{\changefont}{%
    \fontsize{\small}\selectfont
}
\fancyhf{}
\fancyhead[LE]{\small \slshape \rightmark} %section
\fancyhead[RE]{\small \thepage}
\fancyhead[RO]{\small \slshape \leftmark} % chapter
\fancyhead[LO]{\small \thepage}
\renewcommand{\headrulewidth}{0pt}
\pagestyle{fancy}
   
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\setcounter{tocdepth}{3}

%Empieza el documento
\begin{document}

\begin{titlepage}
% Definimos titulo, autor, fecha, generamos titulo e indice de contenidos
%\title{ARTE DIGITAL: UNA PROPUESTA DE INSTALACIÓN BASADA EN TÉCNICAS DE PERCEPCIÓN Y VIDA ARTIFICIAL MEDIANTE LA CREACIÓN DE UN SDK PARA LA CREACIÓN DE INSTALACIONES}
%\author{Antonio José Sánchez López}
%\date{Escuela de Ingeniería Informática.\\
%Universidad de Las Palmas de G.C.}

\begin{center}
\vspace*{\fill}

% Definimos titulo, autor, fecha, generamos titulo e indice de contenidos
\title{ARTE DIGITAL: UNA PROPUESTA DE INSTALACIÓN BASADA EN TÉCNICAS DE PERCEPCIÓN Y VIDA ARTIFICIAL MEDIANTE LA CREACIÓN DE UN SDK PARA LA CREACIÓN DE INSTALACIONES}
\author{Antonio José Sánchez López}
\date{Escuela de Ingeniería Informática.\\
Universidad de Las Palmas de G.C.}

%\maketitle no permite añadir imágenes, pero tampoco podemos eliminar el \title así que repetimos la info



{\LARGE ARTE DIGITAL: UNA PROPUESTA DE INSTALACIÓN BASADA EN TÉCNICAS DE PERCEPCIÓN Y VIDA ARTIFICIAL MEDIANTE LA CREACIÓN DE UN SDK PARA LA CREACIÓN DE INSTALACIONES}

\vspace{15 mm}
{\large Antonio José Sánchez López}

\vspace{10 mm}
{\large Escuela de Ingeniería Informática.\\
Universidad de Las Palmas de G.C.}
\vspace{70 mm}

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/logos.png}
\end{center}
\end{figure}


\end{center}
\end{titlepage}

\thispagestyle{empty}
%\maketitle

%Definimos página posterior a la portada 
\thispagestyle{empty}
\noindent Escuela de Ingeniería Informática. Universidad de Las Palmas de G.C.

% Definimos página con datos sobre el proyecto 
\newpage
\thispagestyle{empty}
\section*{Proyecto fin de carrera}

\bigskip
\noindent {\bf Título: } Arte Digital: Una Propuesta de Instalación basada en Percepción y Vida Artificial mediante la creación de un SDK para la creación de Instalaciones
\\

\noindent{\bf Apellidos y nombre del alumno: } Sánchez López, Antonio José
\\

\noindent{\bf Fecha : } Enero 2014 \\

\vspace{7 mm}
\noindent{\bf Tutor: } Castrillón Santana, Modesto \\

% Definimos página posterior 
\newpage
\thispagestyle{empty}
\noindent Escuela de Ingeniería Informática. Universidad de Las Palmas de G.C.
\newpage

% Definimos una  pagina para los agradecimientos
\newpage
\thispagestyle{empty}
\section*{Agradecimientos}
A mi familia, amigos, compañeros y profesores que me han ayudado a lo largo de la carrera y a llevar a cabo este proyecto. Especialmente a Modesto Castrillón, por su apoyo y guía para convertir este trabajo en una realidad, y también por impartir algunas de las asignaturas más apasionantes y de las que más he disfrutado a lo largo de la carrera. También me gustaría mencionar y agradecer a algunos de los profesores que especialmente me han marcado e influenciado durante estos años, y que han conseguido contagiarme no sólo de conocimientos sino también de cariño por este campo de la ingeniería: Octavio Mayor, Agustín Trujillo, Antonio Falcón, Javier Santana, Luis Álvarez, Alexis Quesada, Mario Hernández, Abraham Rodríguez, Esther González, José Fortes, Roberto Moreno, Miguel Ángel y José Pérez. También quiero dar las gracias a Óscar Rodríguez y a Gloria Godínez por prestarme un ratito su arte.

% Definimos página posterior 
\newpage
\thispagestyle{empty}
\noindent Escuela de Ingeniería Informática. Universidad de Las Palmas de G.C.
\newpage
\setcounter{page}{1}
\tableofcontents

% Definimos página posterior 
\newpage
\thispagestyle{empty}
\noindent Escuela de Ingeniería Informática. Universidad de Las Palmas de G.C.
\newpage

% Empezamos capitulos


\chapter{Introducci\'on}

A lo largo de la historia, diversas disciplinas han hecho uso de la representación bidimensional para mostrar propuestas creativas. El ordenador es en la actualidad una herramienta de enorme potencial para el arte visual \cite{Spalter99}, tanto en el marco de la imagen estática, como en el contexto donde el factor tiempo se introduce en el proceso expresivo.
\\

Las imágenes son fácilmente comprendidas por los humanos, motivo por el cual es un ámbito válido de trabajo creativo. Por otro lado, esa diversidad posible en una imagen ocupa también a multitud de científicos del campo de la Visión por Computador en su búsqueda de técnicas para detectar y reconocer objetos en ese espacio de representación: la imagen. Espacio donde un humano reconoce objetos conocidos con gran facilidad.
\\

Desde el punto de vista de la imagen estática, una imagen es una matriz de píxeles que representan un punto en un espacio de muy alta dimensionalidad. La tecnología digital en este contexto, presenta la singularidad de la no existencia de un original único, el arte digital permite disponer del original en cualquier parte, éste es copiable hasta la saciedad sin pérdida. Adicionalmente, nuevos ámbitos tecnológicos han ido abriendo capacidades y posibilidades expresivas. Las tecnologías del vídeo digital y la animación introducen el factor tiempo en el proceso expresivo, la mutabilidad, la fugacidad y la narrativa temporal. La introducción de la interactividad a través del uso de las tecnologías de visión por computador aporta un nuevo canal expresivo y unas posibilidades para la generación de sensaciones a través de los conceptos de obra viva e interactiva, tal y como ya describiera Krueger en su concepto de Realidad Artificial \cite{Krueger85}. 
\\

La obra se puede convertir así en única y cambiante, reactiva a la interacción con su entorno en cada momento. Recupera el concepto de exclusividad, siendo además posible registrar la vida de la instalación. Este enfoque se relaciona de forma clara con el concepto de instalación manejado en el mundo artístico una obra es instalación si dialoga con el espacio que la circunda \cite{Iges99}. 
\\

La motivación de este proyecto es investigar el uso de capacidades actuales de Visión por Computador y Vida Artificial para su integración en instalaciones artísticas. Hay que destacar que nuestra experiencia se relaciona fundamentalmente con el mundo tecnológico, por tanto, no es nuestro objetivo presentar una obra de creación, sino mostrar las posibilidades interactivas que la Inteligencia Artificial puede introducir en el arte, yendo más allá de aplicaciones básicas en visión de segmentación de figura y fondo en contextos restringidos como en Messa di voce \cite{Levin04}, y abordando el contexto de detección de personas y vida artificial. 
\\

La exploración puede plantear y profundizar en nuevas posibilidades de interfaces y formas de interacción hombre-máquina.
\\

Esta memoria hará una revisión de las técnicas estudiadas e implementadas de forma incremental. De esta forma, su contenido está dividido en 5 etapas de desarrollo. En la primera etapa se estudiarán herramientas y se evaluará cómo abordar el problema. En las siguientes etapas, el objetivo es obtener una demo funcional al final de cada una de ellas, que se irá enriqueciendo hasta conseguir el resultado final. 
\\

A lo largo de esta memoria podremos ver una progresión del proyecto, extendiendo sus funcionalidades y abordando problemas que requieren de la solución de los anteriores.
\\

Finalmente, el lector podrá encontrar un apartado de conclusiones donde se hará un recorrido por los resultados del proyecto. También se hará referencia a las posibles líneas de trabajo futuro que podrían ser de interés y extender la continuidad del mismo.


\chapter{Estado del arte}

Como comentábamos en la introducción, este proyecto se centrará sobre todo en la parte técnica. Sin embargo, y aunque no es el objetivo de este proyecto estudiar la historia del arte, se considera interesante introducir la relación existente entre estas manifestaciones artísticas y la informática para entender el campo en el que se mueve este proyecto y su evolución; incluso más allá, como en la industria de la producción audiovisual y del entretenimiento. 
\\

La relación entre Arte e Informática sucede, como en todos los casos, desde su origen. El arte no es sino una expresión de un individuo, de un momento, sociedad o época. Y como manifestación de esa expresión puede estar presente en todos los aspectos de la vida, desde la pintura, la escultura, la arquitectura, la escritura, la ilustración, el cine, las líneas de código de un programa, un plato de comida o incluso un partido de fútbol. 
\\

En el caso de la unión entre la informática y el arte, y en concreto el caso de las instalaciones, destacan dos propósitos sobre las que suelen girar las propuestas que se realizan: por un lado, la intención de transportar a un individuo a un entorno diferente del real o de hacerle partícipe de una experiencia, por otro la mera exploración de las capacidades tecnológicas viéndolas como herramientas o instrumentos para la producción creativa. De esta forma, el discurso que se plantea tiene dos capas que se superponen: cómo envolver al usuario en una experiencia, y cómo explotar las capacidades tecnológicas para conseguirlo.
\\

De esta forma, podemos hablar del arte virtual, y aquí podemos destacar intentos de construir entornos que envuelven al individuo para que crea o sienta estar en otro lugar, desde los orígenes de la humanidad, a lo largo de toda la historia. Retrocediendo en el tiempo, podemos hablar de los paisajes envolventes de las cuevas de Altamira o del Domus Aurea de Nerón \cite{OLIVERGRAU}, con su salón de banquetes giratorio gracias al agua de una cascada, o a los tejados de madreperla que dejaban caer pétalos y perfume sobre los invitados. Construcciones de este estilo existen a lo largo de todas las épocas, destacando sobre todo las casas paisaje, en cuyo interior, distribuídas incluso en varias plantas, se representaba un panorama completo y por el que el usuario se podía mover como si estuviera dentro de un jardín o en el campo. Algunas construcciones tenían un carácter más revolucionario y prometía trasnportarte a ciudades o parajes lejanos. La construcción del Panorama Rotunda de Robert Barker s.XIX \cite{BARKERXIX}, fue un gran ejemplo de esto, aunque este mismo concepto de cúpula lo tenemos bastante presente y podríamos considerar como su evolución el caso de los cines IMAX \cite{IMAXDOMET}.

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/estadoarte_inicios_realidadvirtual.png}
\end{center}
\caption{ \label{F_DomusAurea_PanoramaRotunda} Izquierda: Interpretación del Domus Aurea por Type-MOON \cite{TYPEMOONDA}, Derecha: Sección de la construcción Panorama Rotunda de Robert Barker \cite{BARKERXIX}.}
\end{figure}

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/estadoarte_inicios_domoIMAX.png}
\end{center}
\caption{ \label{F_DomoIMAX_Tijuana} Izquierda: Domo IMAX, Centro Cultural de Tijuana \cite{IMAXDOMET}, Derecha: Esquema de sección de un Domo IMAX por HowStuffWorks \cite{IMAXDOMEHSW}.}
\end{figure}


Pero sin duda, en relación a la informática las expresiones artísticas surgieron con mayor rapidez y expansión a partir de las capacidades que iban obteniendo los computadores para la representación gráfica. Desde este punto de vista caben destacar los experimentos de Ben F. Laposky y sus Abstracciones Electrónicas, en la década de los 50. Para conseguir los diseñes gráficos que pueden verse en la figura \ref{F_Oscillon_Laposky}, Laposky  \textit{" manipuló transmisiones electrónicas a través de la superficie fluorescente de un tubo de rayos catódicos de un osciloscopio (similar a un tubo de televisión) y entonces grabó los patrones abstractos usando película de alta velocidad, filtros de color, y lentes de cámara especiales"} \cite{LAPOSKY}.

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/estadoarte_laposky.png}
\end{center}
\caption{ \label{F_Oscillon_Laposky} Serie Oscillon de Ben F. Laposky \cite{LAPOSKY}.}
\end{figure}

Sin embargo, aunque es posterior cronológicamente, interesaría destacar antes aproximaciones menos electrónicas, para entender mejor la evolución del arte cibernético. Veinte años después de las Abstracciones de Laposky, se considera que surge en plena década de los 70, el movimiento conocido como Vanguardia Tecnológica \cite{CASTROM09}. Aunque incluye personalidades y trabajos anteriores o posteriores, está asociada mayormente a este época.
\\

En este movimiento se agrupan diferentes corrientes que aplican un enfoque más material y técnico a las obras que crean, formulando esquemas, programas o incluso usando objetos tecnológicos de vanguardia de la época, o cotidianos, como herramientas o piezas de las obras. Dentro de este movimiento de Vanguardia Tecnológica, nos interesa mencionar las siguientes corrientes:
\\

\begin{itemize}
\item Arte óptico: Mezclando ilusión y pintura, se centra en la interpretación de lo que el ojo ve y lo que la retina y el cerebro interpretan. Dentro de este movimiento, se considera a Víctor Vasarely \cite{VASARELY} como el padre y máximo exponente de este movimiento artístico.

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/estadoarte_vvasarely.png}
\end{center}
\caption{ \label{F_DomoIMAX_Tijuana} Arte Optico: Víctor Vasarely \cite{VASARELY}.}
\end{figure}
\vspace{10 mm}

\item Arte cinético: Se trata de otra de las corrientes de la Vanguarda Tecnológica, donde comenzaron a incluirse objetos industriales o cotidianos en las propias obras y cuyo eje gira entorno a la movilidad o la sensación de movilidad de las mismas. Existe en todos los soportes, pero destaca especialmente en la creación de instalaciones y en la escultura. Esta corriente fue muy prolífica y dentro de ella destacan artistas como Marcel Duchamp \cite{MDUCHAMP}, Alexander Calder \cite{ACALDER} o César Manrique \cite{CMANRIQUE}.

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/estadoarte_cieticoduchampcaldermanrique.png}
\end{center}
\caption{ \label{F_DomoIMAX_Tijuana} Arte Cinético: De izquierda a derecha: Rotary Glass Plates y Fountain de Marcel Duchamp \cite{MDUCHAMP}, Standing Mobile de Alexander Calder \cite{ACALDER} y Fobos de César Manrique \cite{CMANRIQUE}. }
\end{figure}


\item Arte cibernético: En esta corriente se da gran importancia a los procesos cognitivos, en cualquiera de sus representaciones. Ya sea la forma en que funciona la retina humana o la exploración de procesos automáticos para la interpretación o manipulación de la información con el fin de crear obras estéticas. En el origen de esta corriente destaca una gran importacia de las descripciones matemáticas formales, lingüísticas y el uso de sistemas computacionales. 
\\

Dentro de esta línea podríamos incluir a científicos, matemáticos o músicos, como a Ben F. Laposky con sus Abstracciones Electrónicas o John Cage, uno de los primeros en usar computadores para la composición musical y en creación de dibujos interpretables como composiciones \cite{JCAGEMKN}. Aunque fuera de esta línea, me gustaría hacer mención a Norbert Wiener, quién acuñó el término cibernética, e igualmente me gustaría hacer mención a su cita \textit{" The nervous system and the automatic machine are fundamentally alike in that they are devices, which make decisions on the basis of decisions they made in the past."} \cite{NWIENER}. 
\\

Norbert Wiener ha sido inspiración para muchos científicos y creadores. El arte cibernético no podría entenderse sin sus aportaciones.
\vspace{35 mm}

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/estadoarte_johncage.png}
\end{center}
\caption{ \label{F_JhonCage} Fontana Mix de John Cage, que puede verse y escucharse en \cite{JCAGEMKN}. }
\end{figure}

\end{itemize}


Más adelante surgiría el pop-art y muchas líneas más inspiradas en el arte tecnológico. En cualquier caso, este es un campo muy extenso, y sólo se desea hacer una introducción a la historia de la relación entre el mundo artístico, las instalaciones y la tecnología.
\\

Para acercarnos al final de este estudio, abordaremos el mundo de la Demoscene y de las instalaciones, conociendo algunas aplicaciones recientes.


\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/estadoarte_demoscene.png}
\end{center}
\caption{ \label{F_Demoscene} Izquierda-Arriba: cdak de Quite and Orange \cite{CDAKQO}, Derecha-Arriba: Chaos Theory del grupo Conspiracy \cite{CHAOSTC}, Inferior: Ellectric Bullet de a n d r o m e d a \cite{EBANDROMEDA}.}
\end{figure}

Como comentaba, no puede evitarse hacer una referencia a la Demoscene y sus demos \cite{DEMOSCENE}. La demo escena es un movimiento surgido en los años 70-80, originada por crackers \cite{SOFTCRACKING} que querían dejar su firma. Este movimiento se centra en la representación de composiciones de audio y escenas gráficas llamativas, que con el tiempo evolucionaron para explotar, cada vez más, el máximo de las capacidades tecnológicas de la máquina sobre la que trabajaban, para crear expresiones gráficas impactantes. Hay que tener en cuenta que no se tratan de vídeos, sino de aplicaciones que se ejecutan en dichas máquinas, principalmente consolas o PCs, en tiempo real. En la figura \ref{F_Demoscene} pueden verse algunos ejemplos, aunque los estilos visuales varían mucho de una escena a otra.
\\

Llegados a este punto, estamos en el momento adecuado para acercarnos a la corriente de las instalaciones basadas en técnicas de computación. Hay que recordar que una instalación es un espacio que dialoga con la persona que se encuentra dentro o a su alrededor. Destacan las que se centran en la producción visual, siendo en ocasiones meras herramientas preprogramadas o que funcionan bajo un control manual donde el individuo es mayormente un espectador, y, por otro lado, las que se alimentan de sensores para habilitar una interfaz humano-máquina no basada en la manipulación física de un periférico, donde el individuo cobra un rol de actor. 

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/estadoarte_instalciones_performances.png}
\end{center}
\caption{ \label{F_Demoscene} Izquierda-Arriba: Body Bavigation - Pong Duet de O. Kristensen\cite{OKRISTENSEN}, Derecha-Arriba: Mesa di voce de G. Levin \cite{FLONG}, Inferior: Mortal Engine de F. Weiss \cite{FWEISS}.}
\end{figure}

En la corriente artística de instalaciones basadas en técnicas de computación, y en especial de visión por computador, podemos destacar a creadores y artistas como Ole Kristensen \cite{OKRISTENSEN}, Golan Levin  \cite{FLONG} o Frieder Weiss entre muchos otros.
\\

Teniendo esto en cuenta, el proyecto que se presenta pretende ser dos cosas: Por un lado, una propuesta de instalación que conjugue elementos de Percepción y otros elementos de Inteligencia Artificial que presentará, mediante un proyector, una vista de un entorno en el que entrará el usuario. Una vez dentro, tanto su presencia como sus acciones irán definiendo el propio entorno creando una experiencia personalizada con la que podrá interactuar. Por otro lado, o mejor dicho, durante el camino, se creará una librería con la que será posible crear nuevas instalaciones o aplicaciones de este estilo. Para el desarrollo de esta librería se pretende que cada módulo de la misma sea completamente independiente de la implementación de los demás, mediante el uso de interfaces para que, de esta forma, sea sencillo hacer cambios o sustituir tecnologías. 
\\

Como cierre de esta sección, y dando un paso más allá de las instalaciones, me gustaría mencionar algunas convergencias de este campo con la creación de productos en la industria audiovisual y del entretenimineto. 

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/estadoarte_miziguchi.png}
\end{center}
\caption{ \label{F_Mizuguchi} Izquierda-Arriba: Juego Rez para PS2, Derecha-Arriba: Concepto usado en el vídeo de promoción de Child of Eden \cite{TMIZUGICHI}, Inferior: Captura in game del juego Child of Eden\cite{TMIZUGICHI} para Xbox con Kinect.}
\end{figure}

Esta combinación de interacción y producción, visual y sonora, proporciona un nicho muy interesante en el ámbito de la definición de interfaces hombre máquina, y sin duda en la producción de elementos artísticos. Empieza a ser frecuente ver algunos de estos ejemplos de uso tanto en el mundo de los videojuegos como en la producción audiovisual.
\\

Como puede verse en la figura \ref{F_Mizuguchi}, en el campo de los juegos, me gustaría destacar el caso de Tetsuya Mizuguchi \cite{TMIZUGICHI}, con el juego Child of Eden. Un título que como su antecesor, Rez, hace uso del concepto de sinestesia, mezclando los sentidos de vista y oído. En estos juegos, todas las acciones que hace el usuario reflejan una consecuencia visual y sonora. En el título original, Mizuguchi hacía uso del mando de la consola, pero con la llegada de la consola XBox y el dispositivo Kinect, el creador hizo uso del reconocimiento postural para que el jugador pudiera interactuar con la escena usando su propio cuerpo y manos. 
\\

Otro caso interesante a modo de ilustración puede verse en vídeos como 'Get Outta My Way' de Kylie Minogue, donde la cantante realiza su actuación dentro de una instalación realizada también por Frieder Weiss \cite{FWEISS} o Depeche Mode en 'Fragile Tension', videoclip donde se usan técnicas de percepción para crear representaciones visuales de los integrantes de la banda, por Memo Akten \cite{MAKTEN}.

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/estadoarte_videoclips.png}
\end{center}
\caption{ \label{F_VideoClips} Arriba: videoclip 'Get Out My Way' de la artista pop Kylie Minoge, instalación creada por F.Weiss \cite{FWEISS}, Debajo: videoclip 'Fragile Tension' de la banda Depeche Mode, usando varios de los experimentos visuales de Memo Akten \cite{MAKTEN}.}
\end{figure}


\chapter{Metodología}

Las Metodologías de Desarrollo Ágiles son un marco de trabajo conceptual de la ingeniería del software. Surgen como antítesis de los paradigmas anteriores y proponen alternativas para conseguir plantear y ejecutar proyectos de forma más eficaz y satisfactoria, teniendo en cuenta realidades que, por su propia naturaleza, distinguen a la informática del resto de industrias. Teniendo esto en cuenta, un grupo de desarrolladores elaboran el Manifiesto por el Desarrollo Ágil del Software, donde plantean un cambio a la hora de valorar los elementos que entran en juego en el desarrollo de un proyecto. Como conclusión,  hacen especial hincapié en los siguientes conceptos: la importancia de las interacciones entre las personas involucradas, la importancia de tener un producto funcional probable desde las primeras etapas de desarrollo, y la capacidad de responder ante el cambio \cite{MANIFESTOA}.
\\

En cualquier caso, alrededor de este concepto surgen múltiples marcos de trabajo y metodologías, algunas que se alejan radicalmente de las propuestas anteriores y otras que se acercan proponiendo una síntesis entre los elementos clásicos del desarrollo y las ventajas que aporta esta nueva aproximación. En cualquier caso, todas parecen estar de acuerdo en realizar iteraciones sobre un producto, volviendo sobre las distintas fases a lo largo del desarrollo para conseguir, en cada iteración, un incremento del mismo hasta alcanzar el resultado final.
\\

La fortaleza del desarrollo ágil se centra en minimizar los riesgos desarrollando software en lapsos cortos y en su capacidad de respuesta al cambio, enfatizando el software funcional como objetivo y promoviendo las comunicaciones eficientes \cite{KenB01}.
\\

Cada metodología introduce sus propias definiciones. Para este proyecto se ha usado una combinación de la metodología planteada por Invar Jacobson, llamada EssUp (Proceso Unificado Esencial) \cite{IJacobson09} y la surgida posteriormente SCRUM Manager \cite{SCRUMMANAGER}, una variación de SCRUM sobre la que se ha realizado un curso de certificación y del que se aprovechan una serie de herramientas que pueden ayudar en el flujo de trabajo. 
\\

Sumando estas metodologías y adaptándolas al caso que se presenta, definiremos un conjunto de fases: Planificación, Análisis de Requisitos, Diseño, Implementación y Validación. 

\begin{itemize}
\item \textbf{Planificación:} Se trata de definir el alcance del proyecto y de estructurarlo, tanto en contenidos como temporalmente. En este caso se hará una excepción y se planteará una planificación inicial al comienzo de este proyecto. Para el día a día, se plantea la práctica de revisar la lista de tareas al comienzo y finalización de cada sesión de trabajo, para revisar lo completado y lo que resta, así como el tiempo disponible. De esta forma se persigue mantener en mente una imagen de lo conseguido y del trabajo que queda por hacer, y permitirá reaccionar para adaptar el trabajo futuro y conseguir los objetivos propuestos manteniendo la planificación.
\item \textbf{Análisis de Requisitos:} Se plantearán los casos de uso que se quieren añadir a la demo que se está desarrollando. Se describirán a los actores y se mencionarán los procesos implicados.
\item \textbf{Diseño:} Se describirá de forma más formal las entidades que entran en juego. Se hará uso de Diagramas UML para reflejar estas descripciones y se dará un paso más en su definición.
\item \textbf{Implementación:} Esta fase es la principal y cubre el desarrollo de las nuevas funcionalidades y cuyo resultado será código que aportará un incremento de la librería y aplicación. En la memoria, se describirá en mayor detalle la solución adoptada para resolver el problema que se plantea, acompañando a las descripciones con fragmentos de código representativo.
\item \textbf{Validación:} En este apartado se hará revisión del trabajo realizado durante esta etapa y su integración con el resto del proyecto. Su objetivo es probar y validar que se consiguen los resultados deseados y que la demo está lo suficientemente cerrada para dar el siguiente paso. Igualmente se comentan y se enumeran los problemas más destacados o que hayan quedado pendientes para resolver en la siguiente en etapa.
\end{itemize}

En cualquier caso, hay que tener en cuenta que a la hora de ejecutar una iteración de este ciclo de desarrollo, puede visitarse cualquiera de las fases en todo momento, si esto es necesario. Es decir, se podrá ajustar el diseño durante la implementación, e igualmente se deberá testear y validar el código añadido mientra se desarrolla, no únicamente al final. 
\\

Como se verá en el Capítulo \ref{C_PlanTrabajo}, y que puede consultarse en la página \pageref{C_PlanTrabajo}, se plantean cinco bloques de trabajo que actuarán como hitos del progreso del proyecto, añadiendo cada vez funcionalidades más avanzadas que dependen de las anteriores. Además, salvo el caso inicial que se centrará en el estudio y búsqueda de herramientas, en el resto de etapas se dará especial importancia a construir una demo funcional al final de cada de ellas. Al final de cada iteración se evaluarán las prioridades del proyecto y planteará el nuevo bloque de trabajo. Con esta metodología se quiere conseguir un producto que podrá probarse desde las primeras semanas, que poco a poco irá creciendo hasta completarse \cite{RCol09}. Esta línea de trabajo refleja la propia naturaleza de los proyectos de desarrollo software donde, con frecuencia, las condiciones sobre los requisitos pueden variar a lo largo de un desarrollo.
\\

Invar Jacobson también ofrece software para el desarrollo de proyectos. De ellas se usará:

\begin{itemize}
\item \textbf{Essential Modeler:} Herramienta visual que permite crear diagramas de casos de uso UML y modelos de clases.
\end{itemize}

Para la planificación y seguimiento del proyecto se hará uso de la herramienta online WebProjectPlan vista durante la asignatura de Proyectos Informáticos. Sin embargo, más adelante, durante la Etapa 5 se hará uso de la herramienta MOOVIA \cite{MOOVIA}, una herramienta online gratuita orientada a proyectos ágiles. Esta herramienta permite la gestión de componentes, etapas de desarrollo y lista de tareas, cuyo estado puede ser modificado transportándolas entre las siguientes etapas:

\begin{itemize}
\item \textbf{Por hacer:} Tareas que deben realizarse y que no se han comenzado.
\item \textbf{En progreso:} Tareas en las se está trabajando actualmente.
\item \textbf{Entregadas:} Tareas realizadas pero que no se consideran cerradas hasta que sean validadas.
\item \textbf{Completadas:} Tareas que ya se consideran completamente terminadas.
\end{itemize}

Para cada etapa, el objetivo consistirá en llevar todas las tareas del estado 'Por hacer' a 'Completadas'.

\chapter{Recursos necesarios}
Se detallan los recursos hardware, software y otros necesarios para el desarrollo del proyecto, incluyendo desde las aplicaciones necesarias así como librerías a utilizar y requerimientos hardware.

\subsection{Software}

\begin{itemize}

\item Aplicaciones:
\begin{itemize}
\item \textbf{TexMaker \cite{TEXMAKER}:} Editor de LaTex \cite{La86}.
\item \textbf{EssWork \cite{ESSWORK}:} Editor de modelos de Casos de Uso.
\item \textbf{Dia \cite{DIA}:} Para modelado de diagramas UML y Entidad-Interrelación.
\item \textbf{WebProjectPlan \cite{WEBPP} y Moovia \cite{MOOVIA}:} Entornos de gestión de proyectos.
\item \textbf{Visual Studio 2008 Express Edition \cite{VS2008}:} Entorno de desarrollo.
\item \textbf{Blender \cite{BLENDER}:} Programa de edición 3D.
\item \textbf{Gimp \cite{GIMP}:} Programa de edición 2D.
\item \textbf{Audacity \cite{AUDACITY}:} Programa de edición de Audio.
\item \textbf{FreeSound \cite{FREESOUND}:} Portal de stock de recursos de audio con licencias Creative Commons.
\item \textbf{PostgreSQL \cite{POSTGRESQL}:} Base de datos SQL.
\end{itemize}

\item Librerías:
\begin{itemize}
\item \textbf{Visión por Computador:} OpenCV \cite{OPENCV}.
\item \textbf{Motor gráfico:} Se estudiarán UDK \cite{UDK}, CryEngine 3 ESDK \cite{CRYENGINEE}, Crystal Space \cite{CRYSTALSPACE}, OpenSceneGraph \cite{OSG}, Ogre \cite{OGRE}, Irrlicht \cite{IRRLICHT} y Panda3D \cite{PANDA3D}.
\item \textbf{Interfaz gráfica de usuario:} se estudiarán Qt \cite{QT} y wxWidgets \cite{WXWIDGETS}.
\item \textbf{Audio 3D:} se estudiará las capacidades del motor gráfico o de juego a usar, así como Fmod \cite{FMOD}, OpenAL 
\cite{OPENAL}, SDL Mixer \cite{SDLMIXER}, Clunk \cite{CLUNK}, Irrklang \cite{IRRKLANG} y SFML \cite{SFML}.
\item \textbf{Composición de audio:} Se estudiarán Chuck \cite{CHUCK} y Marsyas \cite{MARSYAS}.
\item \textbf{De sistema:} Boost \cite{BOOST} y SFML \cite{SFML}.
\item \textbf{Documentación:} Doxygen \cite{DOXYGEN}.
\end{itemize}

\end{itemize}

\subsection{Hardware}
\begin{itemize}

\item Primera aproximación:
\begin{itemize}
\item Entorno controlado: fondo estático, buena iluminación, sin ruido ambiente.
\item 1 PC con: 1 Monitor/Proyector, 1 webcam y altavoces estándares.
\item El PC para desarrollar deberá tener al menos una configuración equivalente a la siguiente: Intel core i7 de cuatro núcleos a 3.0Gz, 6Gb RAM, tarjeta gráfica dedicada válida para juegos con al menos 2Gb de RAM, 10 Gb de espacio libre en disco.
\end{itemize}

\end{itemize}

\subsection{Presupuesto}
\begin{itemize}
\item PC: 1.200 euros
\item Webcam: 20 euros
\item Altavoces: 50 euros
\item Monitor: 120 euros
\item Proyector: 500 euros
\item Gastos intrínsecos de luz y conexión a internet (aprox 20\% de las horas de uso al mes): 
\\18 meses x 31 euros = 558 euros
\item Horas de trabajo: No se percibe remuneración económica por el desarrollo de este proyecto. Si se hiciera, estimando un coste bruto de 20 euros y un 40 por cierto de seguridad social, quedaría en 28 euros por hora: 1.000 x 28 = 28.000 euros. 
\item TOTAL: 30.448 euros
\end{itemize}

\chapter{Plan de trabajo y temporización}\label{C_PlanTrabajo}
A continuación se muestra el plan de trabajo para el presente proyecto, desglosado en sus diferentes etapas. Siguiendo un proceso iterativo, se construirán versiones ejecutables y probables cada vez más completas. Para cada etapa, se refleja una estimación del tiempo que llevaría ejecutarlas, pudiéndose ver la distribución y proporción de la carga de trabajo entre ellas:

\begin{itemize}
\item Etapa 1: Acercamiento: Estudio de herramientas y Planteamiento inicial
\item Etapa 2: Primera Demo: Aplicación principal, GUI, Percepción y Producción
\item Etapa 3: Segunda Demo: Usuario, Entorno, Configuración y Persistencia
\item Etapa 4: Tercera Demo: Detección, Navegación e Interacción
\item Etapa 5: Cuarta Demo: Interacción Avanzada y Creación de Contenidos
\end{itemize}

\begin{figure} [h]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/planificacion.png}
\end{center}
\caption{ \label{Temporizacion} Plan de trabajo y temporización}
\end{figure}



\chapter{Etapa 1: Acercamiento}

En esta fase se llevará a cabo la primera aproximación a la solución que se va adoptar. Para ello se hará un estudio global de las capacidades de las que se desea dotar a la aplicación, que pretende ser completo, aunque no en profundidad. También se hará una búsqueda y estudio de las herramientas disponibles que puedan ofrecer las funcionalidades necesarias para la construcción del proyecto.
\\

En las siguientes fases/iteraciones se abordarán y trabajará en profundidad un subconjunto de cada apartado con el objetivo de conseguir demos intermedias que sean funcionales.

\section{Análisis}

\subsection{Documentación y herramientas}
  Existe un conjunto de librerías bastante amplio que puede ser de utilidad en el desarrollo del proyecto. A continuación se hace una revisión de las mismas con el fin de elegir las que mejor se adapten a las necesidades del proyecto. Finalmente se elegirán las más adecuadas.

\subsubsection {UDK: Unreal Development Kit}
  A noviembre de 2009, el motor gráfico Unreal3, desarrollado por Epic Games, es considerado uno de los más potentes y de mayor calidad junto con el motor CryEngine de Crytek. A mediados del mismo mes Epic Games decide liberar UDK (Unreal Development Kit) \cite{UDK}, su kit de desarrollo gratuito para proyectos sin ánimo de lucro o con beneficios inferiores, además de ofrecer licencias más asequibles para proyectos comerciales de mayor alcance. Inicialmente el coste de una licencia para Unreal3 oscilaba alrededor los \$700.000.  Esta nueva situación hace posible tomarlo en consideración para el desarrollo de la aplicación que tratamos, por lo que se procede a su estudio. Es interesante comentar que Epic Games ya trabajaba en la cuarta de versión de su motor, Unreal4 en 2009, que finalmente fue anunciada en marzo de 2013.
\\

Algunas de las compañías que han usado Unreal3 son: Atari, Activision, Capcom, Disney, Konami, Koei, 2K Games, Midway, THQ, Ubisoft, Sega, Sony, Electronic Arts, Square Enix y 3D Realms. Entre los ejemplos más destacados se encuentran las series BioShock, Mass Effect o Gears of War, entre otros juegos como Medal of Honor: Airbone y Unreal Tournament 3.
\\

Se exponen algunas de las características clave para la toma de decisión:
\\
\begin{itemize}
\item UDK es un kit de desarrollo software que ofrece un entorno completo para el desarrollo de videojuegos o aplicaciones similares. 
\item Está implementado con C++.
\item Aunque el motor Unreal3 es multiplataforma, UDK sólo está disponible para los Sistemas Operativos Windows por ahora.
\item El desarrollo de una aplicación en UDK puede realizarse en gran medida de forma visual a través de las opciones de la interfaz del entorno o escribiendo código en el lenguaje propio UnrealScript, similar a C++.
\item El código escrito en UnrealScript se ejecuta sobre una máquina virtual propia.
\item La máquina virtual de UnrealScript simula ejecución multihilos. Destacan que permiten la gestión de grandes cantidades de hilos, cantidad que hecho de otra forma, con el sistema de hilos nativo de Windows, podría provocar problemas.
\item La ejecución de código UnrealScript es mucho más lenta que C++. Detallan que el código en C++ es 20x más rápido.
\item No se tiene acceso al código fuente del motor.
\item La interacción con otro tipo de código en tiempo de ejecución debe hacerse a través de ficheros (.ini) que puedan leer UnrealScript, modificando o implementando controladores de periféricos, o a través de conexiones TCP/UDP. Este punto dificulta en gran medida la interacción con otras librerías necesarias para el desarrollo del proyecto.
\item Es gratis para uso no comercial. En proyectos comerciales es necesario abonar una licencia y un porcentaje de las ventas en concepto de royalties que puede llegar al 25\% para ingresos superiores a 5000 euros.
\end{itemize}
Las capacidades del motor gráfico son sobresalientes. Sin embargo, el esquema de desarrollo con UDK no se ajusta adecuadamente al del  resto de la aplicación.
\\

\subsubsection{CryENGINE 3 Educational SDK}

Como era de esperar, poco después de la salida del Unreal Development Kit por parte de Epic Games, otro de los grandes motores gráficos comerciales lanzó su propia alternativa. Se trata en este caso del motor CryENGINE 3, de Crytek, con su propio kit de desarrollo con licencia gratuita \cite{CRYENGINEE}. Sin embargo, hay que destacar que los términos de la misma son mucho menos flexibles que la de Epic Games y enfocado sólo hacia un sector. El SDK sólo está disponible para instituciones académicas por parte de jefes de curso, exclusivamente para proyectos de naturaleza interna y con fines no comerciales. Se excluye específicamente el caso de estudiantes individuales o proyectos de grupo. Además, los términos de la licencia en sí no son públicos y sólo son accesibles para personal académico previa solicitud. 
\\

Por ahora el único juego que parece usar este motor es Crysis 2, actualmente en desarrollo por Crytek. 
\\

Al ser una alternativa demasiado limitada, en términos de licencia de uso, y en contradicción con algunos puntos de la naturaleza del proyecto, como es el uso de software gratuito y en la medida de lo posible libre, se considera que no es adecuada para el proyecto que se desea desarrollar. Se exponen las características clave para la toma de decisión:

\begin{itemize}
\item Licencia de uso muy restrictiva. 
\item Sólo para proyectos académicos no comerciales.
\item No puede ser solicitada por estudiantes individualmente.
\item Continuidad futura del proyecto limitada.
\end{itemize}


\subsubsection{Crystal Space}
Crystal Space \cite{CRYSTALSPACE} es un kit de desarrollo comúnmente usado como motor de juego y la primera de las alternativas gratuitas que se estudiarán. Está escrito en C++ usando un modelo Orientado a Objetos y soporta las principales plataformas. El SDK se distribuye bajo licencia LGPL \cite{LGPL}, por lo que puede utilizarse de forma gratuita para cualquier tipo de desarrollo. Durante mucho tiempo se le ha considerado como uno de los motores libres más completos y populares, ya que no sólo es un motor de render sino un motor de juego, que incluye distintos módulos que facilitan el desarrollo.
\\

Algunos ejemplos de uso son PlaneShift, CAVACAN - El Hierro Virtual (ULPGC) y Yo Frankie! (Apricot Open Game Project) del Instituto Blender.
\\

Las claves para su evaluación son:

\begin{itemize}
\item Relación estrecha con Blender, entorno de diseño 3D y de desarrollo de videojuegos con el que se dispone de cierta experiencia.
\item Complejidad alta y relativamente abandonado, pasando largos períodos de tiempo entre revisiones. Curva de aprendizaje también muy elevada.
\item Nueva versión del SDK publicada el 25 de Enero de 2010, que incluye algunas mejoras y pocas funcionalidades añadidas. Entre ellas destaca el uso de un plugin basado en OpenAL para ofrecer sonido 3D.
\item Resultados visualmente pobres y de bajo rendimiento incluso en proyectos desarrollados por equipos con recursos y experiencia.
\end{itemize}

Como conclusión, es un motor que permitiría llevar a cabo el proyecto, pero parece una opción con una complejidad elevada y que podría aportar resultados pobres.

\subsubsection{OGRE: Object-Oriented Graphics Rendering Engine}
OGRE \cite{OGRE} es considerado uno de los motores gráficos open-source multiplataforma más populares y de más éxito. Está escrito en C++ y está dirigido al mismo lenguaje, aunque existen wrappers para Python, Java y .NET. 
\\

A diferencia de otros, no está planteado como un motor de juego, sólo como motor gráfico. Aunque se le puede añadir otras librerías para completar la arquitectura. La filosofía de Ogre es que el desarrollador pueda añadir Ogre como un módulo más en su aplicación, así como los módulos que necesite, en lugar de tener que adaptar la aplicación para que encaje correctamente con una solución que pretenda abarcar más de lo que se necesita y provocando incomodidades e incompatibilidades.

\begin{itemize}
\item Altamente modular, se le pueden añadir librerías y plugins para completar funcionalidades. 
\item Esto mismo hace que las capacidades iniciales sean más limitadas que las de otros motores y SDKs ya vistos, siendo necesario unir varios componentes para obtener una solución equivalente.
\item Dispone de un equipo de desarrollo dinámico y de una comunidad muy activa.
\item Curva de aprendizaje elevada.
\item La versión estable más reciente publicada tiene menos de 4 meses, publicada el 31 de Diciembre de 2009, con una lista amplia de mejoras.
\item Utiliza la licencia del MIT \cite{MITL}, por lo que Ogre es gratuito, libre y puede usarse para cualquier tipo de desarrollo.
\end{itemize}

Como conclusión, Ogre es una buena alternativa, aunque requiere un mayor trabajo inicial para conseguir la base necesaria sobre la que trabajar.

\subsubsection{OSG: OpenSceneGraph}
OSG \cite{OSG} se presenta como una librería multiplataforma para el desarrollo de aplicaciones que requieran de visualización 3D con alto rendimiento, como visualización científica, realidad virtual o incluso juegos. Es una librería ampliamente usada, con cierta tendencia hacia la visualización científica y geográfica, con una lista amplia de capacidades. Algunos ejemplos de uso son TerrainView, Pok3D, o CAPAWARE (ULPGC)

\begin{itemize}
\item Altamente modular y potente, con multitud de extensiones. 
\item Dispone de un equipo de desarrollo dinámico, con revisiones frecuentes y de una comunidad activa.
\item Se dispone de cierta experiencia con el entorno.
\item El diseño de la API puede ser confuso y en cierta medida deficiente en términos de facilidad de uso.
\item Debido a experiencias previas se conocen problemas del motor gráfico, especialmente si se desean múltiples regiones de render.
\item Documentación pobre.
\item Utiliza licencia LGPL, por lo que es gratuita, libre y puede usarse para cualquier tipo de desarrollo.
\end{itemize}

Como conclusión, OpenSceneGraph es una posible alternativa, aunque se conocen algunas deficiencias y problemas debido a experiencias previas.

\subsubsection{Irrlicht: Lightning Fast Realtime 3D Engine}
Irrlicht \cite{IRRLICHT} es un motor gráfico 3D multiplataforma de código abierto y de alto rendimiento. Está escrito en C++, pero también esta disponible para lenguajes .NET, así como otros lenguajes como Java, Perl, Ruby, Python o Lua, mediante bindings. 

\begin{itemize}
\item Altamente modular, potente y con multitud de extensiones. 
\item Dispone de un equipo de desarrollo dinámico, con revisiones frecuentes y de una comunidad activa.
\item Utiliza una licencia propia \cite{IRRLICHTLICENSE} basada en la licencia zlib/libpng, por lo que es gratuito, libre y puede usarse para cualquier tipo de desarrollo.
\item Mediante IrrEDIT se puede construir escenarios con facilidad que pueden usarse dentro del motor.
\item Sin embargo, otras librerías como IrrKlang, para sonido, se distribuyen bajo licencias más restrictivas.
\item Se define a la altura de los motores comerciales pero tiene ciertas carencias.
\end{itemize}

Como conclusión es una posible alternativa, aunque algunos de sus componentes hacen uso de licencias demasiado restrictivas.

\subsubsection{Panda3D: Free 3D Game Engine}
Originalmente Panda3D \cite{PANDA3D}, acrónimo de Platform Agnostic Networked Display Architecture, fue creado por Disney como parte de una de sus atracciones y posteriormente liberado, en 2002, con la intención de facilitar el trabajo con universidades y proyectos de investigación en Realidad Virtual. 
\\

Acogido por el Carnegie Mellon Entertainment Technology Center, fue mejorado y preparado para su uso público. Panda3D se define como un motor de juego y un entorno de trabajo para render 3D y desarrollo de juegos, libre, gratuito y multiplataforma, que puede usarse para cualquier tipo de desarrollo. Incluye un motor gráfico, de audio, gestión de entrada/salida y detección de colisiones entre otras capacidades.

\begin{itemize}
\item Dispone de un equipo de desarrollo dinámico, con revisiones frecuentes y de una comunidad activa. Aunque mayormente para Python.
\item Existe un menor dinamismo y soporte para C++.
\item Tiene un diseño orientado a mejoras futuras e incluye funcionalidades avanzadas cercanas a los motores comerciales que aún no incluyen otros motores libres.
\item Tiene un diseño de la API atractivo y sencillo de usar, aportando definiciones de alto nivel que facilitan la gestión de elementos típicos de un motor gráfico.
\item Énfasis en la documentación de la librería, en distintas modalidades: manuales, referencias, gráficos y plantillas conceptuales, que facilitan en gran medida la comprensión y uso de la librería. 
\item Se añade a la documentación tradicional, diversos video-tutoriales creados tanto por la comunidad, como clases impartidas sobre Panda3D por David Rose, del Instituto de Realidad Virtual de The Walt Disney Company.
\item Ampliamente usada por alumnos del Carnegie Mellon y en proyectos de equipos pequeños con resultados visuales atractivos y con buen rendimiento. 
\item Se distribuye bajo una licencia basada en BSD \cite{PANDA3DLICENSE} que permite su uso libre y gratuito para todo tipo de proyectos.
\end{itemize}

Como conclusión es una alternativa prometedora, especialmente por su diseño de la API y por la calidad visual conseguida, incluso en pequeños proyectos realizados por estudiantes.

\subsubsection{Doxygen: Source code documentation generator tool}
Doxygen \cite{DOXYGEN} es un generador de documentación multiplataforma para C++, además disponible para otros lenguajes. Permite generar documentación para su visualización online en un navegador web en HTML, así como LaTex, MS-Word, PostCript, PDF con hipervínculos, entre otros. Además puede usarse para extraer la estructura de código de fuentes no documentados, y generar grafos de dependencias, diagramas de clases y otros esquemas.
\\

Doxygen se distribuye bajo licencia GPL2 \cite{GPL2}. De cualquier forma, los documentos producidos mediante Doxigen son trabajo derivado de los datos usados en su producción, por lo que no se ven afectados por la licencia. No se estudian más casos por considerarse la mejor opción y tener cierta experiencia con ella, lo que es una ventaja determinante.
\\

\subsubsection{Boost C++ Libraries }
Boost \cite{BOOST} es un conjunto de librerías de código abierto multiplataforma con la intención de extender las capacidades del lenguaje de programación C++. Varios fundadores de Boost forman parte del Comité ISO de Estándares de C++, por lo que algunas de estas librerías terminan por introducirse en la siguiente versión estándar de C++. Utiliza una licencia propia, la Boost Software License, que permite su uso en cualquier tipo de proyectos, comerciales o no. Algunas de las librerías más extensamente utilizadas son las que facilitan las operaciones con el sistema, operaciones de Entrada/Salida con dispositivos y la gestión de hilos, entre muchas otras.


\subsubsection{Qt: Cross-platform Application and UI Framework}

Qt \cite{QT} es la librería multiplataforma de Trolltech para desarrollar interfaces gráficas de usuario, así como para desarrollar aplicaciones de consola y servidores. Utiliza como lenguaje C++, aunque también está disponible para otros lenguajes a través de binding (Python, C\#, Ruby, Java, Ada y Php, entre otros).
\\

Qt es ampliamente usada y considerada como una de las opciones multiplataforma más completa y estable. Algunos ejemplos de uso son principalmente KDE, a partir del cual logró un considerable éxito y expansión, además de otras aplicaciones como Google Earth o Skype.
\\

Tras polémicas en sus inicios por publicitarse como código libre sin serlo, actualmente la biblioteca es gratuita y libre tomando en consideración las condiciones de las opciones LGPL 2.1 y GPL 3.0 para el proyecto. También existe otra opción de pago destinada para software comercial que no quiera cumplir con las condiciones anteriores. 


\subsubsection{wxWidgets: Cross-platform GUI Library}
Wxwidgets \cite{WXWIDGETS} consiste en una librería para C++ para el desarrollo de interfaces gráficas de usuario. Es una de las pocas opciones realmente multiplataforma, gratuita y open source disponibles. Tiene la capacidad para ofrecer interfaces de aspecto nativo dependiendo de la máquina sobre la que se ejecuta el código. 
\\

Wxwidgets, aunque con sus desventajas y limitaciones, es una librería ampliamente usada que ha creado comunidad. Algunos de sus usarios más conocidos son: AOL (AOL Comunicator), California Institute of Technology (Gambit), Carnegie Mellon University (Audacity),   Grisoft Inc. (AVG Antivirus), NASA (NASGRO), National Human Genome Research Institute - USA
(ComboScreen), TomTom (TomTom HOME), Xerox (VIPP) o la propia Universidad de Las Palmas de Gran Canaria (CAPAWARE), entre otros.
\\

Se exponen algunas de las características clave para la toma de decisión sobre su uso:
\begin{itemize}
\item Librería multiplataforma gratuita y open source para desarrollar interfaces gráficas de usuario para aplicaciones de escritorio.
\item No todas las opciones que ofrece la librería funciona en las distintas plataformas, por lo que hay que tener especial cuidado si que quiere que el proyecto sea, en potencia, multiplataforma. Esto limita las opciones de la librería que pueden usarse y la calidad visual del resultado.
\item Existe documentación, foros y una comunidad bastante amplia y activa.
\item Hay que tener en cuenta que durante el funcionamiento de la aplicación no existe una interfaz gráfica destinada al usuario en términos tradicionales. Es decir, no existen ventanas, botones, menús ni indicadores destinados a que el usuario las maneje. Por lo tanto no es un apartado crítico del proyecto. Sin embargo, se desea una interfaz gráfica que permita preparar la instalación en su situación final, además de gestionar ventanas y para mostrar las opciones de configuración de la aplicación. Se considera que las capacidades de wxWidgets resultan suficientes para estas necesidades.
\item Se dispone de amplia experiencia previa con la librería durante el desarrollo de CAPAWARE y otros proyectos.
\end{itemize}

Se decide optar por esta librería por cubrir las necesidades básicas del proyecto, por ser gratuita y libre, y por tener una extensa experiencia con la misma, lo que es determinante para considerarla la mejor opción.

\subsubsection{OpenCV: Open Source Computer Vision}
OpenCV \cite{OPENCV} es una librería multiplataforma de funciones de visión por computador en tiempo real. Está desarrollada inicialmente por Intel, siendo gratuita tanto para uso comercial como para investigación bajo licencia BSD. Surgió en 1999 como una iniciativa de Intel para mejorar aplicaciones intensivas en CPU, formando parte de una serie de proyectos que incluían ray tracing en tiempo real y pantallas de representación 3D. Actualmente acaba de salir la versión 2.0 que incluye amplias mejoras a la interfaz con C++, mejor prototipado, nuevas funciones y mejoras de rendimiento, especialmente en sistemas multicore. 
\\

Algunas de sus aplicaciones son HCI (Human-Computer Interaction), Identificación, Segmentación y Reconocimiento de objetos, Reconocimiento de Caras, Reconocimiento de Gestos, Motion tracking, Ego Motion, Motion Understanding, SFM (Structure from Motion), Calibración estéreo y multi-cámara, Percepción de Profundidad y Robótica móvil.
\\

Se propone utilizar la librería principalmente para las tareas de reconocimiento de objetos, caras, gestos y movimiento, lo que formaría parte del Módulo de Percepción. Además, es un requisito para el uso de la librería Encara2, que se usará para la detección de caras en tiempo real.

\subsubsection{Encara2: A Real-Time Multiple Face and Facial Feature Detector for Perceptual Interfaces for Face Recognition and Description}
Encara2 \cite{Castrillon05} \cite{ENCARA2} es la librería de reconocimiento facial en tiempo real desarrollada desde el SIANI: Instituto Universitario de Sistemas Inteligentes y Aplicaciones Numéricas en Ingeniería \cite{SIANI}. 
\\

Esta librería está desarrollada en C++ sobre OpenCV y propone un sistema de detección de caras y características faciales como ojos, nariz y boca, que da un paso más allá de los esquemas planteados comúnmente para imágenes estáticas. El sistema, dada una secuencia de imágenes, aprovecha información extrínseca sobre coherencia espacial y temporal, para crear un sistema robusto y eficiente para la detección de múltiples caras incluso en tiempo real. Los resultados conseguidos mejoran a los planteados por Viola-Jones \cite{Viola01cvpr}. Su desarrollo continúa, centrándose actualmente en ofrecer más características faciales como sexo y edad del sujeto observado.
\\

Encara2 se encuentra bajo una licencia que permite su uso para fines no comerciales. En cualquier caso, es posible contactarlos para establecer una comunicación. Esto puede hacerse a través de la página oficial \cite{ENCARA2}. Esta herramienta será incluída en el proyecto y podrá verse más información sobre la misma en la sección \ref{Encara2Details}, página \pageref{Encara2Details}. 

\subsubsection{Fmod: Music \& Sound Effects System}
Fmod \cite{FMOD}  es una librería de audio propietaria y multiplataforma de Firelight Technologies que soporta un amplio abanico de formatos de audio. También tiene capacidad para reproducir sonido 3D en sistemas de sonido envolventes.
\\

La biblioteca es ampliamente utilizada en juegos y varios motores gráficos incluyen soporte para la misma. Algunos ejemplos son: BioShock, Call of Duty 4, Crysis, Far Cry, la saga Guitar Hero, Heavenly Sword, Hellgate: London, Metroid Prime 3, Second Life o World of Warcraft, entre otros.
\\

Fmod está disponible siguiendo distintos esquemas. Sin embargo, no es de código abierto y sólo es gratuito para desarrollo de aplicaciones no comerciales.

\subsubsection{OpenAL: Cross-platform 3D Audio API}
OpenAL \cite{OPENAL} ofrece una API de audio multiplataforma desarrollada por Creative Labs destinada la reproducción de audio posicional y multicanal en 3D. Se ideó para su uso extenso en videojuegos siguiendo las mismas convenciones que OpenGL, consiguiendo convertirse en un estándar aceptado. 
\\

La biblioteca es ampliamente usada en juegos y varios motores incluyen soporte para la misma. Algunos ejemplos relativamente recientes son: Doom3, Quake4, Unreal2, Unreal Tournament 3 o Hitman2. 
\\

Sin embargo, en los últimos años el mantenimiento de la misma se ha descuidado y aparecen errores, especialmente en sistemas de 64 bits. Su API es especialmente confusa y su curva de aprendizaje muy elevada. Son comunes las nuevas variaciones de OpenAL que le dan continuidad, como OpenAL Soft, OpenAL++ o distintas librerías a modo de wrapper que usan OpenAL por debajo, como SFML.
\\

OpenAL y sus variaciones parecen ser la única opción encontrada multiplataforma gratuita y libre para cualquier tipo de desarrollo, que provea de posicionamiento 3D de audio en sistemas de sonido envolventes.

\subsubsection{SDL\_Mixer: Simple DirectMedia Layer Mixer}
SDLMixer \cite{SDLMIXER} es una librería multiplataforma que permite el acceso de bajo nivel a dispositivos de audio, periféricos y hardware 3D, SDL\_Mixer se centra en el primer apartado facilitando la mezcla de sonido multicanal, así como la carga de samples y de música de distintos formatos. Sin embargo, aunque permite el acceso a bajo nivel, no aporta mayores funcionalidades por sí misma. Además parece tener un mantenimiento muy escaso.

\subsubsection{Clunk: Open Source 3D-Sound library}
Clunk \cite{CLUNK} es una librería de código abierto para C++ que pretende dar soporte para la generación de sonido 3D, binaural, en tiempo real. Propone una API bastante manejable y sencilla de usar, con un modelo orientado a la gestión de objetos. Sin embargo aún se encuentra en fase de testeo antes de lanzar su primera release. 
\\

Está preparada para generar sonido envolvente binaural, el cual tiene su mejor efecto en auriculares, aunque no tanto en altavoces. Por otro lado del modelo de escucha sólo tiene en consideración posición, velocidad y dirección de orientación, sin información de verticalidad; por lo que no es posible definir realmente su estado en un entorno 3D, sólo en el plano horizontal.

\subsubsection{Irrklang: High level 3D audio engine/API}
Irrklang \cite{IRRKLANG} ofrece una API de alto nivel para sonido 2D y 3D en multiplataforma enfocado hacia C++ y lenguajes .NET. Da soporte para un amplio abanico de formatos de sonido y provee de una API que parece muy sencilla de usar. Se puede encontrar bajo distintas licencias pero sólo es gratuito para desarrollos no comerciales.
\\

Al igual que OpenAL y FMOD, Irrklang sí permite definir completamente las propiedades necesarias para orientar en 3D tanto los sonidos como la escucha. Sin embargo, la limitaciones de licencia son menos interesantes que las de otras alternativas, dado que no permite desarrollar cualquier tipo de proyecto.


\subsubsection{SFML: Simple and Fast Multimedia Library}
SFML \cite{SFML} se ofrece como una API open source multimedia que provee mecanismos para la gestión del sistema, de gráficos, interfaz gráfica de usuario, sonido, periféricos y de red. La librería es multiplataforma y se encuentra disponible para varios lenguajes como C, C++, .NET, Python o Ruby, entre otros. 
\\

Sigue un diseño Orientado a Objetos y define un interfaz fácil de usar y de integrar en otros proyectos. Se compone de diferentes paquetes que pueden ser usados en conjunto o individualmente. Es de especial interés el paquete de sonido, que funciona a modo de wrapper de OpenAL, arreglando algunos de los bugs que tiene y simplificando su uso. 
\\

Se distribuye bajo licencia zlib/png \cite{SFMLLICENSE}, de la Open Source Initiative, por lo que es gratuita y abierta para todo tipo de proyectos.
\\

Tiene varios defectos. Por un lado, en el paquete de sonido la documentación es errónea, y puede llevar a confusiones importantes. Además la definición de la API, aunque más simple, limita la funcionalidad real de OpenAL hasta el punto de que no puede orientarse correctamente la escucha en 3D, por lo que sólo puede usarse en 2D o 3D en el plano horizontal. Sin embargo, pequeñas modificaciones en la librería permitirían recuperar esa funcionalidad sin problemas.

\subsubsection{Chuck: Strongly-timed, Concurrent, and On-the-fly Audio Programming Language}\label{SUBSUBSEC_CHUCK}

Chuck \cite{CHUCK} es un lenguaje de programación para el análisis, síntesis, composición y producción de audio. La librería es multiplataforma y presenta un modelo de programación concurrente basado en tiempo, altamente preciso (strongly-timed) y con la habilidad de añadir y moficar código en tiempo de ejecución. Chuck soporta dispositivos MIDI, OSC, HID y audio multicanal. Se considera como una opción muy interesante para la producción de audio.


\subsubsection{Marsyas: Music Analysis, Retrieval and Synthesis for Audio Signals}\label{SUBSUBSEC_MARSYAS}
Marsyas \cite{MARSYAS} es una librería y framework para C++ para el procesado y síntesis de audio. Hace especial énfasis en sus aplicaciones la extracción de información musical. Creada principalmente por George Tzanetakis y desarrollada de forma abierta, está enfocada hacia el prototipado rápido y la experimentación en el análisis y síntesis de audio, procurando un alto rendimiento. Fue finalista para el premio por elección del público de la comunidad de SourgeForge en 2009, y pueden verse referencias a proyectos de muy diferente naturaleza que la utilizan en su desarrollo. Se considera que es una herramienta muy interesante, tanto para el análisis como la síntesis de audio.

\subsubsection{PostgreSQL}
PostgreSQL \cite{POSTGRESQL} es una potente bases de datos objeto-relacional open source. Es considerada la BBDD open source más avanzada y potente, con más de 15 años de desarrollo. Está disponible para múltiples sistemas operativos Linux, UNIX y Windows y dispone de interfaces con múltiples lenguajes, entre ellos C++. 


\subsubsection{OODBMS: Object Oriented Data Base Management Systems}
Se ha realizado una búsqueda bastante extensa sobre Bases de Datos Orientadas a Objetos. Sin embargo, no ha sido posible encontrar ninguna que se adapte a los requisitos software del proyecto. A continuación, se muestra un listado de las soluciones encontradas y se resume el detalle que implica su descarte:
\begin{itemize}
\item EyeDB: Disponible para C++, potente y estable. Sólo está disponible para Linux.
\item Metakit: Base de datos embebida, esto limita su uso, conectividad y crecimiento futuro del proyecto.
\item db4o: Embebida, no disponible para C++, sólo para Java o C\#.
\item Orient: Versión para Windows sólo embebida. Mala documentación.
\item NeoDatis: Sólo para Java o C\#.
\item Perst: Embebida.
\item Odaba: API confusa, el proyecto es antiguo y parece abandonado.
\item Matisse: Licencia gratuita restrictiva, sólo para prueba de la librería.
\item Jade6: Licencia restrictiva, sólo para prueba de la librería.
\item Bifröst: Abandonado.
\item Cerebrum: Abandonado.
\item Frontier: Abandonado.
\item Oviedo3: Abandonado.
\item Thor: Proyecto del MIT, no disponible aún.
\item MongoDB: No es orientada a objetos sino a documentos, ofrece ventajas si la estructura de datos es lineal, pero ofrece mala eficiencia cuando hay en altas cantidades de consultas relacionales entre los datos.
\end{itemize}

\subsubsection{Debea: Database Access Library}
Debea \cite{DEBEA} consiste en una librería en C++ que actúa como mapper entre el modelo orientado a objetos de la aplicación y el esquema de persistencia elegido (base de datos SQL, CSV o ficheros XML). En el caso de SQL, aunque encapsula la persistencia de los objetos incluyendo una API sencilla para la carga y almacenamiento de los mismos y sus relaciones, sigue siendo posible ejecutar comandos SQL si se desea realizar consultas elaboradas. Usa la licencia de wxWindows \cite{DEBEALICENSE} por lo que es gratuita y libre.
\\

También está disponible para Linux y Windows. Soporta de forma nativa bases de datos SQLite3 y PostgreSQL, además de ficheros csv y xml y ofrece una API sencilla y una documentación clara.

\subsection {Análisis de Requisitos de Usuario}
La propuesta de instalaciones que se podrán crear con este kit de desarrollo se presenta como un espacio en el que el usuario puede entrar y moverse. Dentro de ese espacio se encontrará con una o varias pantallas, cámaras y altavoces. A través de las cámaras se capturará información para su análisis y para la producción de efectos, y mediante las pantallas y los altavoces se mostrará el entorno generado. Como puede verse en la Figura \ref{F_CasosdeUso}, es mediante la interacción del usuario con el espacio como el entorno virtual es generado. A su vez, este entorno será capaz de actuar y evolucionar por su cuenta dentro de un esquema de comportamiento, también definido mediante la interacción del usuario durante la sesión. Capacidades añadidas permitirían a un usuario hablar con el entorno o entrar en el entorno de otro usuario. Aunque estos apartados se consideran extensiones del trabajo a realizar y se dejarán como trabajo futuro en función de la evolución del proyecto.
\\

Teniendo en cuenta estos aspectos, se detallan los siguientes requisitos:

\begin{itemize}
\item Captura de imagen del usuario.
\item Captura de sonido del usuario (en función de la evolución del proyecto).
\item Reconocimiento mediante técnicas de visión por computador (presencia, posición, cara, etc.).
\item Análisis de audio (en función de la evolución del proyecto).
\item Crear entornos propios para un usuario (crear, borrar, cargar escenarios).
\item Generación de efectos visuales.
\item Generación de efectos de audio y composición musical.
\item Generación de elementos 3D.
\item Generación de esquemas de comportamiento
\item Generación de elementos de vida artificial.
\item Capacidad para crear una experiencia única que dependa de la interacción del usuario.
\item Interacción.
\begin{itemize}
\item Con la escena:
\begin{itemize}
\item Vista: Perspectiva Visual (asociación de la cámara a la presencia).
\item Oído: sonido estéreo ó 3D envolvente.
\item Movimiento dentro de la escena:
\begin{itemize}
\item No estar.
\item Estar.
\item Quedarse quieto.
\item Moverse. 
\item Colisiones del usuario con los elementos de la escena (tocar o golpear).
\item Elementos: crear, destruir o provocar una interacción o respuesta.
\item Estudio del comportamiento e interacción que sentará las bases del estado del entorno (tipo de escena, iluminación, sonidos, efectos, tipos de elementos creados, comportamientos de los mismos, etc.).
\end{itemize}
\end{itemize}
\end{itemize}
\end{itemize}

\newpage
\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/casosdeuso.png}
\end{center}
\caption{ \label{F_CasosdeUso} Modelo de Casos de Uso.}
\end{figure}

\subsection {Análisis de Requisitos de Software}

Además de conseguir como resultado una Instalación que muestre las capacidades tecnológicas estudiadas, así como la experimentación en  el campo de las interfaces y formas de interacción del usuario, este proyecto pretende ofrecer un entorno de trabajo para el futuro desarrollo de nuevas aplicaciones de este tipo. De esta forma, la aplicación se enfoca como un proyecto open source. 
\\

El proyecto estará compuesto de diferentes módulos independientes, de forma que la implementación subyacente sea modificable sin afectar al resto de los módulos. Para ello se debe definir un esquema de interfaces.
\\

Teniendo en cuenta estas características, se detallan los requisitos software del proyecto:
\begin{itemize}
\item Uso de software libre y multiplataforma.
\item Uso de recursos gratuitos.
\item El framework producido será software libre.
\item Facilitar su futura portabilidad usando sólo recursos para los que existan equivalentes en las otras plataformas a ala de desarrollo.
\item Diseño de interfaces de forma que los módulos sean fácilmente sustituibles.
\item Sistema local/online de persistencia (según la evolución del proyecto).
\end{itemize}


\section{Diseño}

\subsection{Estructuración de Conceptos}

\subsubsection{Aplicación}

\begin{itemize}
\item Aplicación:
\begin{itemize}
\item Programa de que se ejecuta en un ordenador.
\item Carga un escenario para un usuario.
\item Puede funcionar sin que ningún usuario lo manipule manualmente durante las sesiones.
\end{itemize}
\end{itemize}

\subsubsection{Usuario}
\begin{itemize}
\item Usuario:
\begin{itemize}
\item El usuario se presenta en la instalación y es reconocido por su cara, si no era conocido se le crea un entorno/escenario.
\item El usuario puede querer crear un escenario nuevo o abrir uno anterior.
\item La interacción del usuario define un patrón de comportamiento y modifica la producción sonora y visual.
\end{itemize}
\end{itemize}

\subsubsection{Entorno}
\begin{itemize}
\item Escenario/Entorno:
\begin{itemize}
\item Es creado para un usuario.
\item Representa elementos mediante objetos 3D, sonidos y efectos.
\item Responde ante la interacción del usuario y cambia según sus acciones.
\item Los cambios y la interacción del escenario define un patrón de comportamiento que establece cómo cambiará el entorno.
\item Componentes: Espacio, entidades.
\end{itemize}
\end{itemize}

\begin{itemize}
\item Espacio: 
\begin{itemize}
\item Elementos gráficos usados como trasfondo.
\item Sonido ambiental.
\end{itemize}
\end{itemize}

\begin{itemize}
\item Entidades:
\begin{itemize}
\item Usuarios: El usuario es una entidad que existe en la escena.
\item Objetos no interactivos.
\item Objetos interactivos que reaccionan ante el usuario.
\item Sonidos de las entidades según sus acciones.
\end{itemize}
\end{itemize}


\subsubsection{Resumen}
Llegados a este punto se observan las siguientes necesidades:

\begin{itemize}
\item Aplicación que implemente y englobe las funcionalidades.
\item Interfaz gráfica de usuario genérica para la gestión de ventanas y para mostrar las opciones de configuración de la aplicación.
\item Modificación de elementos y datos, carga y guardado de los mismos.
\item Base de Datos.
\item Percepción del entorno mediante técnicas de Visión por Computador.
\item Generación de elementos con los que componer un entorno.
\item Análisis y definición del patrón de interacción del usuario y de la evolución del entorno.
\end{itemize}

\subsection{Diseño de la Aplicación}

A partir de esta descripción se pueden diferenciar y extraer los diferentes módulos de los que es necesario que se componga la aplicación y que pueden verse en la Figura \ref{F_DiagramadeClasesUML}. Hay que tener en cuenta además que se desea conseguir una alta modularidad e independencia entre las distintas secciones, que permita extraer e intercambiar módulos con facilidad. De esta forma, las implementaciones de cada módulo deben ser capaces de comunicarse, pero también deben desconocer con qué tecnología se han desarrollado. Por ello, se sigue un modelo basado en el uso de interfaces. 
\\

Se define un núcleo que albergará la definición de estas interfaces abstractas, así como de estructuras de datos básicos o plantillas que permitan la manipulación de estructuras de datos comunes entre módulos. Sobre el núcleo, se implementará cada módulo y, finalmente, se construirá la aplicación, haciendo uso de las librerías generadas por éstos.

\begin{itemize}
\item core: Conjunto de interfaces de la aplicación.
\begin{itemize}
\item IAplication: Define la interfaz de una aplicación básica que use este kit de desarrollo. Es además una composición de los distintos módulos del kit.
\item IGui: Usada para definición de la interfaz gráfica de la aplicación de ventanas, para las opciones básicas de visualización y configuración.
\item IPersistence: Persistencia de la aplicación, encapsula los cambios que se efectúan sobre los datos de la misma.
\item IPercept: Engloba la interfaz de usuario mediante percepción.
\item Iprod: Es el módulo referido a la Producción, generación de elementos 3D y de audio.
\item ICog: Ofrecerá elementos de Inteligencia Artificial.
\end{itemize}
\item VOX: Implementación de una aplicación que usa este kit de desarrollo como propuesta de una Instalación.
\item Base de Datos
\end{itemize}


\vspace{50 mm}

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/DiagramadeclasesUML.png}
\end{center}
\caption{ \label{F_DiagramadeClasesUML} Diagrama de Clases UML.}
\end{figure}


\chapter{Etapa 2: Aplicación principal, GUI, Percepción y Producción}

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/blocks_full_etapa2.jpg}\label{F_DiagramaCompleto}
\end{center}
\caption{ \label{F_BloquesResumen_Etapa2} Componentes sobre los que se va a trabajar.}
\end{figure}

Para el desarrollo de la primera demo se pretende conseguir una aplicación ejecutable que integre el modelo de interfaz y la separación de módulos definida en la etapa anterior, centrándose en implementar las funcionalidades de un subconjunto de módulos básicos. 
\\

Como puede verse en la Figura \ref{F_BloquesResumen_Etapa2}, los componentes a abordar (resaltados en color) son aquellos que aportan la capacidades mínimas para que ésta se ejecute y provea de las herramientas que permitan, posteriormente, analizar y generar contenido. Esto es, nos centraremos en capturar y mostrar la información que se manejará en la aplicación. A la izquierda, en azul, se resalta el componente de interfaces 'core'. A la derecha, con el marco resaltado, vemos a 'vox' (Virtual Operating Experience) que será nuestra propuesta de instalación y representa a la aplicación. Entre ambos extremos se representan en color los módulos igui, ipercept e iprod, y en gris a ipersistence e icog, todas implementaciones de sus correspondientes interfaces definidas en 'core', con la tecnología elegida. De esta forma, tendremos una aplicación, que se compone de varios módulos, que implementan una interfaz.
\\

Durante esta etapa, la intención es únicamente crear la estructura básica de la aplicación, así como establecer una relación de las dependencias entre los módulos, cargando en cada caso correctamente las librerías de las que dependen. 
\\

En este punto no se tratan la gestión del usuario ni de sus datos. No existirá como un objeto dentro de la aplicación aún. Sin embargo, puede verse como un actor en el diagrama de Casos de Uso que se muestra en la Figura \ref{F_CasosdeUsoPrimeraDemo}, dado que participa como actor en dichos procesos. Tampoco se abordarán aún la Persistencia ni, por tanto, la Base de Datos.

\section{Análisis}

\subsection{Análisis de Requisitos de Usuario}
Como se ha comentado, estudiaremos un subconjunto básico de los casos de uso para esta primera demo ejecutable. En ella, se abordarán las funcionalidades básicas de gestión de aplicación y de interfaces gráficas, de forma que se obtenga un ejecutable capaz de mostrar información en distintas ventanas. Además se cargarán los módulos de Percepción y de Pproducción. Se mostrarán las imágenes capturadas por las cámaras y un entorno 3D por defecto, con sonido posicional.

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/casosdeuso_primerademo.png}
\end{center}
\caption{ \label{F_CasosdeUsoPrimeraDemo} Modelo de Casos de Uso.}
\end{figure}

\subsection{Selección de Herramientas}

A partir del estudio de las herramientas disponibles se han seleccionado las más adecuadas, tanto por sus características como por compatibilidades técnicas en el conjunto del proyecto.

\begin{itemize}
\item GUI: wxWidgets
\item Motor gráfico: Panda3D
\item Gestión de Hilos: Boost
\item Captura y Audio Posicional: SFML
\item Captura de Vídeo y Visión por Computador: OpenCV
\end{itemize}

\section{Diseño}

\subsection{Diseño de la Aplicación}
\subsubsection{Breve Descripción de los Módulos}
\begin{itemize}
\item core:
\begin{itemize}
\item IGui: Interfaz básica para la creación del módulo y registro de ventanas.
\begin{itemize}
\item IGuiWindow: Creación, Mostrar/Ocultar.
\end{itemize}
\item IPercept: Interfaz básica para la creación e inicialización del módulo de Percepción.
\item IPersistence: Interfaz básica para la creación del módulo de Persistencia.
\item IProd: Interfaz básica para la creación e inicialización del módulo de Producción.
\item ICog: Interfaz básica para la creación del módulo de Inteligencia Artificial.
\item IApplication: Interfaz básica para la creación de la aplicación.
\end{itemize}
\item igui:
\begin{itemize}
\item Crear ventana de aplicación.
\item Operación básicas de ventana: mostrar, mover, cerrar, cambiar contenido.
\item Menú de Aplicación: Archivo, Vista, Herramientas, Ayuda.
\end{itemize}
\item ipercept: 
\begin{itemize}
\item Capacidad para capturar imágenes de n-cámaras.
\item Capturar imágenes de las n-cámaras.
\item Mostrar imágenes capturadas.
\item Capturar audio.
\end{itemize}
\item iprod:
\begin{itemize}
\item Cargar una escena 3D por defecto.
\item Introducir y reproducir audio posicional 3D.
\item Capacidad para mostrar n-ventanas de render de la misma escena 3D, con vistas independientes.
\end{itemize}
\item vox: Carga y ejecución de los distintos módulos.
\end{itemize}

\subsubsection{Diseño General en UML}

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/DiagramadeclasesUML_PrimeraDemo.png}
\end{center}
\caption{ \label{F_DiagramadeClasesUML_PrimeraDemo} Diagrama de Clases UML. Resumen}
\end{figure}

En la Figura \ref{F_DiagramadeClasesUML_PrimeraDemo} puede verse un esquema general en UML del proyecto completo. Es una visión simplificada pero que refleja la arquitectura del sistema.
\\

Se distinguen dos grandes bloques: En azul se encuentra el paquete core, que se compone de las interfaces separadas en módulos; en naranja la implementación de los distintos módulos y de la aplicación en sí. Cabe destacar que se persigue intencionadamente una alta modularidad y que en última instancia parte de estos módulos se ejecutarán en hilos independientes, explotando el reparto de la carga entre todos los núcleos físicos disponibles del sistema. 
\\

Debe tenerse en cuenta que los módulos implementados sólo podrán comunicarse entre ellos y con la aplicación a través de dicha interfaz. De esta forma se abstraen los detalles de la construcción de cada componente y se asegura su independencia, de forma que cualquier módulo pueda ser reemplazado o reimplementado de forma independiente sin que afecte al resto del proyecto. También permitiría la construcción de forma flexible de distintas aplicaciones, utilizando sólo los módulos necesarios o aprovechando módulos de otras ya creadas.

\subsubsection{Diseño en UML - Núcleo de la Interfaz}

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/DiagramadeclasesUML_PrimeraDemo_core.png}
\end{center}
\caption{ \label{F_DiagramadeClasesUML_PrimeraDemo_core} Diagrama de Clases UML. Detalle - core}
\end{figure}

Como puede verse en la Figura \ref{F_DiagramadeClasesUML_PrimeraDemo_core}, la interfaz provee de unas herramientas básicas para la creación de cada módulo y la comunicación entre ellos. Desde la aplicación, y entre los módulos, toda interacción se hará a través de esta interfaz, aislando completamente cada componente de los detalles de implementación de los otros. No es intención del kit de desarrollo definir cómo se va usar, sino ofrecer una colección de herramientas. Por ello, también se procura reducir la intencionalidad e interdependencia entre ellos, en la medida de lo posible. A modo de ejemplo, el módulo de Producción será completamente independiente del de Percepción y nunca hará uso de éste. Sólo ofrecerán una colección de herramientas, y será la aplicación que se desarrolle la que decida si quiere usarlos y cuál será su relación.
\\

En cualquier caso, en la fase actual no existe una gran funcionalidad. Sin embargo, a medida que avance el proyecto se irán añadiendo nuevas capacidades que permitan, por ejemplo, el paso de datos, la generación de contenido o la gestión de objetos.
\\

En esta etapa, el trabajo se concentra en los módulos de interfaz gráfica de usuario, percepción y producción. Con ellos se podrá disponer de un entorno con las capacidades básicas para la captura de los datos necesarios y la visualización de los contenidos a generar.

\subsubsection{Diseño en UML - Módulo de GUI}

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/DiagramadeclasesUML_PrimeraDemo_igui.png}
\end{center}
\caption{ \label{F_DiagramadeClasesUML_PrimeraDemo_igui} Diagrama de Clases UML. Detalle - igui}
\end{figure}

Como puede verse en la Figura \ref{F_DiagramadeClasesUML_PrimeraDemo_igui}, este módulo controla gestión de la interfaz gráfica de usuario de tipo ventana de la aplicación. La interfaz principal se define en la clase MainFrame, desde la que se gestionará la aplicación con los menús y paneles principales. Por otro lado, hay que tener en cuenta que la aplicación está destinada a mostrar multitud de ventanas independientes, que podrían mostrarse en distintas pantallas o proyectores. Un modelo de interfaz SDI (Single Document Interface) nos aportará la capacidad de tener múltiples ventanas flotantes, libres de una ventana principal.
\\
 
Hay que tener en cuenta que varias de las librerías a usar aportan mecanismos propios para la visualización de ventanas. Considerando también que se tratan de capacidades estándares de estos tipos de librerías, la simplicidad del uso de estos mecanismos y que las distintas ventanas visualizarían distintos tipos de datos, que pertenecen a distintos módulos, que se ejecutan en distintos hilos, se plantea definir, en este módulo, la clase IGUIWindow a modo de wrapper que englobe las funcionalidades que se requieren de los mismos, y que permita su gestión centralizada desde el controlador de interfaz principal.
\\

\subsubsection{Diseño en UML - Módulo de Percepción}

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/DiagramadeclasesUML_PrimeraDemo_ipercept.png}
\end{center}
\caption{ \label{F_DiagramadeClasesUML_PrimeraDemo_ipercept} Diagrama de Clases UML. Detalle - ipercept}
\end{figure}

El módulo de Percepción está destinado a la captura y análisis de los datos de entrada. En el sistema que se plantea estos datos llegan a través de distintas cámaras web y de un micrófono. De esta forma, la Figura \ref{F_DiagramadeClasesUML_PrimeraDemo_ipercept} representa los objetos que serían necesarios estos procesos. Por el momento, en esta primera demo sólo se procede a la captura de datos, dejando el análisis para fases posteriores. 
\\

Debido a la distinta naturaleza de entrada y procesado para vídeo y audio, MainPercept lanza dos módulos que se ejecutan independientemente, cada uno en un hilo: PerceptAudio captura sonido y lo almacena en un buffer, y PerceptVideo captura imágenes. Este último está preparado para gestionar un número configurable de cámaras web y mostrar sus capturas en sus respectivas ventanas. 
\\

Hay que tener en cuenta que se crean las ventanas para visualizar las imágenes utilizando las herramientas de la propia librería. Esto se hace por simplicidad. Sin embargo, debe notarse que se utiliza el wrapper de ventanas IGuiWindow para encapsular las ventanas creadas, y de esta forma, permitir su manipulación desde el módulo de interfaz gráfica de usuario.

\subsubsection{Diseño en UML - Módulo de Producción}

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/DiagramadeclasesUML_PrimeraDemo_iprod.png}
\end{center}
\caption{ \label{F_DiagramadeClasesUML_PrimeraDemo_iprod} Diagrama de Clases UML. Detalle - iprod}
\end{figure}

MainProd es el módulo principal de Producción y también se ejecuta en un hilo independiente. Está destinado no sólo a la visualización del entorno sino a la generación de contenido, tanto gráficos como de audio. En el alcance de esta etapa, el módulo sólo mostrará una escena por defecto y reproducirá un audio con escucha posicional. La Figura \ref{F_DiagramadeClasesUML_PrimeraDemo_iprod} muestra esta primera aproximación.
\\

Aunque el módulo se ejecuta en un hilo independiente está preparado para que otros hilos intervengan mediante llamadas a unos métodos determinados. Además, de igual forma que lo hace el módulo de percepción, MainProd esta preparado para manejar un número configurable de ventanas, cuya intención es ser visualizadas en distintas pantallas o proyectores. Estas ventanas también son creadas mediante las capacidades propias de la librería y utilizan IGuiWindow como envoltorio para su manipulación centralizada.
\\

Por otro lado, se definen en el espacio las fuentes de sonido y las propiedades del observador-oyente para conseguir disponer de sonido posicional en tres dimensiones. 

\section{Implementación}
Se ha escogido como entorno de desarrollo Visual Studio 2008 Express Edition. Esto es debido a que se trata del entorno de desarrollo con que el se tiene mayor experiencia, así como por la comodidad de las herramientas de las que dispone. Se ha escogido la versión Express por ser gratuita y permitir el desarrollo de cualquier tipo de proyectos, tanto comerciales como no comerciales.
\\

El proyecto consiste en una solución o colección de proyectos. Estos proyectos son:
\begin{itemize}
\item core: Núcleo de la interfaz.
\item igui: Proyecto para el Módulo de GUI.
\item ipercept: Proyecto para el Módulo de Percepción.
\item ipersistence: Proyecto para el Módulo de Persistencia.
\item iprod: Proyecto para el Módulo de Producción.
\item icog: Proyecto para el Módulo de Inteligencia Artificial.
\item vox: Proyecto para la Aplicación.
\end{itemize}

El núcleo de la interfaz 'core' se implementa como una librería estática y es una dependencia necesaria para todos los proyectos. Por simplicidad los proyectos icog, igui, ipercept, ipersistence e iprod, que correspondería cada uno con un módulo, son también librerías estáticas de las que depende la aplicación ejecutable 'vox'.
\\

Hay que tener en cuenta que el IDE elegido no es multiplataforma, por lo que sólo puede ser utilizado bajo el sistema operativo Windows. Sin embargo, todas las librerías utilizadas sí lo son, y el código es C++. De esta forma, se ha procurado seguir un línea de trabajo que facilite la portabilidad futura. Se propone como alternativa el IDE open source Eclipse o la configuración de una distribución preparada con CMAKE, de forma que se puedan generar proyectos para otros entornos. Sin embargo, esto quedará como trabajo futuro.
\\

Finalmente, comentar que se decide seguir una estructura de ficheros que se corresponda directamente con la jerarquía de los proyectos. Además se generarán los resultados en un directorio común llamado $\backslash$bin y las librerías externas a usar se almacenarán dentro del subdirectorio correspondiente dentro de la carpeta $\backslash$extern. El código de los módulos implementados puede encontrarse en $\backslash$src, y el de la aplicación el $\backslash$app.
\\

Llegados a este punto se procede a revisar la implementación de cada apartado:

\subsubsection{Núcleo de la Interfaz}
$\backslash$src$\backslash$core
\\

Contiene las cabeceras que definen las clases virtuales puras, de las cuales heredarán las clases de los módulos que se implementen. Esto es así para asegurar la abstracción e independencia entre los detalles de implementación de cada módulo.
\\

%Como se trata de una librería dinámica hay que tener en cuenta la necesidad del uso de una macro de exportación. 
A continuación puede verse un ejemplo de su uso con la interfaz IPercept:

\begin{lstlisting}[language=C++]
namespace core{
class IPercept 
{ public:
   virtual ~IPercept(){}
   virtual void Delete()=0;
   virtual void Init()=0;
};}
\end{lstlisting}

Este ejemplo es válido para el resto de las interfaces, que incluyen en cada caso los métodos virtuales necesarios. Estos métodos implementados se corresponden directamente con los ilustrados en la figura \ref{F_DiagramadeClasesUML_PrimeraDemo_core}, página \pageref{F_DiagramadeClasesUML_PrimeraDemo_core}. 

%La macro de exportación utilizada es \_COREEXPORT\_ y se muestra en el siguiente cuadro:
%\begin{lstlisting}[language=C++]
%#ifndef _COREEXPORT_
%#define _COREEXPORT_
%
%#if defined(_MSC_VER) || defined(__CYGWIN__) || defined(__MINGW32__) || defined( __BCPLUSPLUS__)  || defined( __MWERKS__)
%    #  if defined( CORE_EXPORTS )
%    #    define COREEXPORT __declspec(dllexport)
%    #  else
%    #    define COREEXPORT __declspec(dllimport)
%    #  endif
%#else
%    #  define COREEXPORT
%#endif
%
%#ifdef _DEBUG
%	#define _CRTDBG_MAP_ALLOC
%	#define _CRTDBG_MAP_ALLOC_NEW
%	#include <stdlib.h>
%	#include <crtdbg.h>
%#endif
%#endif
%\end{lstlisting}


\subsubsection{Módulo de GUI}
$\backslash$src$\backslash$igui
\\

La clase principal del Módulo de GUI y que implementa la interfaz IGui se llama MainGui. Esta clase es la que se encarga de toda la gestión de la interfaz de ventanas del proyecto y la librería escogida para su implementación es wxWidgets. 
\\

Las instrucciones para descargar y compilar wxWidgets pueden encontrarse en la web oficial \cite{wx09}. Los paquetes descargados vienen preparados para VS2008 y compilan sin necesidad de ningún ajuste. Sólo hay que incluir las cabeceras y las librerías necesarias, que pueden verse en el Apéndice, sección \ref{detallesImp:ModuloGUI}, en la página \pageref{detallesImp:ModuloGUI}. 
\\

Hay que tener en cuenta que una de las particularidades de wxWidgets es que necesita tomar control de la aplicación principal, mediante la macro IMPLEMENT\_APP. Este detalle puede verse en profundidad en la sección \ref{sec:AplicacionPrincipal}, página \pageref{sec:AplicacionPrincipal}. De todas formas, eso no afecta a la construcción y gestión de la interfaz, que se realizará completamente en este módulo.
\\
 
Por otro lado, hay que tener en cuenta que distintos módulos tienen la capacidad de crear ventanas para mostrar información y que se desea usar esos mecanismos. Para ello, se utiliza la interfaz IGuiWindow para encapsular la creación y la gestión, y se define MainGui siguiendo el patrón de diseño Singleton para poder acceder a la misma instancia desde cualquier módulo que lo necesite. Para asegurar que el acceso a la instancia sea seguro se utilizan los cerrojos como mecanismo de sincronización, implementados con boost.

\begin{itemize}
\item MainGui: Crea y mantiene los elementos de la interfaz. Se compone de un marco principal de aplicación llamado MainFrame y diversos paneles. Así mismo, también mantiene una lista de las ventanas independientes que se hayan registrado. Se crea la macro IMPLEMENT\_MIIAPP para encapsular la macro de wxWidgets IMPLEMENT\_APP. Un resumen de la clase puede verse en el siguiente cuadro:
\begin{lstlisting}[language=C++]
#define IMPLEMENT_MIIAPP(name) IMPLEMENT_APP(name)
namespace core {namespace igui{
class MainGui : public core::IGui
{ public:    ...
  private:		
    MainGui(const std::string &title = "");
    static MainGui* instance; 
    static MainFrame *main_frame;
    static std::map<IGuiWindow*, int> registered_windows;
    static std::map<IGuiWindow*, int> fullscreenable_windows;
};}}

MainGui* MainGui::GetInstance(const std::string &title)
{ boost::mutex::scoped_lock lock(m_mutex);
  if (instance == NULL) 
   instance = new MainGui(title);
  return instance; }
\end{lstlisting}

\item MainFrame: Crea el marco principal de la aplicación. Contiene el menú y los paneles principales. En esta fase los únicos paneles que existen por el momento son el de Inicio y el de Ayuda. Se ha implementado un mecanismo para la sustitución de contenidos de ventanas, de forma que la ventana principal de la aplicación asumirá el contenido de los paneles que se usen en cada momento de forma dinámica.

\begin{lstlisting}[language=C++]
namespace core { namespace igui	{
class MainFrame : public wxFrame
{ public:    ...
 private:
    wxMenu *file_menu, *view_menu, *tools_menu, *help_menu;
    wxMenuItem *item_file_close, *item_view_fullscreen, *item_view_start, *item_tools_configure, *item_help_about;
    ...
    DECLARE_EVENT_TABLE()
    void OnClose(wxCommandEvent& WXUNUSED(event));
    void OnViewStart(wxCommandEvent& WXUNUSED(event));
    void OnHelpAbout(wxCommandEvent& WXUNUSED(event));
    void DismissPanels();};}}
\end{lstlisting}

\item GUIStart: Es el panel principal de la aplicación que contendrá un acceso rápido a las funciones principales de la aplicación mediante los botones: Login, Inicio y Configuración.
\\

El aspecto más relevante de esta clase son los botones  y la necesidad de capturar el evento de render EVT\_PAINT para tener un acceso directo a cómo se dibuja el contenido de la misma. Esto es necesario para conseguir acabados más interesantes; por ejemplo, dibujar una imagen de fondo o texto con fondo transparente sobre imágenes. Debe tomarse el DC (contexto del dispositivo) con el que se va a dibujar, en este caso la pantalla, y usar las operaciones de dibujo directamente.

\begin{lstlisting}[language=C++]
namespace core { namespace igui { 
class GUIStart : public wxPanel 
{ public:   ...
   void OnPaint(wxPaintEvent &evt);
   void paintNow();	        
   void render(wxDC& dc);
 private:
   wxBitmap background_image;
   wxButton *login_button, *start_button, *configure_button;
};}}

BEGIN_EVENT_TABLE(GUIStart, wxPanel)
   EVT_PAINT (GUIStart::OnPaint)
END_EVENT_TABLE()

GUIStart::GUIStart(...):wxPanel(parent, id, pos, size, style, name)
{ background_image = wxBitmap(...);
  login_button = new wxButton(...); } 
  
void GUIStart::OnPaint(wxPaintEvent & evt)
{ wxPaintDC dc(this);
  render(dc); }
  
void GUIStart::render(wxDC& dc)
{ dc.DrawBitmap(background_image, 0, -20, false ); }
\end{lstlisting}

\item GUIHelp: Panel de información de la aplicación donde puede verse la versión y donde añadirá un enlace a la web del proyecto. Los detalles de implementación son muy similares al panel GUIStart.
\end{itemize}

Para ver más detalles sobre la configuración de la librería, dependencias necesarias y la implementación del módulo se puede consultar la sección \ref{detallesImp:ModuloGUI}, en la página \pageref{detallesImp:ModuloGUI}.

\subsubsection{Módulo de Percepción}
$\backslash$src$\backslash$ipercept
\\

La clase principal que implementa la interfaz IPercept y que está destinada a gestionar el módulo se llama MainPercept. Las tareas que se resuelven aquí son la captura de información del espacio de la instalación y de su interpretación. La entrada de datos del sistema consiste en audio mediante micrófonos, y de vídeo, mediante webcams. En esta fase las tareas iniciales se centran en capturar audio y almacenarlo en un buffer, y capturar imágenes de las cámaras para mostrarlas. Más adelante, se usará esta información para calibrar los puntos de vista o localizar al usuario dentro del espacio de la instalación para posicionarlo dentro del entorno virtual, entre otras funcionalidades.
\\

Puede apreciarse que la captura de información y su procesado son de naturaleza distinta y, de hecho, independiente; no sólo del resto de la aplicación, sino entre sí. Además hay que tener en cuenta que los accesos a los periféricos son tareas costosas en tiempo. Por ello, la mejor opción es mantener estas tareas de forma independiente, ejecutándose en hilos separados. Cuando el sistema necesite alguna información, solicitará al módulo la más reciente, pero no será necesaria la espera para que éste termine de acceder a los dispositivos o de realizar sus tareas.
\\

Las librerías utilizadas son SFML para la captura de audio, y OpenCV para la captura de vídeo y su posterior proceso. Son librerías que han requerido ajustes importantes para su funcionamiento en el proyecto. Pueden verse los detalles en el Apéndice, en las secciónes \ref{detallesImp:SFML} y \ref{detallesImp:OpenCV}, páginas \pageref{detallesImp:SFML} y \pageref{detallesImp:OpenCV}.

\begin{itemize}
\item MainPercept: Implementa los mecanismos necesarios para la gestión global del módulo y de sus componentes, de tal forma que se pueda controlar la entrada de datos y su procesado. Por ahora sólo instancia los módulos necesarios y los inicializa.

\begin{lstlisting}[language=C++]
namespace ipercept { 
class MainPercept : public core::IPercept
{ public:   ...
  private:
   static int num_cam;
   static PerceptAudio* perceptAudio_module;
   static PerceptVideo* perceptVideo_module;
};}}

MainPercept::MainPercept()
{ perceptAudio_module = new PerceptAudio();
  perceptVideo_module = new PerceptVideo(); }

void MainPercept::Init()
{ perceptAudio_module->Init();
  perceptVideo_module->Init(); }
\end{lstlisting}

\item PerceptAudio: Esta clase implementa la interfaz IPerceptAudio y se encargará de ejecutar la captura y procesado de audio. Por ahora sólo captura y almacena el sonido en un buffer. Las librerías usadas son SFML para el acceso a los dispositivos de audio y grabación, y Boost para la gestión de hilos.
\\

Por coherencia se decide seguir un mismo esquema para los módulos de este tipo. Se tratan de clases con métodos y atributos estáticos que internamente lanzan un hilo independiente para la ejecución de su código principal. Todas tienen un método llamado DoInit(), para preparar el arranque del módulo y ejecutar la llamada a DoMainloop(), el bucle principal, en un hilo aparte. Internamente Iterate() recoge el código destinado a ejecutarse para cada iteración, mientras que Capture() realiza la captura de datos. Se hace uso de cerrojos no bloqueantes definidos en contexto para el acceso a ciertos atributos. Cuando la ejecución de un hilo llega a este cerrojo se permite saltar el código bloqueante a la espera de que en la siguiente iteración el cerrojo esté disponible, por lo que el hilo no llega a bloquearse nunca.
\\

\begin{lstlisting}[language=C++]
class PerceptAudio : public core::IPerceptAudio
{ public:    ...
  private:
    static void DoInit();
    static void DoMainLoop();
    static void Iterate();
    static void Capture();
    
    static boost::shared_ptr<boost::thread> m_thread;
    static boost::try_mutex m_mutex;
    static bool initialized, stop_requested;
    
    static sf::SoundBufferRecorder Recorder;
    static sf::SoundBuffer recordingBuffer;};}}
\end{lstlisting}

\item PerceptVideo: Esta clase implementa la interfaz IPerceptVideo y se encarga de ejecutar la captura y procesado de información visual. Por ahora la tarea consiste en capturar imágenes de un número configurable de cámaras web y mostrarlas en ventanas independientes. Como estás tareas son costosas en tiempo es altamente relevante que este módulo se ejecute en un hilo aparte. Las librerías a usar son Boost (\ref{detallesImp:Boost}), para la gestión de hilos, y OpenCV (\ref{detallesImp:OpenCV}), librería de Visión por Computador, para la captura de imágenes y su posterior procesado.
\\

En la construcción del módulo se sigue el mismo esquema que se ha visto en casos anteriores para mantener la coherencia. Así, se dispone de métodos para la inicialización y se separa la ejecución del bucle principal, que es lanzando en un hilo independiente, con cerrojos no bloqueantes. Así mismo, se distingue la ejecución de cada iteración, donde se incluye un método para la captura de datos de entrada. Se dispone de un vector de cámaras que permita la flexibilidad del módulo para recoger datos de un número configurable de fuentes. A continuación se muestran algunos detalles relevantes. 
\begin{lstlisting}[language=C++]
class PerceptVideo : public core::IPerceptVideo
{ public:   ...
  private:
   static void DoInit();
   static void DoMainLoop();
   static void Iterate();
   static void Capture();
   
   static int num_cam;
   static std::map< int, CvCapture* > capture_cam_array;
   static std::map< std::string, CamWindow* > camWindow_array;};}}
\end{lstlisting}

A continuación se muestra un cuadro de código de resumen que ilustra el proceso de inicialización, creación del nuevo hilo de ejecución y el bucle principal del mismo, donde se capturan los datos de entrada. 

\begin{lstlisting}[language=C++]
PerceptVideo::PerceptVideo()
{ capture_cam_array[i] = cvCaptureFromCAM(i);
  camWindow_array[window_name] = new CamWindow(window_name);}}}

void PerceptVideo::DoInit()
{ assert(!m_thread);
  m_thread = boost::shared_ptr<boost::thread>(new boost::thread(boost::function0<void>(&PerceptVideo::DoMainLoop)));}}

void PerceptVideo::DoMainLoop()
{ while(!stop_requested)
  { Iterate();
    m_thread->sleep(system_time()+milliseconds(10));}}

void PerceptVideo::Iterate()
{  boost::try_mutex::scoped_try_lock lock(m_mutex);
   if (lock) {Capture();}}

void PerceptVideo::Capture()
{ for (...)
  { capture_img = cvQueryFrame(iter->second);
    std::map<...>::iterator cam_iter = camWindow_array.find(window_name);
    cam_iter->second->ShowImage(capture_img);}}
\end{lstlisting}

Para más información se puede consultar la sección \ref{detallesImp:ModuloPercepcion}, página \pageref{detallesImp:ModuloPercepcion}.

\item CamWindow: Esta clase es utilizada para encapsular la creación de ventanas mediante las herramientas que provee OpenCV, de forma que se adapten a la interfaz IGUIWindows. Esto permitirá su acceso y gestión desde el módulo de GUI. Se elige usar los mecanismos que ofrece OpenCV para la creación de ventanas para simplificar el uso de las mismas y permitir su actualización a partir de las herramientas propias de la librería, sin necesidad de estar delegando ni transmitiendo información innecesariamente entre módulos. 
\begin{lstlisting}[language=C++]
namespace core { namespace ipercept {	
class CamWindow : public core::IGuiWindow
{ public:   ...
  private:
   std::string window_name;
   bool isShown;
};}}

CamWindow::CamWindow(const std::string &_window_name)
{ cvNamedWindow(window_name),1);
  core::igui::MainGui::GetInstance()->RegisterWindow(((core::IGuiWindow*)this)); }

void CamWindow::ShowImage(const IplImage *image)
{ if (isShown) cvShowImage(window_name, image);}
\end{lstlisting}

\end{itemize}

\subsubsection{Módulo de Producción}
$\backslash$src$\backslash$iprod
\\

Siguiendo el mismo modelo que en los módulos comentados anteriormente, la clase principal que implementa la interfaz IProd recibe el nombre de MainProd. Es a través de esta interfaz como se manejará el contenido a generar para construir y exponer el entorno. En esta primera demo el objetivo es integrar un motor gráfico que sea capaz de visualizar una escena 3D por defecto, con sonido 3D en un sistema envolvente, mostrando una cantidad configurable de ventanas.
\\

De la misma forma que en el resto de los casos, este módulo se ejecuta en un hilo independiente, utilizando los mecanismos de sincronización necesarios para el manejo de hilos gracias a la librería Boost (\ref{detallesImp:Boost}). Por otro lado, el motor gráfico integrado es Panda3D. Sin embargo, es necesario destacar que fueron necesarios ajustes en las opciones de compilación del mismo para que las librerías generadas pudieran funcionar correctamente dentro del proyecto. También impone algunas limitaciones como una política restrictiva en el nombrado y jerarquía de directorios, así como algunas modificaciones en los fuentes poco comunes pero que fueron necesarias realizar y aceptadas en la revisión oficial. Para más detalles, consultar \ref{detallesImp:Panda3D}, página \pageref{detallesImp:Panda3D}. 
\\

Finalmente para la localización de las fuentes de sonido y del oyente en el espacio 3D, se ha hecho uso de la librería SFML. De nuevo, es necesario destacar la necesidad de realizar modificaciones en el código fuente de la librería para que ésta funcione correctamente. Esto es debido que existen errores en el diseño de la API que impide que pueda situarse libremente al oyente en el espacio 3D. Para ver más detalles sobre la integración de esta librería y las modificaciones necesarias se puede consultar la sección \ref{detallesImp:SFML}, página \pageref{detallesImp:SFML}.

\begin{itemize}
\item MainProd: Implementa la interfaz IProd y provee los mecanismos para cargar una escena 3D y mostrarla en una cantidad de ventanas configurable. También carga y localiza en el espacio sonidos y el oyente.

\begin{lstlisting}[language=C++]
namespace core { namespace iprod {
class MainProd : public core::IProd
{ public: ...
  private:
   static void CreateDefaultWindows(int numWindows);
   static void LoadDefaultScene();

   static PandaFramework framework;
   static std::map<int, WindowFramework*> pandawindows_array;
   static std::map<int, NodePath>         windowcamera_array;
   static NodePath cam_viewpoint, origin, up;
   static double listener_position[],listener_target[],sound_pos[];
   static sf::Sound Sound;
};}}
\end{lstlisting}

Las secciones más relevantes son la carga de la escena y el render de las distintas vistas. Por otro lado, se tienen en cuenta políticas de rendimiento y se trata que la escena sea ligera. Por ejemplo, los objetos se cargan una sola vez para el marco principal y se instancian como referencias en los grafos de escena del resto de vistas, sin necesidad de cargar varias veces el mismo modelo.
\\

Para poder realizar pruebas para comprobar el buen funcionamiento y para preparar trabajo futuro, se añade un mecanismo para el acceso externo a la escena, donde realizar operaciones DoStuff(). En este caso, rotar la cámara. Así mismo, se habilita la navegación libre en la segunda ventana de visualización mediante el uso del ratón (para comprobar que el correcto funcionamiento de la localización 3D tanto de oyente como de sonidos posicionados). 
\\

Gran parte del código se asemeja a los módulos explicados anteriormente, siguiendo por coherencia la misma política de incialización, ejecución del hilo independiente, y bloques de código a ejecutar en cada iteración. por lo que sólo se detallan. A continuación se muestran las secciones de código más relevantes:
\begin{lstlisting}[language=C++]
void MainProd::DoMainLoop()
{ framework.open_framework(m_argc,m_argv);
  CreateDefaultWindows(DEFAULT_NUM_WINDOWS);
  LoadDefaultScene();
  sf::SoundBuffer Buffer;
  Sound.SetBuffer(Buffer);
  
  while(!stop_requested) 
  { Iterate();
    m_thread->sleep(get_system_time()+milliseconds(10)); }

  framework.close_all_windows();
  framework.close_framework();}
\end{lstlisting}

Algo a destacar es la función Iterate(). Por lo general, los motores gráficos y de juego toman posesión del bucle principal de la aplicación. Este también es el caso del motor de juego elegido, Panda3D. Esta peculiaridad limita el número de actividad y la facilidad con la que nuevas tareas pueden ser añadidas, o no sólo añadidas, sino generadas mientras se ejecuta la aplicación. Sin embargo, la flexibilidad del motor escogido permite abstraerse de las secciones de inicialización y del bucle principal, de forma que se pueden ejecutar las tareas de cada iteración mediante el método step(). Gracias a este mecanismo podemos disponer de la ejecución del bucle principal en un hilo aparte, mientras se ejecutan o generan tareas y objetos dinámicamente y de forma sincronizada, desde el mismo u otros hilos.

\begin{lstlisting}[language=C++]
void MainProd::Iterate()
{ boost::try_mutex::scoped_try_lock lock(m_mutex);
  if (lock)
  { framework.do_frame(graphic_thread);
    CIntervalManager::get_global_ptr()->step();}}

void MainProd::DoInit()
{ if (!initialized)
	{ assert(!m_thread);
      m_thread = boost::shared_ptr<boost::thread>(new boost::thread(boost::function0<void>(&MainProd::DoMainLoop)));}}
\end{lstlisting}
Como se comentaba, el siguiente fragmento de código muestra la forma en que, desde la ejecución de otro hilo puede solicitarse a la clase realizar determinadas tareas. En este caso, se rota la cámara de la primera vista para que gire en torno al centro de la escena, a la vez que se actualiza la posición del oyente en el espacio 3D según la posición de la cámara, con navegación mediante el movimiento del ratón, de la vista2. 
\\

Es necesario tener en cuenta dos puntos en relación al sonido 3D en la aplicación para un sistema envolvente. Por un lado los sistemas de referencia espacial del motor gráfico y del motor de audio no coinciden, por lo que es necesario realizar las transformaciones pertinentes. Por otro lado, la librería de audio utilizada comete errores importantes que deben ser solucionados para poder disponer de un posicionamiento y orientación correctos de las fuentes de sonido y del oyente. Estos detallen pueden verse con mayor profundidad en la sección \ref{detallesImp:SFML}, página \pageref{detallesImp:SFML}.

\begin{lstlisting}[language=C++]
void MainProd::DoDoStuff()
{ boost::try_mutex::scoped_try_lock lock(m_mutex);
  if (lock && initialized)
  { //rotar la cámara en vista 1
    windowcamera_array[1].set_pos(20*sin(angleradians),-20.0*cos(angleradians),3);
    windowcamera_array[1].set_hpr(angledegrees, 0, 0);
    //actualizar oyente según navegación en vista 2
    sf::Listener::SetPosition(new_pos.get_x(), new_pos.get_y(), new_pos.get_z());
    sf::Listener::SetTarget(new_at.get_x(), new_at.get_y(), new_at.get_z() , new_up.get_x(), new_up.get_y(), new_up.get_z());
   }
   else 
   { //no se pudo coger el cerrojo, pero no se bloquea  }
}
\end{lstlisting}
Finalmente se muestra fragmentos que ilustran la carga de la escena y cómo se enlazan a los distintos renderers. Como se comentaba inicialmente, y como es lógico, se ha tenido en cuenta cuestiones de eficiencia para visualización en múltiples vistas. Como en el problema al que nos enfrentamos la escena a visualizar es la misma para todas las vistas, se sigue la política de cargar una única vez los elementos en la escena, ligarlos al render principal e instanciarlos para el resto, de forma que sólo se cargan una vez y no existen copias del mismo objeto en los distintos grafos de la escena.
\begin{lstlisting}[language=C++]
void MainProd::LoadDefaultScene()
{ if (pandawindows_array.begin() != pandawindows_array.end())
  { NodePath environment = pandawindows_array[1]->load_model(framework.get_models(),"environment");
    environment.reparent_to(pandawindows_array[1]->get_render());
    NodePath pandaActor = pandawindows_array[1]->load_model(framework.get_models(), "panda-model");
    pandaActor.reparent_to(pandawindows_array[1]->get_render());
    pandawindows_array[1]->load_model(pandaActor, "panda-walk4");
    pandawindows_array[1]->loop_animations(0);

    std::map<...>::iterator iter = pandawindows_array.begin(); iter++;
    while(iter != pandawindows_array.end())
    { environment.instance_to(iter->second->get_render());
      iter->second->setup_trackball();
      iter++;}}}
\end{lstlisting}

\item Prod3DWindow: Esta clase es utilizada para encapsular la creación de ventanas mediante las herramientas que provee Panda3D, de forma que se adapten a la interfaz IGUIWindows. Esto permitiría su acceso y gestión desde el módulo de GUI. De la misma forma que sucede en el módulo de Percepción para la visualización de la entrada de datos en distintas ventanas, se utiliza por simplicidad las herramientas propias de la librería para la creación de las mismas y la actualización de su contenido.
\begin{lstlisting}[language=C++]
namespace core { namespace iprod {
class Prod3DWindow : public core::IGuiWindow
{ public:
   WindowFramework *GetWindowFrameWork() {return m_windowFramework;}
  private:
   bool isShown;
   WindowFramework *m_windowFramework;};}}
\end{lstlisting}
\end{itemize}

\subsubsection{Aplicación Principal}\label{sec:AplicacionPrincipal}
$\backslash$apps$\backslash$vox
\\

Finalmente se muestra la sección correspondiente a la aplicación principal. Su objetivo es enmarcar los múltiples módulos, gestionar la ejecución de la misma y de sus componentes.
\\

Algo a tener en cuenta es que al usar wxWidgets como librería para crear la interfaz de usuario de la aplicación, se impone un requisito incómodo: wxWidgets necesita tomar control de la aplicación principal. Esto impide la completa independencia entre la aplicación y la librería usada para la GUI. Sin embargo, hay que tener en cuenta que la Aplicación principal no hará nada: esta sección está únicamente destinada la carga de los distinto módulos. Por ello, su complejidad y contenido son mínimos. Es por ello que se decide proseguir.
\\
En el caso de desear cambiar el módulo de GUI, son dos los cambios necesarios en la aplicación: eliminar la herencia de la clase wxApp, y sustituir la macro IMPLEMENT\_MIIAPP(Application) por un cuerpo main() donde se cree una instancia de la clase Application.
\\

A continuación se muestran algunos detalles relevantes:

\begin{itemize}
\item Application: Implementa la interfaz IApplication que se usa para la creación de la aplicación. Su cometido es cargar los distintos módulos y proveer herramientas para su gestión. En caso de no desear usar wxWidgets como librería para la GUI, es necesario eliminar la herencia de la clase a wxApp.
\\

También hay que tener en cuenta que, en Windows, por motivos de incompatibilidades entre librerías, es necesario incluir la cabecera winsock2.h al inicio del fichero. Para más detalles se puede consultar la sección \ref{detallesImp:Incompatibilidades}, página \pageref{detallesImp:Incompatibilidades}

\begin{lstlisting}[language=C++]
#ifdef _WINDOWS
#include <winsock2.h>
#endif
class Application : public wxApp, public core::IApplication
{ public:    ...
  private:
   core::IGui		*app_maingui;
   core::IPercept	*app_mainpercept;
   core::IProd		*app_mainprod;};

bool Application::OnInit()
{ app_maingui = MainGui::GetInstance("VOX");
  app_mainpercept=(core::IPercept*)new core::ipercept::MainPercept(); 
  app_mainprod=(core::IProd*)new core::iprod::MainProd(argc, argv); 
  app_mainpercept->Init();
  app_mainprod->Init();
  return true; }
\end{lstlisting}

\item Main: Fuente en el que se define el punto de entrada de la aplicación. En caso de no desear usar wxWidgets es necesario sustituir la línea IMPLEMENT\_MIIAPP(Application) por la definición de una función main(), donde crear una instancia de la clase Application.

\begin{lstlisting}[language=C++]
#define _WINSOCKAPI_
#include "Application.h"
IMPLEMENT_MIIAPP(Application)
\end{lstlisting}
\end{itemize}


\section{Validación y Publicidad}
\subsection{Validación}
Se sigue una filosofía de pruebas continuas para resolver en el momento en que aparecen los incidentes que puedan surgir al hacer cambios. Así mismo, cuando se añade una nueva funcionalidad, se usan mecanismos para comprobar que las capacidades incorporadas funcionan correctamente y se mantiene un buen rendimiento.

\begin{itemize}
\item Comprobación de uso de recursos de la máquina mediante las herramientas del sistema. La máquina en la que se desarrolla muestra un consumo del 5\% de CPU y 75Mb de memoria mantenidos. Hay que destacar la posibilidad de que la aplicación consuma mayor CPU en máquinas con menor cantidad de procesadores debido a la sobrecarga por la gestión de hilos. Sin embargo, la tendencia actual se mueve hacia el aumento de número de procesadores.
\item Comprobación de la ejecución y cierre correctos, sin salidas de la aplicación inesperadas ni pérdidas de memoria.
\item Uso de herramientas para medición de fotogramas por segundo para comprobar el rendimiento de la ventana de render. Se comprueba que se mantiene siempre a 60fps para dos ventanas, y 30fps con hasta 10 ventanas de render.
\item Uso de distintas opciones de configuración para comprobar las variantes del uso de los módulos, por ahora mediante macros: Abrir distinto número de ventanas de render (entre 1 y 10) y captura desde distintas cantidades de cámaras web (entre 1 y 2).
\end{itemize}

Las características de la máquina de referencia son las siguientes:

\begin{itemize}
\item Procesador Intel Core i7 CPU 870 2.92Ghz (4 núcleos reales, 8 núcleos lógicos).
\item 6Gb de Memoria Principal.
\item Sistema Operativo Windows 7 64 bits
\item Tarjeta gráfica GeForce GTS 240
\end{itemize}

Finalmente, en las Figuras \ref{F_CapturaFase2PrimeraDemo_Fig1} y \ref{F_CapturaFase2PrimeraDemo_Fig2} se muestran capturas de ejemplo del estado actual:

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/screenshot_20100214_p.jpg}
\end{center}
\caption{ \label{F_CapturaFase2PrimeraDemo_Fig1} Captura 1 - Fase2: Primera Demo. La aplicación mostrando una implementación básica de los módulos principales; es capaz de capturar de múltiples cámaras y mostrar múltiples ventanas de visualización}
\end{figure}
\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/6windows_30fps_release_01_p.jpg}
\end{center}
\caption{ \label{F_CapturaFase2PrimeraDemo_Fig2} Captura 2 - Fase2: Primera Demo. Un ejemplo con múltiples ventanas de visualización.}
\end{figure}

\subsection{Publicidad}
Aunque el desarrollo del proyecto se encuentra aún en sus inicios se considera interesante tener una visión del panorama actual y valorar distintas opciones de publicidad e incluso posibles destinos para la exposición de la instalación. Además, se considera la creación de una presencia web para la iniciativa, así como para alojar o referenciar el acceso a los fuentes del proyecto. 
\\

Se valora establecer contacto con las siguientes instituciones:
\begin{itemize}
\item ULPGC.
\item CAAM: Centro Atlántico de Arte Moderno.
\item Gran Canaria Espacio Digital.
\item Museo Elder de la Ciencia y la Tecnología.
\item La Casa Encendida.
\end{itemize}
Posibles festivales:
\begin{itemize}
\item Artfutura
\item Arco
\item Estampa
\item Sónar
\end{itemize}

\chapter{Etapa 3: Usuario, Entorno, Configuración y Persistencia}

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/blocks_full_etapa3.jpg}\label{F_DiagramaCompleto}
\end{center}
\caption{ \label{F_BloquesResumen_Etapa3} Componentes sobre los que se va a trabajar.}
\end{figure}

Partiendo de la primera demo estable, se desea añadir añadir nuevas funcionalidades para completar la aplicación. En este punto se dispone de la estructura básica de la aplicación y de las herramientas necesarias para la captura de información y la reproducción de las vistas. Ahora se abordará el problema de la creación de usuarios, proyectos, escenas y su gestión. También se trabajará en las opciones de configuración del sistema básicas.
\\

El punto más relevante en esta etapa consiste en el modelo de persistencia a seguir. Para ello, se propone hacer uso de una base de datos para guardar la información relativa a los usuarios y sus escenarios, teniendo en mente la posibilidad de implementar en un futuro un sistema multiusuario persistente. Para este tipo de aplicación, convendría conseguir una Base de Datos Orientada a Objetos, a ser posible en tiempo-real o con un buen tiempo de respuesta. 
\\

Entre otras ventajas, las OODBMS permiten eliminar la necesidad de un doble modelado de la información. El modelo de la base de datos se corresponde directamente con el modelo de datos de la aplicación. De esta forma se ahorra tiempo de diseño y se evita el problema del desajuste por impedancia. Además no es necesaria ninguna transformación explícita del modelo de datos. Por otro lado, se observan mejores respuestas en estos tipos de bases de datos, especialmente en el caso de accesos de tipo navigacional, es decir, aquellos en los que se accede a objetos a partir de relaciones con otros objetos, caso de esta aplicación.
\\

Sin embargo, tras el estudio realizado no ha sido posible encontrar una herramienta que encaje con los requisitos software del proyecto. De entre las candidatas, EyeDB destacaba por ser la más prometedora; sin embargo, actualmente sólo está disponible para sistemas Linux. Del resto, o no disponían de licencias gratuitas, éstas eran muy limitadas, no disponían de APIs para C++ o las librerías eran antiguas y estaban abandonadas. Finalmente, se decidió apostar por usar una opción intermedia. 
\\

Finalmente, se usará una BBDD relacional (PostgreSQL), con un mapper objeto-relacional (Debea). Aunque no se disponga de las ventajas de tener una base de datos directamente implementada como Orientada a Objetos, Postgre es actualmente consideraba la mejor opción libre para Bases de Datos Relacionales: potente, veloz y con un buen soporte de transacciones. Por otro lado, con el mapper objeto-relacional Debea se conservarán los beneficios para el desarrollo al eliminar la traducción del modelo OO al modelo relacional y simplificar la capa de persistencia. De esta forma se relega el problema del desajuste por impedancia al mapper. Con esto se espera reducir significativamente el tiempo de desarrollo y mantener un modelo fácil de mantener y flexible frente a cambios futuros.

\section{Análisis}
\subsection{Análisis de Requisitos de Usuario}

En esta nueva iteración se añadirán elementos y se extenderán algunos de los anteriores. Como puede verse en la evolución del diagrama de casos de uso, en la Figura \ref{F_Casos_de_Uso_Etapa3}, el subconjunto de funcionalidades añadidas se centra en la gestión de usuarios y de escenas con sus entidades, además del esquema de persistencia y la configuración de la aplicación. 
\\

En el apartado de Gestión, la aplicación permitirá dar de alta nuevos usuarios, crear o cargar entornos con entidades asociadas y editar sus propiedades. 
\\

De esta forma se permite la creación y borrado de usuarios, la modificación de permisos y que éste haga login en la aplicación. Un usuario registrado puede crear nuevos escenarios, modificar sus permisos, cargarlos o eliminarlos del sistema. Finalmente, esta información será guardada y cargada en futuras sesiones siguiendo el esquema de persistencia discutido. Sin embargo, las escenas y elementos serán objetos de prueba generados manualmente. Tampoco existirá interacción en esta fase.
\\

En relación a la configuración de la aplicación se permitirá guardar datos relacionados a la configuración del sistema instalado en el equipo. Se utilizará para obtener directorios de uso común por los distintos módulos. También se usará para guardar y recuperar la configuración de las cámaras (número, orientación), así como de las ventanas de render (número, resolución, orientación). Se incluye de la misma forma una ventana de log donde los distintos módulos puedan volcar información útil.

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/casosdeuso_segundademo.png}
\end{center}
\caption{ \label{F_Casos_de_Uso_Etapa3} Diagrama de Casos de Uso. Usuario, Entorno, Configuración y Persistencia}
\end{figure}


\subsection{Selección de Herramientas}

A partir del estudio de las herramientas disponibles se han seleccionado las más adecuadas, tanto por sus características como por compatibilidades técnicas en el conjunto del proyecto.

\begin{itemize}
\item \textbf{BBDD:} PostgreSQL
\item \textbf{Mapper O-R:} Debea
\end{itemize}

\section{Diseño}
\subsection{Diseño de la Aplicación}

Se añaden elementos al módulo IAplication para manejar los conceptos de usuario y proyecto, y se describe el módulo IPersistence. 

\begin{itemize}
\item core: Conjunto de interfaces de la aplicación.
\begin{itemize}
\item IAplicationConfiguration: Se añade interfaz para la configuración de la aplicación.
\item IPersistence: Interfaz de la capa de Persistencia para el guardado y recuperación de los datos de la aplicación.
\begin{itemize}
\item IUserPersistence: Interfaz que refleja del modelo de datos y persistencia del objeto Usuario. 
\item IWorldPersistence: Interfaz que refleja del modelo de datos y persistencia del objeto Mundo (escenario). 
\item IEntityPersistence: Interfaz que refleja del modelo de datos y persistencia del objeto Entidad. Una entidad es una elemento contenido en un escenario.
\end{itemize}
\end{itemize}
\item igui: Interfaz básica para la creación del módulo y registro de ventanas. Modificaciones para soporte de opciones de configuración. Login, inicio y cierre de sesión de usuarios.
\begin{itemize}
\item GUIConfiguration: Paneles de Configuración de la aplicación.
\item GUILogPanel: Panel de log donde se muestra información de la aplicación de los módulos que vuelquen datos en él.
\item GUIUser: Panel de login y de creación de nuevos usuarios.
\item GUIUserInfo: Panel de gestión de usuarios y creación y gestión de escenarios.
\end{itemize}
\item ipersistence: Módulo de persistencia
\begin{itemize}
\item EntityPersistence: Refleja el modelo de datos y la persistencia del objeto Entidad.
\item WorldPersistence: Refleja el modelo de datos y la persistencia del objeto Mundo.
\item UserPersistence: Refleja el modelo de datos y la persistencia del objeto Usuario.
\end{itemize}
\item iprod: Módulo de producción
\begin{itemize}
\item Prod3DEntity: Encapsula el modelo de datos del objeto Entidad y gestiona las características particulares relativas al módulo de producción.
\end{itemize}
\item vox: 
\begin{itemize}
\item ApplicationConfiguration: Implementación de los mecanismos para guardar y cargar las opciones de configuración de la aplicación.
\end{itemize}
\end{itemize}

\subsubsection{Breve Descripción de los Módulos}
Los cambios principales se realizan en los módulos de Aplicación, GUI y Persistencia.\\

En el primero se introducen los controladores de sesión y de configuración y la API necesaria para su uso desde otros módulos. En GUI, se añade un controlador genérico y las interfaces gráficas necesarias para la configuración del sistema y para el inicio de sesión en la misma por parte de usuario. Finalmente, en el módulo de Persistencia se modelan los objetos a conservar y los mecanismos para poder guardarlos y recuperarlos en la base de datos. \\

Por otro lado, tanto el módulo de persistencia como el de GUI, así como el de percepción y producción reciben ajustes para poder usar las opciones de configuración. \\

Como modelo, recordar que cada módulo es responsable de las tareas a su cargo y las comunicaciones se establecen a través de los módulos principales. Ellos disponen de todos los mecanismos necesarios para realizarlas, delegando en su caso en controladores, y ofreciendo una API sencilla de usar. Por ejemplo, cuando el usuario introduce los datos de inicio de sesión el controlador de GUI recibe los datos desde el panel de la interfaz, que termina accediendo al módulo principal de GUI para que llame al módulo principal de aplicación, que es el responsable de gestionar la sesión. En este punto, la aplicación hace uso del controlador de sesión que terminará accediendo al módulo de persistencia para recuperar los datos.
\newpage

\subsubsection{Diseño General en UML}
\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/DiagramadeclasesUML_SegundaDemo.png}
\end{center}
\caption{ \label{F_DiagramadeClasesUML_SegundaDemo} Diagrama de Clases UML, Segunda Demo. Resumen.}
\end{figure}

En este momento se aborda de lleno el módulo de persistencia, añadiendo los elementos Usuario, Mundo (Escenario) y Entidad. Este módulo será accedido para cargar o guardar los datos que se desean conservar. Por otro lado, también se da la posibilidad de configurar elementos de la aplicación, como pueden ser características relacionadas a los dispositivos de entrada o salida del sistema. Además se añade la capacidad de hacer login en la aplicación para cargar los mundos que tenga asociados un usuario, crear escenarios nuevos o editar propiedades del mismo.\\
 
Por simplicidad, en el diagrama se muestran sólo los detalles principales. Para ver los detalles se puede consultar las secciones siguientes.

\subsubsection{Diseño en UML - Núcleo de la Interfaz}

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/DiagramadeclasesUML_SegundaDemo_core.png}
\end{center}
\caption{ \label{F_DiagramadeClasesUML_SegundaDemo_Core} Diagrama de Clases UML, Segunda Demo. Núcleo de la Interfaz.}
\end{figure}

Se introducen cambios de relevancia en la mayoría de las clases, especialmente debido al uso de la interfaz de configuración que usarán los módulos. También cabe destacar el desarrollo de la interfaz de Persistencia que usarán aquéllos módulos que requieran acceder o guardar datos que se deseen conservar.\\

IApplicationConfiguration está destinada a servir de interfaz para conservar datos de directorios de relevancia, como pueden ser el de datos genéricos, recursos de la aplicación, modelos 3D, sonidos o imágenes, lenguaje, datos de conexión con el dispositivo de almacenamiento, el número de cámaras, el número de ventanas de visualización, además de la configuración particular de los mismos (localización en el espacio de la instalación, orientación y resolución).\\

Por otro lado tenemos las interfaces de persistencia. Es interesante recordar que se tiene como objetivo independizar los detalles de implementación de los módulos entre sí, de forma que sean fácilmente sustituíbles por otros nuevos, sin necesidad de realizar modificaciones en los demás. También se ha optado por un diseño de persistencia en el que existe una correspondencia directa entre el modelo OO y el modelo de datos a persistir. Por ello el modelo fundamental de los datos recae en el mismo módulo de persistencia. Como puede verse, es un esquema bastante simple en el que existen tres objetos: usuarios, mundos y entidades. \\

Los usuarios tienen los datos clásicos de conexión, para hacer login en el sistema pero también datos relativos a su experiencia dentro del mismo, como pueden ser desde la localización o permisos, hasta el término nombrado como psique. Psique es una codificación de la conducta y actividades que realiza el usuario en el sistema que será abordada más adelante, en la sección \ref{Etapa5}. 
\\

Las entidades son los elementos componentes de un mundo. Hasta este punto sólo se ha tenido en cuenta que sean elementos 3D posicionados en el espacio. Estas entidades también actúan y su naturaleza o comportamiento vendrá definida en un atributo llamado psique, que influirá en sus acciones.\\

Finalmente, los mundos son espacios creados por un usuario y que contendrán colecciones de entidades conservando una visión global de la experiencia. Como es razonable desear que un escenario permanezca inalterable, o poder acceder sólo a determinadas opciones en él, el usuario puede definir unos permisos sobre cada mundo. Este es un campo que sólo se plantea y se presenta, es una línea de trabajo interesante pero su desarrollo se plantea como trabajo futuro.
\newpage

\subsubsection{Diseño en UML - Módulo de Aplicación}

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/DiagramadeclasesUML_SegundaDemo_vox.png}
\end{center}
\caption{ \label{F_DiagramadeClasesUML_SegundaDemo_Vox} Diagrama de Clases UML, Segunda Demo. Módulo Principal de la Aplicación.}
\end{figure}

En la Figura \ref{F_DiagramadeClasesUML_SegundaDemo_Vox} se muestra el módulo principal de la aplicación, el responsable de todas las acciones fundamentales del mismo. Es el responsable de instanciar y gestionar los módulos necesarios y suministrar métodos para la manipulación de la aplicación. \\

De esta forma, es ahora también responsable de la configuración de la aplicación, que realiza mediante el controlador ConfigurationController, así como de la sesión mediante SessionController.\\

SessionController da capacidades para cargar los datos de un usuario y permitir su entrada en el sistema, así como su salida, además de capacidades para saber cuál es el usuario y escenario actuales, o si un determinado usuario o escenario es el actual. Se entiende como sesión el estado en el cual un usuario que accede al sistema carga de un escenario para comenzar el uso del mismo. Esto es, se establece una sesión cuando se asigna un usuario y un escenario en el sistema.\\

Por su lado, ConfigurationController es usado fundamentalmente al comienzo y final de la aplicación para cargar y guardar datos relativos al funcionamiento de la misma. Por ello, sus métodos principales son Load y Save.

\subsubsection{Diseño en UML - Módulo de Persistencia}

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/DiagramadeclasesUML_SegundaDemo_ipersistence.png}
\end{center}
\caption{ \label{F_DiagramadeClasesUML_SegundaDemo_IPersistence} Diagrama de Clases UML, Segunda Demo. Módulo de Persistencia.}
\end{figure}

Como puede verse en la Figura \ref{F_DiagramadeClasesUML_SegundaDemo_IPersistence}, en este módulo se implementan las interfaces definidas referentes a persistencia en el núcleo de la interfaz y son un reflejo del mismo adaptado a la implementación final. Como se ha comentado, se ha optado por fusionar los conceptos del modelo de objetos y modelo de persistencia. Siendo así, la gestión de la persistencia de un objeto recae sobre él mismo (por ejemplo, para crear un objeto basta con instanciarlo, y para guardar sus datos basta con llamar a su método 'Save'). Esta fusión podrá verse en mayor detalle en la sección de Implementación.\\

Como con el resto de módulos independientes existe un núcleo principal que será el encargado de iniciar el dispositivo de almacenamiento y de atender peticiones de otros módulos. Estas peticiones son consultas que pueden hacerse sin necesidad de instanciar el objeto, por ejemplo, para saber si un usuario determinado o un mundo existe u obtener la lista de mundos que un usuario puede ver. 

\subsubsection{Diseño en UML - Módulo de GUI}
\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/DiagramadeclasesUML_SegundaDemo_igui.png}
\end{center}
\caption{ \label{F_DiagramadeClasesUML_SegundaDemo_IGUI} Diagrama de Clases UML, Segunda Demo. Módulo de Persistencia.}
\end{figure}

Una de las primeras consecuencias lógicas con la entrada de la capacidad para gestionar la configuración de la aplicación es su impacto sobre la interfaz gráfica de usuario. Este impacto puede verse en la Figura \ref{F_DiagramadeClasesUML_SegundaDemo_IGUI} y se debe, por un lado, a la necesidad de localizar recursos estéticos como las imágenes de los botones, siendo el caso más común y básicamente el que sucede sobre las clases previamente existentes. Por otro lado, es también a través de la interfaz de usuario como éste va a editar los parámetros de configuración del sistema, mediante el panel de configuración. Además puede verse cambios relativos al uso del sistema, como son los paneles de login, de gestión de la información del usuario, y de log del sistema.\\

De esta forma, tenemos por un lado el panel GUIConfiguration. Este panel permitirá al usuario cambiar la cantidad de cámaras de las que se alimentará el módulo de Percepción, así como los parámetros de las mismas. De igual forma, podrá establecer la cantidad de ventanas que se mostrarán y su configuración, administrando vistas independientes de la escena. Gracias a esta posibilidad se dota de una gran flexibilidad al sistema para establecer modelos de instalaciones distintos o espacios de escritorio, proyecciones frontales o sistemas Cave que envuelven al usuario con pantallas.\\

Por otro lado, tenemos el panel GUIUser, desde el cual un usuario podrá hacer login en el sistema o registrarse en el mismo como nuevo usuario. Este panel está estrechamente relacionado con GUIUserInfo, que muestra al usuario actual la lista de mundos que ha creado y donde le permite modificar sus atributos, así como crear nuevos usuarios, destruirlos o borrarse él mismo del sistema.\\

Finalmente, se ha añadido un panel de log en el cual se mostrará información de interés del funcionamiento del sistema.\\

Hay que tener en cuenta que estas interfaces son sólo componentes que muestran y capturan información. No contienen ninguna lógica relevante de aplicación. Para estas tareas se hace uso, como es lógico, de un controlador. Sin embargo, las necesidades son mínimas y en su mayor parte este controlador sólo derivará las acciones hacia sus responsables. Por ello y por simplicidad, se ha decidido usar un controlador genérico para todo el módulo. Este controlador responderá a las necesidades de gestión de los paneles, para mostrar unos u otros. Pero a su vez, también encauzará las peticiones del usuario al interactuar con la interfaz, hacia el responsable de su gestión (por ejemplo, para crear un usuario nuevo o cargar un escenario).

\subsubsection{Diseño en UML - Módulo de Producción}

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/DiagramadeclasesUML_SegundaDemo_iprod.png}
\end{center}
\caption{ \label{F_DiagramadeClasesUML_SegundaDemo_IProd} Diagrama de Clases UML, Segunda Demo. Módulo de Producción.}
\end{figure}

Como puede verse en la Figura \ref{F_DiagramadeClasesUML_SegundaDemo_IProd}, durante esta fase los cambios en el Módulo de Producción son moderados. Por lado, usará la información de los parámetros de configuración para las ventanas que mostrará; por otro, se le ha añadido capacidad para limpiar y cargar datos en la escena. Estos datos serán los relativos al mundo que el usuario desee cargar.\\

De esta forma, destacan los métodos RunWorld() y CloseWorld(), que cargan un escenario y libera la escena respectivamente. También LoadEntityIntoScene() que carga una entidad persistida dentro de la escena, o LoadDefaultScene() que carga una escena sencilla por defecto, que se usará con carácter estético cuando no hay cargado ningún escenario.\\ 

Por otro lado, se puede ver que aparece una nueva clase llamada Prod3DEntity. Esta nueva clase representa las entidades 3D cargadas en la escena, que efectivamente se corresponderán con las Entidades persistidas que forman un escenario. Prod3Dentity encapsula los datos de EntityPersistence y añade información concreta dependiente de la implementación, relacionada con el motor gráfico. En este punto, esta información es concretamente el nodo del grafo de la escena.

\subsubsection{Diseño en UML - Módulo de Percepción}

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/DiagramadeclasesUML_SegundaDemo_ipercept.png}
\end{center}
\caption{ \label{F_DiagramadeClasesUML_SegundaDemo_IPercept} Diagrama de Clases UML, Segunda Demo. Módulo de Percepción.}
\end{figure}

Al igual que en el caso del módulo de Producción, este módulo sufre pocos cambios. En realidad incluso menos, ya que en esta fase del desarrollo no se abordarán funcionalidades nuevas en la percepción. Sin embargo, el módulo sí se ve afectado por los parámetros de configuración de la aplicación, permitiendo establecer el número de cámaras desde las que se capturan imágenes y los parámetros de las mismas.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Implementación}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Los módulos afectados en la presente fase son los siguiente:

\begin{itemize}
\item core: Añadida interfaz de configuración de aplicación IApplicationConfiguration.
\item igui: Añadidas interfaces gráficas para la gestión de usuarios, escenas y paneles de configuración de la aplicación.
\item ipercept: Configuración del módulo.
\item ipersistence: Persistencia de los objetos usuario, mundo y entidad.
\item iprod: Configuración del módulo.
\item vox: Controlador de la configuración de la Aplicación.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Núcleo de la Interfaz}
$\backslash$src$\backslash$core
\begin{itemize}
\item IApplicationConfiguration: Interfaz destinada a la configuración de la aplicación y que puede ser usada por parte de los módulos con independencia de la implementación final. 
\\
Algunas características son, sin embargo, inherentes a la naturaleza de los módulos y los dispositivos genéricos. Hablamos del caso de las propiedades clásicas de los dispositivos de entrada como son las cámaras y de salida como las ventanas de visualización. Algunas de estas opciones son la resolución y orientación de los mismos. De esta forma, se añaden las clases CameraData y DisplayData.
%\\

\begin{lstlisting}[language=C++]
class DisplayData
{ public:
   DisplayData() : x(0),y(0),z(0),flip_h(false),flip_v(false), resolution_x(800),resolution_y(600){}
   ~DisplayData() {}
   double x, y, z;
   bool flip_h, flip_v;
   unsigned int resolution_x, resolution_y; };
\end{lstlisting}

Finalmente, se añaden los métodos a utilizar para acceder y modificar las opciones de configuración de la aplicación. Se muestran algunas de las principales. Para ver más detalles consultar la sección \ref{detallesImp:InterfacesCore}, página \pageref{detallesImp:InterfacesCore}.  

\begin{lstlisting}[language=C++]
class IApplicationConfiguration
{ public:
   virtual unsigned int GetNumCams()     = 0;
   virtual unsigned int GetNumDisplays() = 0;   
   virtual core::CameraData  GetCameraData(const int &id)=0;
   virtual core::DisplayData GetDisplayData(const int &id)=0;	
   virtual std::string GetSDHost() = 0;
   virtual std::string GetSDPort() = 0;
   virtual std::string GetSDPassword()          = 0;
   virtual std::string GetUIResourceDirectory() = 0;			
\end{lstlisting}

\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Aplicación Principal}
$\backslash$apps$\backslash$vox

Se añade la clase ApplicationConfiguration que representa el modelo de datos de la configuración del sistema, y los controladores ConfigurationController y SessionController. Se describen a continuación:

\begin{itemize}
\item $\backslash$apps$\backslash$ApplicationConfiguration: Implementa la interfaz IApplicationConfiguration que puede verse en el apartado anterior, disponiendo de los mismo métodos para obtener y asignar valores.
\item $\backslash$apps$\backslash$controllers$\backslash$ConfigurationController: Controlador destinado a la gestión de la configuración. Carga y guarda los parámetros mediate los métodos Load() y Save() de forma estructurada en un archivo de configuración en texto plano llamado config.ini. Como puede verse, para esta tarea se buscan las etiquetas asociadas a cada atributo y toma o guarda directamente su valor. 
\begin{lstlisting}[language=C++]
bool ConfigurationController::Load()
{   if ( config_file.is_open() )
    { while (!config_file.eof())
      { if ( tag == "[DATA_DIRECTORY]" )
        { config_file.getline(str, size);
          app_config->SetDataDirectory(str); }
\end{lstlisting}
Hay que tener en cuenta que la mayoría de los datos son atómicos, es decir, un atributo está asociado sólo con un único valor indivisible. Sin embargo, existen atributos compuestos de múltiples valores que requieren parsing para su lectura y escritura. Este es el caso de la configuración particular de las distintas cámaras y ventanas. Para ellos se usan etiquetas a modo de separadores para distinguir cada dispositivo y sus atributos.
\begin{lstlisting}[language=c++]
std::string ConfigurationController::ParseDisplayConfig()
{ ...
  unsigned int win_num = app_config->GetNumDisplays();
  for (unsigned int i = 1; i <= win_num; i++)
  { core::DisplayData data = app_config->GetDisplayData(i);
    wop << "::ID::" << i 
    << "::X::" << data.x << "::Y::" << data.y << "::Z::" << data.z 
    << "::FLIPH::" << data.flip_h << "::FLIPV::" << data.flip_v
    << "::RES_X::" << data.resolution_x 
    << "::RES_Y::" << data.resolution_y;	}
  ... }
\end{lstlisting}
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Módulo de Persistencia}\label{Etapa3detallesImp:ModuloPersistencia}
$\backslash$src$\backslash$ipersistence\\

La clase principal del Módulo de Persistencia y que implementa la interfaz IPersistence es MainPersistence. Esta clase se encarga de la inicialización del módulo y del dispositivo de almacenamiento. La verdadera intención era hacer uso de una librería que tuviera un enfoque Orientado a Objetos. Sin embargo, no fue posible encontrar una que cumpliera los requisitos software de la aplicación. Por ello, se decidió adoptar una solución intermedia. De esta forma se decide utilizar una base de datos relacional, PostGreSQL, y un mapper objeto-relacional, Debea. Para ver los detalles de implementación necesarios para configurar el módulo con estas librerías se puede consultar la sección \ref{detallesImp:PostgreSQL} página \pageref{detallesImp:PostgreSQL}. De esta forma, el mapper permite fusionar el modelo de objeto y el modelo de persistencia en uno. Sin embargo, es interesante saber que, en el fondo, al tratarse el dispositivo de almacenamiento de una base de datos SQL, también se permiten realizar consultas de forma manual.

\begin{itemize}
\item $\backslash$apps$\backslash$MainPersistence: Inicializa la conexión con la base de datos y en caso de no existir contenido, es la responsable de crear las tablas de las entidades e introducir datos por defecto. Para crear las tablas, solicita los esquemas de traducción a las clases de los objetos a persistir. 
\begin{lstlisting}[language=C++]
MainPersistence::MainPersistence(IApplicationConfiguration *app_cfg)
{ std::string db_name = app_cfg->GetSDName();
  std::string db_user = app_cfg->GetSDUser();
  std::string db_passwd = app_cfg->GetSDPassword();
  std::stringstream connect;
  connect << "dbname=" << db_name << " user=" 
          << db_user << " password=" << db_passwd;
  ar.setIdFetcher(new dba::GenericFetcher()); 
  unlink(db_name.c_str());
  ar.open("dbapgsql-static", connect.str().c_str());
  //create needed tables
  ar.getOStream().sendUpdate(counter_create);
  ar.getOStream().sendUpdate((*(UserPersistence::GetSchema())));
  ar.getOStream().sendUpdate((*(EntityPersistence::GetSchema())));
  ar.getOStream().sendUpdate((*(WorldPersistence::GetSchema())));
  //first id //
  ar.getOStream().sendUpdate(dba::SQL("INSERT INTO debea_object_count VALUES (:d)") << 1);
...
\end{lstlisting}
Los modelos de los objetos están ligados directamente a su persistencia, por lo que MainPersistence no tiene responsabilidad explícita sobre la gestión particular de cada uno de ellos. Sin embargo, dispone de mecanismos para consultar la base de datos y obtener información de interés, como puede ser consultar si un usuario o un mundo existe, o recuperar la lista de mundos de los que un usuario es propietario. También permite borrar un escenario o un usuario, sin necesidad de cargarlo en el sistema, o modificar las propiedades de un escenario.\\

\item $\backslash$apps$\backslash$UserPersistence: Representa al objeto usuario y contiene sus datos y métodos para guardarlos y recuperarlos. Como puede verse, la clase hereda de dba::Storeable lo que permitirá hacer uso de los mecanismos de persistencia. Aunque el mapper tiene una API suficiente posee algunas carencias. Por un lado es necesario establecer el esquema de la clase (definición de la tabla en la base de datos) y la relación de las variables con las mismas. 
\begin{lstlisting}[language=C++]
//::MAPPING:: Class name, parent class name and relation name
BEGIN_STORE_TABLE(UserPersistence, dba::Storeable, "user_table")
 BIND_STR(UserPersistence::name,     dba::String, "name"    )
 BIND_STR(UserPersistence::password, dba::String, "password")
 BIND_INT(UserPersistence::psique,   dba::Int,    "psique"  )
 ...
END_STORE_TABLE()
//SQL schema
dba::SQL UserPersistence::schema(
"CREATE TABLE user_table ("
"  id INT PRIMARY KEY,"
"  name VARCHAR,"
"  password VARCHAR,"
"  psique INT,"
...
\end{lstlisting}
Una vez mapeado se pueden crear instancias de la clase que ya tienen capacidad directa de persistencia. Sin embargo, se desea una abstracción completa de la implementación de estos mecanismos, además de un mejor diseño para ofrecer una mejor interfaz de uso. Para ello, se encapsulan los métodos básicos de carga, almacenamiento y borrado en los métodos Load(), Save() y Delete().
\begin{lstlisting}[language=C++]
bool UserPersistence::Load(const int &id)
{ try 
  { boost::mutex::scoped_lock lock(m_mutex);
    UserPersistence new_object;
    dba::SQLIStream istream = ar->getIStream();
    istream.setWhereId(id);
    bool success = istream.get(&new_object);
    this->operator =(new_object);
    return success; } 
  catch (const dba::SQLException& pEx){ ProcessException(pEx); }
  catch (const dba::Exception& pEx)   { ProcessException(pEx); }
  return false;
} 
void UserPersistence::Save()
{ try 
  { boost::mutex::scoped_lock lock(m_mutex);
    ar->getOStream().put(this);	} 
  catch (const dba::SQLException& pEx){ ProcessException(pEx); }
  catch (const dba::Exception& pEx)   { ProcessException(pEx); }
} 
void UserPersistence::Delete()
{ try 
  { boost::mutex::scoped_lock lock(m_mutex);
    this->setState(dba::Storeable::stState(DELETED));
    ar->getOStream().put(this); } 
  catch (const dba::SQLException& pEx){ ProcessException(pEx); }
  catch (const dba::Exception& pEx)   { ProcessException(pEx); }
}
\end{lstlisting}
Hay que tener en cuenta que los objetos pueden ser accedidos desde distintos módulos que se ejecutan en hilos en paralelo, por lo que es necesario el uso de cerrojos para sincronizar los accesos. Se utiliza para ello los cerrojos de ámbito que ofrece boost.\\

\item $\backslash$apps$\backslash$EntityPersistence: Es la clase que implementa la interfaz IEntityPersistence que modela el objeto Entidad. Una entidad es un elemento básico de los cuales se compone una escena. Su implementación sigue exactamente el mismo esquema que el caso anterior: un mapeo de variables, una definición de esquema, y métodos que encapsulan la API de Debea para las operaciones de persistencia.\\

Otro detalle a tener en cuenta es la necesidad de un filtro para mapear las variables de tipo Float o Double correctamente, ya que la librería demostraba problemas en sistemas de localización no inglesa, debido a los separadores usados como punto decimal. Estudiado el problema se corrigió y se añadió un filtro adecuado vFloat. Los cambios fueron remitidos y pendientes de admisión en revisión.\\

Finalmente, para reflejar la relación de pertenencia que tienen los escenarios con las entidades, es necesario incluir en el esquema de la tabla de la base de datos una entrada que es la clave ajena de mundo al que pertence.

\begin{lstlisting}[language=C++]
//::MAPPING:: Class name, parent class name and relation name
BEGIN_STORE_TABLE(EntityPersistence, dba::Storeable, "entity_table")
 BIND_STR(EntityPersistence::name,       dba::String, "name"       )
 BIND_FLT(EntityPersistence::position_x, dba::vFloat, "position_x" )
END_STORE_TABLE()
//SQL schema
dba::SQL EntityPersistence::schema(
"CREATE TABLE entity_table ("
"  id INT PRIMARY KEY,"
"  name VARCHAR,"
"  position_x FLOAT,"
"  fk_world INT",
...
\end{lstlisting}

\item $\backslash$apps$\backslash$WorldPersistence: El último modelo de objeto a persistir es el Mundo o escenario. Simplemente define una colección de Entidades y algunos atributos como nombre, permisos o el usuario que creó dicho mundo.\\

Para mapear la lista de entidades que contiene un Mundo se usa la macro BIND\_COL, que establece una relación de cada elemento con la tabla correspondiente y que implica la necesidad de que exista una clave ajena en el esquema de la otra clase.\\

Puede apreciarse que existe una relación similar entre Usuario y Mundo, pero en ese caso no se lleva a cabo por dos motivos: Primero, porque conceptualmente el objeto usuario no se compone de Mundos, y segundo porque, aunque en el desarrollo de este proyecto un usuario sólo puede acceder a los mundos que ha creado, se mantiene una perspectiva abierta para trabajo futuro en la que otros usuarios conectados puedan acceder o compartir cualquier mundo, siendo sus acciones limitadas por los permisos que el creador definiera para el mismo.
\begin{lstlisting}[language=C++]
//::MAPPING:: Class name, parent class name and relation name
BEGIN_STORE_TABLE(WorldPersistence, dba::Storeable, "world_table")
 BIND_STR (WorldPersistence::name,        dba::String, "name")
 BIND_STR (WorldPersistence::owner,       dba::String, "owner")
 BIND_INT (WorldPersistence::permissions, dba::Int,    "permissions")
 BIND_COL (WorldPersistence::entities,    
           dba::stdList<EntityPersistence>, "fk_world")
END_STORE_TABLE()
//SQL schema
dba::SQL WorldPersistence::schema(
"CREATE TABLE world_table ("
"  id INT PRIMARY KEY,"
"  name VARCHAR,"
"  owner VARCHAR,"
"  permissions INT"
\end{lstlisting}

\end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Módulo de GUI}
$\backslash$src$\backslash$igui

\begin{itemize}
\item $\backslash$src$\backslash$igui$\backslash$controllers$\backslash$GUIGenericController: Dado que la lógica asociada al módulo de GUI es simple y con pocas opciones, que además son comunes, se ha incluido un controlador genérico. Este controlador es utilizado para alternar entre la visualización de distintos paneles de GUI, así como para gestionar los accesos a funcionalidades del núcleo principal del módulo, MainGUI. 

\begin{lstlisting}[language=C++]
class GUIGenericController
{public:
  static GUIGenericController* GetInstance() {return instance;}
  bool CreateUser(const string &name,const string &passwd);
  bool LoginUser(const string &name,const string &passwd);
  bool DeleteUser(const string &name);
  bool DeleteWorld(const string &name);
  void LogOut();
  void ViewUserInfoPanel();
  void ViewLogPanel();
  void ViewConfigurePanel();
  private:
  static GUIGenericController *instance; 
  ... };
\end{lstlisting}

\item $\backslash$src$\backslash$igui$\backslash$GUIUser: Interfaz destinada al acceso de los usuarios, mediante nombre y contraseña, y para la creación de nuevas cuentas de usuario. La lógica relacionada al login y a la creación de nuevos usuarios se realiza a través del controlador GUIGenericController. Sin embargo, si las acciones no son responsabilidad de este módulo lo único que hace GUIGenericController es derivarlas al responsable.

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/gui_user.png}
\end{center}
\caption{ \label{F_CapturaFase3SegundaDemo_GUIUSer} GUI: User login}
\end{figure}

El código de la interfaz es simple y común a las mostradas anteriormente. De forma ilustrativa se muestra el caso de la operación de login. Primero, Se recuperan los valores introducidos por el usuario en los controles, luego se usa el controlador, que relegará la consulta. En caso de no tener éxito la petición se muestran los mensajes de error.

\begin{lstlisting}[language=C++]
BEGIN_EVENT_TABLE(GUIUser, wxPanel)
   EVT_PAINT   (                 OnPaint         )
   EVT_BUTTON  ( wxID_LOGIN    , OnLoginButton   )
   EVT_BUTTON  ( wxID_REGISTER , OnNewUserButton )
END_EVENT_TABLE()

void GUIUser::OnLoginButton(wxCommandEvent& WXUNUSED(event))
{ s_user_name = user_name->GetValue();
  s_user_password = user_passwd->GetValue();
  DoLogin(s_user_name, s_user_password);   }

void GUIUser::DoLogin(const string &name, const string &passwd)
{ GUIGenericController *guiGc = GUIGenericController::GetInstance();
  if (guiGc != NULL) login = guiGc->LoginUser(name, passwd);
  if (login)
  { user_logged_in = true;
    guiGc->ViewUserInfoPanel(); }
  else
  { user_logged_in = false;
    wxMessageDialog message_dialog(this, _("User or password incorrect"), "Message box", wxOK \| wxICON_EXCLAMATION \| wxSTAY_ON_TOP);
    message_dialog.ShowModal(); } }
\end{lstlisting}

\item $\backslash$src$\backslash$igui$\backslash$GUIUserInfo: Interfaz destinada a la gestión del usuario y de sus datos. Desde este panel el usuario puede ver la lista de Entornos que tiene, crear nuevos entornos, borrarlos, modificar sus permisos o lanzarlos para su ejecución en la sesión. Estas acciones se realizan a través del controlador GUIGenericController. 
\\

Como se comentaba en el punto anterior, no es el controlador GUIGenericController el que realiza directamente las acciones de login en el sistema, y tampoco el núcleo principal del módulo de GUI. De hecho, sólo derivan las peticiones a los módulos responsables. En este caso, la responsabilidad de gestionar la sesión, así como todas las acciones sobre el sistema caen sobre el módulo principal de la Aplicación, que en este caso, a su vez, relegará en el controlador de la sesión SessionController, que hará uso finalmente del módulo de Persistencia. 

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/gui_userinfo.png}
\end{center}
\caption{ \label{F_CapturaFase3SegundaDemo_GUIUserInfo} GUI: User info}
\end{figure}

\item $\backslash$src$\backslash$igui$\backslash$GUIConfiguration: Este panel de la interfaz está destinado a la configuración de la aplicación. En el puede establecerse el número de cámaras que se van a conectar y el número de ventantas de render que se quieren ver. Además por cada cámara o ventana de render se pueden establecer parámetros de configuración, como puede ser su orientación o resolución. Debido a limitaciones de las librerías y a la relevancia de algunos cambios, algunos de éstos requieren el reinicio de la aplicación y de tal forma es notificado al usuario. Por otro lado, hay que tener en cuenta que algunos parámetros, como el calibrado posicional de las cámaras en el espacio, no son usados todavía aunque ya se almacenan y se permite su gestión.

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/gui_configure.png}
\end{center}
\caption{ \label{F_CapturaFase3SegundaDemo_GUIConfiguration} GUI: Panel de configuración}
\end{figure}

\item $\backslash$src$\backslash$igui$\backslash$GUILogPanel: La última de las interfaces añadidas muestra una ventana donde aparecerán mensajes que puedan resultar de interés enviados desde cualquier módulo de la aplicación. No pretende ser un log exhaustivo del funcionamiento de la aplicación sino una forma de ofrecer información a modo de curiosidad. En la Figura \ref{F_CapturaFase3SegundaDemo_GUILogPanel} pueden verse mensajes de incialización, así como la entrada y salida de usuarios en el sistema.

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/gui_logpanel.png}
\end{center}
\caption{ \label{F_CapturaFase3SegundaDemo_GUILogPanel} GUI: Panel de log}
\end{figure}
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Módulo de Producción}
$\backslash$src$\backslash$iprod
En esta fase el módulo de Producción sufre pocos cambios. Se añade la clase Prod3DEntity que encapsula el objeto entidad añadiendo particularidades propias del motor gráfico y métodos a MainProd que permiten cargar y limpiar la escena.
\begin{itemize}
\item $\backslash$src$\backslash$iprod$\backslash$Prod3DEntity: Como se ha comentado encapsula el objeto EntityPersistence y añade particularidades relacionadas con la implementación del módulo de aplicación. En este caso se añade un puntero al nodo del grafo de la escena que se corresponde con el objeto 3D asociando.
\begin{lstlisting}[language=C++]
class Prod3DEntity
{ public:
  Prod3DEntity(core::IEntityPersistence* ent);
  virtual ~Prod3DEntity();
  std::string GetData()               { return data;     }
  NodePath*	GetNodePath()             { return nodepath; }
  core::IEntityPersistence* GetEntity(){return entity;   }
  void SetData(const std::string &value){data = value;   }
  void SetNodePath(NodePath *value);
 private:
  std::string data;
  core::IEntityPersistence* entity;
  NodePath *nodepath; };
  
Prod3DEntity::Prod3DEntity(core::IEntityPersistence* ent)
{ if (entity != NULL ) data = entity->GetModelData(); }

void Prod3DEntity::SetNodePath(NodePath *value)		
{ nodepath = value; 
  if ( (entity != NULL) && (nodepath != NULL) )
  { float posx, posy, posz, rotx, roty, rotz, scale;
    entity->GetPosition(posx, posy, posz);
    entity->GetOrientation(rotx, roty, rotz);
    entity->GetScale(scale);
    nodepath->set_pos(posx,posy,posz);
    nodepath->set_hpr(rotx, roty, rotz);
    nodepath->set_scale(scale,scale,scale);	} }  
\end{lstlisting}

\item $\backslash$src$\backslash$iprod$\backslash$MainProd: Durante esta fase el módulo de Producción sufre algunos cambios leves, aunque ha tenido que hacerse frente a ciertas dificultades del motor gráfico. Se hace uso de la configuración de la aplicación para mostrar las ventanas de visualización y se dota de capacidad para cargar y cerrar escenarios. Por ello, cabe destacar los métodos RunWorld(), que lee los datos de un objeto Mundo, extrae sus entidades y las carga en escena, y CloseWorld(), que vacía la escena y limpia los datos internos.

\begin{lstlisting}[language=C++]
void MainProd::CloseWorld()
{ ClearScene();
  { boost::mutex::scoped_lock lock(m_mutex);
    current_user  = NULL;
    current_world = NULL;
    for (unsigned int i = 0; i < scene_entities.size(); i++)
     delete scene_entities[i];
    scene_entities.clear(); } }
bool MainProd::RunWorld(core::IUserPersistence *user, core::IWorldPersistence *world)
{ boost::mutex::scoped_lock lock(m_mutex);
  current_user  = user;
  current_world = world;	
  for (int i=0; i < current_world->GetNumEntities(); i++)
  { IEntityPersistence *ient = current_world->GetEntity(i);
    Prod3DEntity *new_entity = new Prod3DEntity(ient);
    scene_entities.push_back(new_entity);
    LoadEntityIntoScene(new_entity); }
  initialized = true;
\end{lstlisting}

Dentro de RunWorld es usado el método LoadEntityIntoScene() que es usado para cargar las entidades leídas en la escena 3D. Se recuerda que por motivos de eficiencia el modelo 3D leído es cargado sólo en el primer framework, correspondiente a la primera vista, y posteriormente se instancian en el resto.

\begin{lstlisting}[language=C++]
void MainProd::LoadEntityIntoScene(Prod3DEntity * entity)
{ std::string data = entity->GetData();
  scene_nodepaths[entity] = pandawindows_array[1]->load_model(framework.get_models(),data);
  entity->SetNodePath(&(scene_nodepaths[entity]));
  scene_nodepaths[entity].reparent_to(pandawindows_array[1]->get_render());
  std::map<int, WindowFramework*>::iterator iter = pandawindows_array.begin(); iter++;
  while(iter != pandawindows_array.end())
  { scene_nodepaths[entity].instance_to(iter->second->get_render());
    iter++; } }
\end{lstlisting}
Las últimas modificaciones afectan a la configuración de las ventanas de visualización. Por un lado se cargan los valores de la resolución de cada ventana de visualización y por otro se realiza una transformación horizontal o vertical de la imagen renderizada. De hecho, por eficiencia lo que se hace es escalar la cámara por el valor -1 en los ejes horizontal o vertical según corresponda. De esta forma no será necesario transformar la imagen después de ser renderizada. Un detalle a tener en cuenta es que al realizar el escalado en uno de los ejes del plano de visualización, el vector normal se invierte, por lo que la ocultación de caras usada en la escena (cull facing) también debe ser invertido o se verán las caras de los polígonos contrarias a las deseadas.
\begin{lstlisting}[language=C++]
for (unsigned int i = 1; lock && (i <= num_windows); i++)
{ int flip_me_x = (app_config->GetDisplayData(i).flip_h) ? -1 : 1;
  int flip_me_y = (app_config->GetDisplayData(i).flip_v) ? -1 : 1;
  windowcamera_array[i].set_sx(flip_me_x);
  windowcamera_array[i].set_sz(flip_me_y);
  if ( (flip_me_x == -1) ^ (flip_me_y == -1) )
   pandawindows_array[i]->get_render().set_attrib(CullFaceAttrib::make_reverse());
  else
   pandawindows_array[i]->get_render().set_attrib(CullFaceAttrib::make_default()); } }
\end{lstlisting}

\end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Módulo de Percepción}
$\backslash$src$\backslash$ipercept$\backslash$PerceptVideo: Los cambios en este módulo son mínimos y afectan sólamente al establecimiento de los parámetros de configuración. En este caso se aplica una transformación de reflejo horizontal o vertical según se haya especificado en la configuración.
\begin{lstlisting}[language=C++]
if ( app_config != NULL )
{ flip_me_x = app_config->GetCameraData(iter->first).flip_h;
  flip_me_y = app_config->GetCameraData(iter->first).flip_v;  
  if(flip_me_x && !flip_me_y) //Flip Horizontally
  {	IplImage *aux = capture_img;
    cvFlip(capture_img, capture_img, 1); }
  else if(!flip_me_x && flip_me_y) //Flip Vertically
  { IplImage *aux = capture_img;
    cvFlip(capture_img, capture_img, 0); }
  else if(flip_me_x && flip_me_y) //Flip Both
  { IplImage *aux = capture_img;
    cvFlip(capture_img, capture_img, -1); }
\end{lstlisting}

\section{Validación y Publicidad}
\subsection{Validación}
\begin{itemize}
\item Comprobación de uso de recursos de la máquina mediante las herramientas del sistema. En la máquina en la que se desarrolla la aplicación muestra consumir un 4 y un 6\% de CPU, con 21 subprocesos asociados y 85Mb de memoria mantenidos con el escenario por defecto de prueba. Al cargar los distintos escenarios se ven cambios dependiendo de la complejidad de los mismos, lo cual es esperado. 
\item Comprobación de la ejecución y cierre correctos, sin salidas de la aplicación inesperadas ni pérdidas de memoria.
\item Uso de herramientas para medición de fotogramas por segundo para comprobar el rendimiento de la ventana de visualización. No se aprecian cambios en la tasa de fotogramas por segundo que se mantiene a 60fps para dos ventanas de visualización. Teniendo en cuenta que 60 es el límite máximo impuesto por la sincronización vertical del monitor.
\end{itemize}

\vspace{20 mm}

Se muestran algunas capturas del estado actual de la aplicación. Esta versión ya incluye persistencia y gestión de usuarios y escenas. En la Figura \ref{F_CapturaFase3SegundaDemo_20100717} se ve a un usuario cargando una escena personalizada, mientras que en la figura \ref{F_CapturaFase3SegundaDemo_20100717p} otro usuario muestra otra escena diferente.


\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/screenshot_20100717_o.jpg}
\end{center}
\caption{ \label{F_CapturaFase3SegundaDemo_20100717} Fase 3: Segunda Demo: Un usuario que ha iniciado sesión ve una escena personalizada.}
\end{figure}

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/screenshot_20100717_p.jpg}
\end{center}
\caption{ \label{F_CapturaFase3SegundaDemo_20100717p} Fase 3: Segunda Demo: Otro usuario visualizando su propia escena.}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Etapa 4: Detección, Navegación e Interacción}

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/blocks_full_etapa4.jpg}\label{F_DiagramaCompleto}
\end{center}
\caption{ \label{F_BloquesResumen_Etapa4} Componentes sobre los que se va a trabajar.}
\end{figure}

Siguiendo la mecánica establecida se continúa añadiendo funcionalidades a partir de la demo anterior. En esta ocasión se abordará fundamentalmente el problema de la detección, reconocimiento y localización del usuario. Se hará uso de técnicas de visión de computador para situarlo en el espacio de la instalación y establecer la situación de las cámaras del espacio 3D en correspondencia. También se establecerán mecanismos de interfaz visual de usuario, que pretenden ser simples y lo más transparentes posible, para permitir el movimiento del sujeto por el espacio 3D, así como la posibilidad de activar login automático mediante reconocimiento facial. Finalmente, se hace una introducción a los mecanismos de interacción del usuario con el entorno. En un principio, se integrará un esquema de navegación y preparación del sistema de colisiones básicos. \\

Al finalizar esta fase se dispondrá de los mecanismos principales para poder crear los contenidos, así como para poder utilizar la interfaz visual con el usuario, tanto para la navegación por la escena como la interacción con los elementos que existan en ella. En la siguiente etapa se seguirá trabajando en estos apartados para incorporar capacidades más avanzadas, pero además se comenzará a abordar el esquema a usar para la creación de contenido.\\

Como se ha comentado, existen tres apartados a abordar:\\

\begin{itemize}
\item Por un lado, la percepción. En este caso, es necesario establecer y configurar el espacio de la instalación mediante la calibración de las cámaras. Esta opción está ya disponible a través de las ventanas de configuración de la aplicación, pero es ahora cuando se implementará su funcionalidad. Una vez definido el espacio se procederá a la localización del usuario dentro del mismo, estableciendo una relación entre el espacio de la instalación y su correspondencia con el espacio de la escena 3D. Se preparan diversos detectores, a saber: presencia, movimiento y caras. También se incorporará reconocimiento facial. \\

\item Partiendo del punto anterior, se abordará la Navegación, esto es, los mecanismos que se establecerán para permitir el movimiento del usuario por la escena. Se utilizará el detector de presencia y el de caras para la localización global y el movimiento; la cara se usará para el movimiento relativo de las cámaras de la escena. \\

\item De forma paralela se usará el detector de caras para alimentar el entrenamiento del reconocedor de caras que permitirá hacer login de forma automática en la aplicación. Cuando la aplicación encuentre un sujeto que no pueda reconocer lo añadirá al registro, creando una nueva cuenta de usuario y una escena por defecto vacía. Cuando el sistema sea capaz de reconocer a un usuario iniciará sesión y cargará la última escena creada. \\
\end{itemize}

Para terminar esta fase, se hará una introducción a los mecanismos que permitirán al usuario interactuar con el entorno y sus elementos. De esta forma, se propone detectar colisiones entre objetos de la escena 3D, estableciendo una clasificación de tipos de objetos.\\

En relación a las áreas de trabajo, el módulo en el que se centrará el desarrollo en esta fase será el de Percepción. A partir de ahora será capaz de ofrecer acceso a las imágenes capturadas, así como de analizar las mismas para extraer información. También se añadirán controladores al módulo de aplicación principal, especialmente para incorporar el controlador de navegación; y se realizarán ajustes en los módulos de GUI y Producción.


\section{Análisis}

\subsection{Análisis de Requisitos de Hardware}

Entrando de lleno en el campo de la detección en este capítulo, se comentarán los supuestos sobre los que se espera trabajar. Para ello, se ha tenido en cuenta el ranking de clasificación de Thomas B. M. \cite{THOMASB00}, para sistemas basados en Visión por Computador que puede verse en la figura \ref{F_Typical_Assumptions_Etapa4}, donde se relacionan los supuestos más comunes. \\

Las instalaciones creadas con esta librería se suponen situadas en un entorno controlado. Los paneles de proyección y las cámaras pueden colocarse donde se desee, y pueden ser configuradas para estudiar planos de estudio no frontales a la misma. Sin embargo, las cámaras deben estar fijas durante su funcionamiento. Por otro lado, no es necesario que el fondo sea uniforme, pero sí debe ser estático. También se espera que las condiciones de iluminación sean constantes y adecuadas y, aunque no es imprescindible, se presupone que el usuario se moverá sobre un suelo plano. \\

Se recuerda que puede hacerse uso de cualquier cantidad de cámaras para la captura de datos, así como cualquier cantidad de paneles de proyección que el sistema sea capaz de soportar. Para la realización de este proyecto no se abordará el trabajo sobre imágenes en estéreo, en su lugar las cámaras pueden prepararse para analizar un plano de estudio que no necesariamente sea frontal. Finalmente, dependiendo de las intenciones que se tengan para la implementación de una instalación, hay que tener en cuenta que si una cámara se coloca de forma que capture la imagen proyectada, ésta puede afectar a la sustracción de fondo.

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/typical_assumptions.png}
\end{center}
\caption{ \label{F_Typical_Assumptions_Etapa4} Cuadro de supuestos considerados. Detección, Navegación e Interacción \cite{THOMASB00}}
\end{figure}

\subsection{Análisis de Requisitos de Usuario}

Observamos por un lado la necesidad de configurar las cámaras para hacer uso de las mismas para la percepción. Para ello, se podrán calibrar a través de las opciones de configuración de la aplicación. Se ofrecerán mecanismos para corregir la distorsión de las cámaras, reorientarlas (flip vertical-horizontal) y calcular la homografía si se desea definir un plano de estudio. \\

También existen opciones similares para configurar las ventanas de visualización para ajustarse a su localización. \\

Por otro lado, se hará uso de las técnicas de reconocimiento para dos tareas: primero para el reconocimiento, poder indentificar al sujeto; y segundo para la interacción, donde consideramos la navegación dentro de la escena así como la interacción con los objetos que existan en la misma. Desde este punto de vista se considerará una clasificación de los tipos de objetos. Se tendrán objetos interactivos, capaces de participar en las acciones, ya sea de forma pasiva o activa; por otro lado, se tendrán los objetos no interactivos, que simplemente se dibujan, podrán ser modificados o verse afectados por el entorno pero no tendrán la capacidad de participar en las acciones o interactuar.

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/casosdeuso_tercerademo.png}
\end{center}
\caption{ \label{F_Casos_de_Uso_Etapa4} Diagrama de Casos de Uso. Detección, Navegación e Interacción}
\end{figure}




\subsection{Selección de Herramientas}

La librería principal que se usará para los elementos de Visión por Computador sigue siendo OpenCV. Se usarán las funcionalidades ofertadas por esta librería para todo el proceso de Visión por Computador. Hay que destacar que para la detección facial se han implementado dos alternativas: Por un lado, se incluye la detección simple que implementa por defecto OpenCV, que se basa en un clasificador Haar (cascade), a partir de los trabajos de Paul Viola y mejorado por Rainer Lienhart \cite{Viola01cvpr} \cite{LIENHART}; y por otro, mediante una opción de compilación, se da soporte para la librería de detección de rostros Encara2 ya mencionada \cite{Castrillon05}. De esta forma se mantienen los requisitos software del proyecto, especialmente en relación a la licencia del proyecto. Para la detección de colisiones se ha optado por usar el propio motor de juego de Panda3D que ofrece estas capacidades, lo que evitará mantener dos modelos de forma independiente.

\begin{itemize}
\item Visión por Computador: OpenCV / Encara2
\item Detección de Colisiones: Panda3D
\end{itemize}


\section{Diseño}
\subsection{Diseño de la Aplicación}
Se añaden elementos al módulo IPercept para incorporar los nuevos tipos de detectores. También se realizaran ajustes en el núcleo de la interfaz y los módulos Application e IProd.

\begin{itemize}
\item core: Conjunto de interfaces de la aplicación.
\begin{itemize}
\item IAplicationConfiguration: Se añaden opciones para indicar el uso de reconocimiento de identidad y login automático.
\item IPercept: Interfaz para el módulo de Percepción. Se añaden métodos para acceder a las capacidades de los detectores creados.
\begin{itemize}
\item IPerceptVideo: Ajustes en la interfaz para acceder a las funcionalidades ofrecidas por los nuevos detectores añadidos. 
\begin{itemize}
\item IFaceDetection: Interfaz para los detectores faciales que se quieran implementar. Se ofrecen mecanismos para detectar la existencia o no, así como obtener características.
\item IFaceRecognition: Interfaz para los reconocedores faciales que se quieran implementar. Capacidad para añadir nuevos usuarios al catálogo y de identificarlos a partir de una imagen de la cara.
\item IPresenceDetection: Interfaz para los detectores de presencia que se quieran implementar. Métodos para detectar la existencia o no, así como obtener características.
\item IMotionDetection: Interfaz para los detectores de movimiento que se quieran implementar.
\end{itemize}
\end{itemize}
\end{itemize}
\item igui: Ajustes para añadir opciones de configuración de las cámaras.
\begin{itemize}
\item GUIConfiguration: Se añade un panel de opciones generales, cuya funcionalidad se implementará más adelante, y otro panel de Visualización, mediante el cual se podrá ver información de interés del funcionamiento de los distintos módulos. También se añaden opciones de configuración avanzada en los paneles de configuración de las cámaras.
\item GUIUser: Se añade un cuadro de imagen donde se mostrará la cara que esté siendo estudiada.
\end{itemize}
\item iprod: Se realizan cambios para añadir capacidad de detectar colisiones entre objetos de la escena.
\item vox: 
\begin{itemize}
\item ApplicationConfiguration: Ajustes necesarios para guardar las opciones de reconocimiento y login automático.
\item NavigatorController: Controlador que implementa la navegación del usuario dentro de la escena.
\item Application: Ajustes para automatizar el inicio de sesión haciendo uso del módulo de percepción.
\end{itemize}
\end{itemize}

\subsubsection{Breve Descripción de los Módulos}
Los cambios a realizar afectarán a los módulos de Aplicación, Percepción y Producción.\\

En el primero se introducirá un controlador que permitirá al usuario navegar en la escena presentada. También se añadirá capacidad para, mediante el uso del módulo de percepción, detectar presencia dentro del espacio de estudio y reconocer al usuario facialmente para, de forma opcional, cargar su último escenario creado. \\

En el módulo de percepción, el trabajo se centrará en añadir los primeros detectores al sistema. Se definirán interfaces para poder acceder a las características de interés de los mismos. Se tratará de tres detectores (cara, presencia, movimiento) y de un reconocedor (facial). \\

Finalmente se realizaran los cambios necesarios en el módulo de producción para detectar colisiones entre las entidades que existen dentro de la escena. Se consideran dos tipos de objetos: Interactivos, que a su vez se separan en Activos y Pasivos, y No-Interactivos.

\newpage
\subsubsection{Diseño General en UML}
\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/DiagramadeclasesUML_TerceraDemo.png}
\end{center}
\caption{ \label{F_DiagramadeClasesUML_TerceraDemo} Diagrama de Clases UML, Tercera Demo. Resumen.}
\end{figure}
En la Figura \ref{F_DiagramadeClasesUML_TerceraDemo} se muestran los cambios principales para esta etapa. En esta ocasión, se centra el trabajo en el módulo de Percepción. En concreto se diseñan los módulos e interfaces necesarios para la Detección de presencia, movimiento y caras, así como reconocimiento facial. De esta forma, se añaden los componentes IFaceDetection, IFaceRecognition, IPresenceDetection e IMotionDetection a la interfaz y los correspondientes SimpleFaceDetection, Encara2FaceDetection, FaceRecognition, PresenceDetection y MotionDetection al Módulo de Percepción, formando parte de PerceptVideo. También se añade un controlador a la aplicación con el fin de integrar la navegación del usuario dentro de la escena. Finalmente, los cambios relativos al módulo de Producción no son visibles a este nivel. Se estudiará cada apartado en detalle.

\subsubsection{Diseño en UML - Núcleo de la Interfaz}
\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/DiagramadeclasesUML_TerceraDemo_core.png}
\end{center}
\caption{ \label{F_DiagramadeClasesUML_TerceraDemo_core} Diagrama de Clases UML, Tercera Demo. Interfaces.}
\end{figure}
No existen modificaciones de relevancia en las secciones ya definidas del módulo, a excepción de ciertos ajustes necesarios en IPercept e IPerceptVideo para ofrecer acceso a las nuevas características que se integran. Estos cambios pueden verse reflejados en la Figura \ref{F_DiagramadeClasesUML_TerceraDemo_core}. El trabajo se centrará principalmente en añadir las nuevas interfaces para los detectores: IFaceDetection, IPresenceDetection, IMotionDetection e IFaceRecognition. Cada uno de ellos ofrecerá mecanismos para consultar información relevante sobre el resultado de su análisis. En especial, si detectan o no su objeto de estudio e información relativa al mismo.

\subsubsection{Diseño en UML - Módulo de Aplicación}
\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/DiagramadeclasesUML_TerceraDemo_vox.png}
\end{center}
\caption{ \label{F_DiagramadeClasesUML_TerceraDemo_vox} Diagrama de Clases UML, Tercera Demo. Aplicación.}
\end{figure}

Como se refleja en la figura \ref{F_DiagramadeClasesUML_TerceraDemo_vox}, se añade un controlador de navegación, NavigationController, que permitirá al usuario moverse por la escena. La interfaz del usuario es eminentemente visual, por lo que accederá a los módulos de percepción para detectar y localizar al sujeto, y al módulo de producción para situarlo en la escena 3D. Se recuerda que también será necesario trabajar en este módulo para añadir las tareas de reconocimiento y login automático. Sin embargo, se usarán mecanismos ya disponibles por lo que no serán necesarios cambios en el diseño del módulo.

\subsubsection{Diseño en UML - Módulo de Persistencia}
\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/DiagramadeclasesUML_TerceraDemo_ipersistence.png}
\end{center}
\caption{ \label{F_DiagramadeClasesUML_TerceraDemo_ipersistence} Diagrama de Clases UML, Tercera Demo. Persistencia.}
\end{figure}

Se añaden en la Figura \ref{F_DiagramadeClasesUML_TerceraDemo_ipersistence} los cambios realizados en el Módulo de Persistencia. Como se ha comentado, las entidades que existen en la escena se categorizarán en dos tipos: Interactivos y No-Interactivos. Siendo los primeros los que pueden verse involucrados en las acciones que realiza el usuario u otras entidades, y los segundos los que no. Todas las entidades interactivas podrán responder a las acciones que se llevan a cabo. Sin embargo, sólo las activas provocarán acciones, siendo pasivas las que no. Estas características están definidas en el atributo psique. Se añaden los métodos IsInteractive e IsActive a EntityPersistence para facilitar el acceso.

\subsubsection{Diseño en UML - Módulo de GUI}
\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/DiagramadeclasesUML_TerceraDemo_igui.png}
\end{center}
\caption{ \label{F_DiagramadeClasesUML_TerceraDemo_igui} Diagrama de Clases UML, Tercera Demo. GUI.}
\end{figure}

Se recuerda que el módulo de GUI, incluye los paneles de la interfaz gráfica de la aplicación pero no son sólo paneles de interfaces. Se trata de un conjunto de mecanismos para trabajar sobre las interfaces gráficas, por lo que también atiende peticiones sobre las mismas. \\

Se realizan modificaciones en el panel GUIUser para mostrar la cara del sujeto y añadir login automático, como puede verse en la Figura \ref{F_DiagramadeClasesUML_TerceraDemo_igui}. Se añaden a MainGUI métodos para rellenar las interfaces y ejecutar acciones bajo demanda, como son SetFace, que establece la cara de estudio actual, o FillLoginUserGUI o FillNewUserGUI rellena los campos nombre-contraseña del usuario reconocido o del nuevo usuario a registrar en correspondencia. MainGUI también ofrece mecanismos para mostrar la interfaz que se solicite, así como realizar acciones sobre la misma.

\subsubsection{Diseño en UML - Módulo de Producción}
\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/DiagramadeclasesUML_TerceraDemo_iprod.png}
\end{center}
\caption{ \label{F_DiagramadeClasesUML_TerceraDemo_iprod} Diagrama de Clases UML, Tercera Demo. Producción.}
\end{figure}

En la Figura \ref{F_DiagramadeClasesUML_TerceraDemo_iprod} pueden apreciarse los cambios realizados en el Módulo de Producción. En relación a la Navegación, hay que tener en cuenta de que se trata de varios sistemas independientes, que se ejecutan en sendos hilos y que se comunican entre sí. De esta forma se obtendrán posiciones en determinados momentos, que serán frecuentes pero no ocurrirán en cada iteración. Por ello, para conseguir un movimiento suave, se ha optado por realizar una variación del esquema PDU (Protocol Data Unit) \cite{DEADRECKONING} para la localización y movimiento de objetos. Se estudiará en detenimiento en la sección de Implementación \ref{etapa4Implementacion:produccion}. \\

Para resolver el problema de las colisiones será necesario añadir varios componentes. Por motivos de eficiencia se decide usar colisionadores sencillos asociados a las geometrías de las entidades. Se conservará una relación entre éstas y sus colisionadores. Además, será necesario un buscador de colisiones, así como una cola donde mantener las colisiones detectadas en cada fotograma.

\subsubsection{Diseño en UML - Módulo de Percepción}
\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/DiagramadeclasesUML_TerceraDemo_ipercept.png}
\end{center}
\caption{ \label{F_DiagramadeClasesUML_TerceraDemo_ipercept} Diagrama de Clases UML, Tercera Demo. Percepción.}
\end{figure}

Como puede observarse en la Figura \ref{F_DiagramadeClasesUML_TerceraDemo_ipercept}, se añaden los nuevos detectores al componente PerceptVideo, dentro del módulo de Percepción. PerceptVideo será el responsable de su creación y manejo. Así mismo, ofrece métodos para acceder a la información de los distintos detectores. La información consiste en la posición de la característica detectada, la imagen de la misma así como otros datos asociados. Hay que recordar que las implementaciones de los detectores son independientes de la solución tecnológica elegida en cada caso. \\

Otra consideración es que existirá un detector por cada cámara conectada al sistema. Debido a esto, sumado a que se trata de procesos computacionalmente costosos y que no es deseable mantener el sistema de percepción saturado o bloqueado, se plantea el que cada detector se ejecute en un hilo paralelo. Por ello, entre otras consideraciones, se deben mantener copias independientes para cada detector.

\begin{itemize}
\item SimpleFaceDetection: Implementación del Detector de Caras que utilizará los métodos ya proporcionados por la librería OpenCV. Se basa en un clasificador Haar (cascade), basado en los trabajos de Paul Viola \cite{Viola01cvpr}.
\item Encara2FaceDetection: Implementación del Detector de Caras utilizando la librería Encara2 \cite{Castrillon05}. También necesita un clasificador Haar.
\item PresenceDetection: Implementación del Detector de Presencia, usando la librería OpenCV. Esta librería utiliza un algoritmo de detección de diferencias basado en los trabajos de Liyuan Li et al. \cite{LIYUAN03}. Los atributos que se muestran en la definición son los necesarios para la aplicación del algoritmo.
\item MotionDetection: Implementación del Detector de Movimiento, usando la librería OpenCV que mantiene un historial de imágenes de movimiento.
\end{itemize}
\newpage


\section{Implementación}
Los módulos afectados en la presente fase son los siguiente:

\begin{itemize}
\item core: Añadidas las interfaces en IPercept para los diversos detectores: IFaceDetection, IFaceRecognition, IPresenceDetection, IMotionDetection. \\

También será necesario realizar ajustes de menor relevancia en las interfaces de IApplication, IApplicationConfiguration, IEntityPersistence, IGui, e IProd.
\item igui: Ajustes en GUIUser para mostrar la cara que está siendo objeto de estudio. Se añade un nuevo panel de Visualización a la interfaz de Configuración de la Aplicación.
\item ipercept: Implementación de los detectores Encara2FaceDetection, SimpleFaceDetection, PresenceDetection, MotionDetection y del reconocedor FaceRecognition.
\item ipersistence: Se añaden métodos para consultar el tipo de entidad: Interactivo, No-Interactivo.
\item iprod: Se realizarán ajustes para modificar el movimiento de la cámara, que será accedido con el controlador de navegación, además de los cambios necesarios para realizar la detección de colisiones entre objetos de la escena.
\item vox: Se añade un Controlador de Navegación y se realizan cambios que permitan utilizar reconocimiento, así como inicio de sesión y carga de escenarios de forma automática.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Núcleo de la Interfaz}
$\backslash$src$\backslash$core$\backslash$IPercept$\backslash$video \\

Es necesario recordar que debe mantenerse una interfaz común para cualquier implementación. Por ello, las imágenes se describen con los parámetros necesarios para construirla a partir de un vector de caracteres. Estos parámetros son dimensiones en ancho y alto, número de canales, profundidad, ancho de línea (calculada como Ancho x Alto x NCanales x Profundidad) y finalmente el bloque de datos en sí. \\

También hay que tener en cuenta que cada detector posiblemente realice trabajo altamente costoso computacionalmente, y que además se desea usar uno en cada cámara que se quiera insertar. Por ello se decide que la mejor opción es que se ejecute cada uno en un hilo en paralelo. Por ello, cada detector necesita una copia propia de la imagen para trabajar, ya que el acceso a la misma zona de memoria no sería adecuado. Además, se decide que sea el componente PerceptVideo (el que captura las imágenes y realiza tareas generales de percepción) el que realice las copias y las envíe en cada iteración a los distintos detectores. En caso contrario, seguramente se realizarían demasiadas peticiones de parte de los detectores a este componente, aumentando en gran medida la posibilidad de bloqueo de los hilos solicitantes y reduciendo el rendimiento tanto del detector como del componente PerceptVideo. \\

De esta forma una vez PerceptVideo haya distribuído las imágenes, cada detector realizará su trabajo en paralelo. Por otro lado, para evitar que un detector realice trabajo innecesario se utilizará un flag para indicar cuándo se ha introducido una imagen nueva, de forma que en caso de que no haya una nueva imagen que tratar el hilo vuelva a dormirse. \\

Sin embargo, llegados a este punto surge una pregunta, ¿por qué no usar el patrón Observador-Observable? El motivo es simple: se trata de ejecución en distintos hilos, por lo que en que el fondo esta solución debería hacer lo mismo que la actual, sólo que introduciendo lógica innecesaria: el hilo del observable debería ejecutar su Notify que activaría un flag en el método Update del observador; por otro lado, el observador debería chequear dicho flag para realizar o no su trabajo. Si el Update del Observador tuviera el código que realiza el trabajo, sería el hilo del Observable el que lo ejecutara al llamar a la función, y es precisamente esto lo que se desea evitar.

\begin{itemize}
\item IFaceDetection: Define los métodos que deberán tener todos los detectores de caras que se deseen implementar. Contiene los métodos relacionados con los módulos que se ejecutan en paralelo, en concreto el método Init. Sin embargo, los métodos que lo caracterizan son aquéllos usados para introducir imágenes de estudio, y por otro lado los usados para recuperar información: 
\begin{itemize}
\item FaceDetected: Devuelve verdadero o falso dependiendo de si se ha detectado una cara en la última image procesada.
\item SetCurrentImage: Inserta una nueva imagen de estudio. 
\item GetCopyOfCurrentImage: Devuelve una copia de la última imagen que ha sido procesada, con los marcadores del resultado dibujados sobre ella. 
\item GetCopyOfAreaOfInterest: Devuelve una copia del área de interés, que en el caso de este detector será la mayor cara detectada. Como puede querer usarse para otras tareas se devuelve sin marcadores dibujados.
\item GetFaceCenterPos: Para obtener las coordenadas en píxeles del centro de la cara detectada.
\item GetFaceRec: Devuelve el rectángulo que enmarca la cara detectada.
\item GetFeaturePos: Devuelve la posición de la característica que se desea. La característica se pasa como una cadena de texto, puede indicar valores como 'boca', 'nariz', 'ojo izquierdo' y 'ojo derecho'.
\item Apply: Método interno que realiza el trabajo de detección.
\end{itemize}
\begin{lstlisting}[language=C++]
namespace core{
class _COREEXPORT_ IFaceDetection
{ public:
  virtual ~IFaceDetection(){}
  virtual void Delete()=0;
  virtual void Init()=0;
  virtual bool FaceDetected() = 0;
  virtual void SetCurrentImage(
               const int &size_x, const int &size_y, 
               const int &n_channels, const int &depth, 
               const int &width_step, char * data) = 0;
  virtual char * GetCopyOfCurrentImage(
                 int &size_x, int &size_y, ...) = 0;
  virtual char * GetCopyOfAreaOfInterest(
                 int &size_x, int &size_y, ...) = 0;
  virtual void GetFaceCenterPos(corePoint2D<int> &pos) = 0;
  virtual void GetFaceRec(corePoint2D<int> &corner_a, 
                          corePoint2D<int> &corner_b) = 0;
  virtual void GetFeaturePos(const std::string &feature, 
                             corePoint2D<int> &pos) = 0;
 private:
  virtual bool Apply() = 0; };}
\end{lstlisting}

\item FaceRecognition: En este caso no se trata de un detector, sino de reconocedor. Su objetivo es identificar a un usuario a partir de una imagen facial. \\

Como la identidad del objeto de estudio no es probable que cambie con frecuencia entre fotogramas, se considera que su uso está más destino a ser bajo demanda, por lo que no es necesario que se ejecute como un componente en paralelo. Ofrece mecanismos básicos para solicitar el reconocimiento de una persona dada una imagen de su cara, así como métodos para añadir un nuevo usuario al registro y solicitud de entrenamiento.
\begin{lstlisting}[language=C++]
namespace core{	
class _COREEXPORT_ IFaceRecognition
{ public:
   virtual ~IFaceRecognition(){}
   virtual void Train() = 0;
   virtual int RecognizeFromImage(
               char* data, const int &size_x, const int &size_y, 
               const int &n_channels, const int &depth, 
               const int &width_step) = 0;
   virtual void AddUser(std::vector<core::Image> faces, 
                        const int &user_id) = 0; };}
\end{lstlisting}
\item PresenceDetection: Este detector está enfocado a distinguir si existe una presencia dentro del espacio de estudio o no. Por lo general una presencia se considerará como la existencia de algo que no forma parte del fondo. \\

Hay que tener en cuenta que el fondo puede no ser uniforme, aunque como podía verse en la figura \ref{F_Typical_Assumptions_Etapa4}, página \pageref{F_Typical_Assumptions_Etapa4}, se espera que sea regularmente estático. \\

Los métodos necesarios son similares a los ya vistos en el caso de detector de caras. Se añaden:
\begin{itemize}
\item PresenceDetected: Indica si se ha detectado o no una presencia en la última imagen procesada.
\item GetPresenceCenterOfMass: Para obtener el centro de masas del área detectada como presencia.
\item GetPresenceRec: Para obtener las coordenadas en píxeles del contenedor que encuadra la presencia.
\item TrainBackground: Solicita que se comience a entrenar el detector.
\end{itemize}
\begin{lstlisting}[language=C++]
namespace core{
class _COREEXPORT_ IPresenceDetection
{ public:
   virtual ~IPresenceDetection(){}
   virtual void Delete()=0;
   virtual void Init()=0;
   virtual void SetCurrentImage(
                const int &size_x, const int &size_y, 
                const int &n_channels, const int &depth, 
                const int &width_step, char * data) = 0;
   virtual char * GetCopyOfCurrentImage(
                  int &size_x, int &size_y, 
                  int &n_channels, int &depth, 
                  int &width_step, 
                  const bool &switch_rb = false) = 0;	
   virtual void GetPresenceCenterOfMass(corePoint2D<int> &pos)=0;
   virtual void GetPresenceRec(corePoint2D<int> &corner_a, 
                               corePoint2D<int> &corner_b) = 0;
   virtual bool PresenceDetected() = 0;
   virtual void TrainBackground() = 0;
  private:
   virtual bool Apply() = 0;  };}
\end{lstlisting}

\item MotionDetection: En este caso se desea detectar zonas de la imagen donde exista movimiento. Por el momento sólo obtiene y muestra el movimiento detectado. Será en la siguiente fase, en concreto en interacción avanzada cuando se trabaje en mayor profundidad con este detector.
\begin{lstlisting}[language=C++]
namespace core{
class _COREEXPORT_ IMotionDetection
{ public:
   virtual ~IMotionDetection(){}
   virtual void Delete()=0;
   virtual void Init()=0;
   virtual void SetCurrentImage(
                const int &size_x, const int &size_y, 
                const int &n_channels, const int &depth, 
                const int &width_step, char * data) = 0;
virtual char * GetCopyOfCurrentImage(
                int &size_x, int &size_y, 
                int &n_channels, int &depth, int &width_step, 
                const bool &switch_rb = false) = 0;   };}
\end{lstlisting}
\end{itemize}

$\backslash$src$\backslash$core$\backslash$IApplication \\

Se añaden métodos para ofrecer acceso a funcionalidades de los nuevos módulos. En especial para la configuración de los mismos:
\begin{lstlisting}[language=C++]
...
   virtual bool Calibrate()=0;
   virtual bool CalculateHomography()=0;
   virtual bool TrainBackground()=0;
\end{lstlisting}

$\backslash$src$\backslash$core$\backslash$IApplicationConfiguration \\

Se añaden las opciones de configuración necesarias para los nuevos módulos. Por un lado se tendrán las opciones de visualización para poder mostrar datos de interés de cada uno de los detectores. Por otro, se introducen Setters y Getters para las dos opciones de configuración nuevas 'USE RECON', que indica que se desea realizar reconocimiento, y 'AUTO LOGIN' que indica que se desea realizar login en la aplicación de forma automática.
\begin{itemize}
\item ShowCamCapture: Muestra las imágenes que capturan directamente las cámaras conectadas al sistema.
\item ShowHomography: Muestra el resultado de aplicar la matriz de transformación de que sitúa el plano de estudio de forma frontal a la cámara.
\item ShowFaceDetection: Muestra la imagen de entrada con la localización de la cara detectada resaltada sobre ella.
\item ShowForeground: Muestra una imagen binaria que indica la presencia que se ha detectado.
\item ShowMotion: De igual forma, basada en la imagen de entrada muestra el resultado de realizar el proceso. En este caso se verá resaltado las zonas en las que se detectan movimiento siguiendo el histórico de las imágenes.
\item GetUseRecognition: Indica si se desea utilizar el reconocimiento en la aplicación.
\item GetAutoLogin: Indica si se desea que la aplicación inicie la sesión de forma automática cuando ha detectado a una persona.
\end{itemize}
\begin{lstlisting}[language=C++]
...
   virtual bool GetUseRecognition() = 0;
   virtual bool GetAutoLogin() = 0;
...
   virtual void ShowCamCapture(const bool &value) = 0;
   virtual void ShowHomography(const bool &value) = 0;
   virtual void ShowFaceDetection(const bool &value) = 0;
   virtual void ShowForeground(const bool &value) = 0;
   virtual void ShowMotion(const bool &value) = 0;
   virtual void SetUseRecognition(const unsigned bool &value)= 0;
   virtual void SetUseRecognition(const unsigned bool &value)= 0;
\end{lstlisting}

src$\backslash$core$\backslash$IPersistence$\backslash$IEntityPersistence\\

Se añaden dos métodos para acceder fácilmente al tipo de entidad que se trata, en relación a su capacidad de interacción: IsInteractive e IsActive. \\

Como se ha comentado en las secciones previas, se consideran entidades interactivas aquéllas que provoquen o puedan participar en las acciones de otras. Dentro de estas entidades serán activas, aquéllas que puedan provocar acciones sobre las otras. Las entidades consideradas pasivas sólo podrán reaccionar, no podrán provocar acciones sobre ninguna entidad. \\

src$\backslash$core$\backslash$IGUI$\backslash$IGui\\

Recordemos que el módulo de GUI ofrece opciones para ser manipulado, de forma que se puedan ejecutar acciones sobre la interfaz o indicar qué paneles mostrar. De esta forma se añaden los métodos necesarios para rellenar los campos de usuario y contraseña, tanto para login como para la creación de un usuario nuevo. \\

Para el caso del login automático se añaden los métodos LogIn y LogOut, que desencadenarán las acciones necesarias: es cierto que la aplicación por sí misma puede realizar las operaciones subyacentes al inicio y cierre de sesión, a excepción de los cambios en la interfaz. Por este motivo se opta por encauzar esta acción con la existente del módulo de GUI. La interfaz realizará los cambios necesarios y además, ya que previamente realizaba estas peticiones a la aplicación, cuando era el usuario el que provocaba los eventos al pulsar sobre los botones, solicitará a la aplicación las acciones necesarias para el inicio de la sesión. Se añaden los siguientes métodos a la interfaz de IGui:

\begin{lstlisting}[language=C++]
...
   virtual void LogOut() = 0;
   virtual void LogIn(const std::string &name, 
                      const std::string &passwd) = 0;
   virtual void ShowFaceAtGUI(char* data, 
                const int &size_x, const int &size_y, 
                const int &n_channels, const int &depth, 
                const int &width_step) = 0;
   virtual void FillLoginUserGUI(const std::string username, 
                                 const std::string userpasswd)=0;
   virtual void FillNewUserGUI(const std::string username, 
                               const std::string userpasswd) = 0;
\end{lstlisting}

src$\backslash$core$\backslash$IProd$\backslash$IProd\\

El módulo de producción deberá proveer de un mecanismo para modificar la posición de las cámaras. Será el controlador de la aplicación NavigatorController el que a partir de ahora defina cómo situarlas. Será éste el encargado de definir la forma en la que el usuario se moverá por la escena, y dado que las cámaras representan el punto de vista el usuario lo hará modificando la posición de las mismas. \\ 

Por el momento basta con añadir el método SetCamerasPosition(). Es evidente que el esquema actual de la aplicación introduce elementos que complican el proceso de establecer una nueva posición y actualizar el render. Estos detalles podrán verse en las notas de implementación del módulo de producción.

\begin{lstlisting}[language=C++]
virtual void SetCamerasPosition(const core::corePoint3D<double> &pos)=0;
\end{lstlisting}

En relación a la detección de colisiones por el momento no serán necesarios cambios en la interfaz del módulo.\\

\subsubsection{Módulo de Aplicación}
app$\backslash$vox$\backslash$Application \\

Respondiendo a los cambios introducidos en la interfaz IApplication se implementan los métodos de calibrado, cálculo de homografías y entrenamiento de la detección de fondo. Como puede verse en el cuadro que se muestra a continuación, estos métodos sólo encauzarán la petición al módulo responsable, que en este caso será el módulo de Percepción. \\

\begin{lstlisting}[language=C++]
...
bool Application::Calibrate()
{	if (app_mainpercept)
		app_mainpercept->Calibrate(true);
	return true; }
bool Application::CalculateHomography()
{	if (app_mainpercept)
		app_mainpercept->CalculateHomography();
	return true; }
bool Application::TrainBackground()
{	if (app_mainpercept)
		app_mainpercept->TrainBackground();
	return true; }
\end{lstlisting}

Adicionalmente, se introducen los cambios necesarios para soportar login automático. Esta tarea se ejecuta en la función OnIdle, que se había preparado para capturar los eventos de wxWidgets y destinada a realizar tareas no demasiado exigentes en frecuencia. \\

Lo primero que se realiza es obtener las opciones de configuración de Applicationconfiguration para saber si se debe realizar o no reconocimiento facial y login automático. Si al menos desea utilizarse reconocimiento, el siguiente paso es consultar si se detecta actualmente alguna presencia. \\

En el caso en el que se desee utilizar login automático, pero no se detecte ninguna presencia se cierra la sesión actualmente abierta. \\

\begin{lstlisting}[language=C++]
void Application::OnIdle(wxIdleEvent &event)
{...
 bool use_recon = app_config->GetUseRecognition();
 bool autologin = app_config->GetAutoLogin();
 if(session_controller && app_mainpercept && app_maingui && use_recon)
 { bool session_closed = session_controller->IsSessionClosed();
   bool presence_detected = app_mainpercept->PresenceDetected();
   if(autologin && !session_closed && !presence_detected)
     app_maingui->LogOut();
\end{lstlisting}

Siempre que se desee usar reconocimiento, si la sesión está cerrada y se detecta presencia se consultará si se ha detectado alguna cara en el entorno. Si es así, se mostrará en la interfaz e usuario y se solicitará al módulo de percepción que lo identifique. En caso de éxito en la identificación, se obtendrá la información y rellenarán las interfaces de usuario. En el caso contrario, si no ha sido posible identificar al usuario, se propone la nueva cara detectada como candidata a ser un nuevo usuario. \\

\begin{lstlisting}[language=C++]     
   if(session_closed && presence_detected)
   { bool face_detected = app_mainpercept->FaceDetected();
     if(face_detected)
       img = app_mainpercept->GetCopyOfCurrentFeature("FACE", ...);
     if (app_maingui != NULL)
       app_maingui->ShowFaceAtGUI(img, ...);
     if(face_detected)
     { int user_detected_id = app_mainpercept->RecognizeCurrentFace();
       if (user_detected_id != -1) //known face
       { benefit_of_the_doubt = 0;
         app_mainpersistence->GetUserData(user_detected_id, name, passwd);
         app_maingui->FillLoginUserGUI(name, passwd);
         if (autologin)
         { app_mainpersistence->GetWorldList(name, world_names, world_permissions); 
           app_maingui->LogIn(name, passwd);
           this->RunWorld(world_name);	}	 
        } else { //unknown face
           name = passwd = "anonymous";
\end{lstlisting}

Si durante una cantidad determinada de iteraciones siguientes sigue sin ser posible identificar al sujeto habiéndose detectado una cara, se creará un nuevo usuario en el sistema, asociándole un nuevo escenario vacío por defecto. Adicionalmente, se actualizará la interfaz de usuario. Es necesario mantener esta ventana de incertidumbre debido a que es posible que el reconocedor no sea capaz de detectar al individuo en todas las ocasiones. \\

\begin{lstlisting}[language=C++]            
           if (autologin && app_mainpercept->IsFacePoolReady()
               && (benefit_of_the_doubt > BENEFIT_OF_THE_DOUBT))
           { session_controller->CreateUser(name, passwd);
             session_controller->CreateWorld(wop_world, name, permissions);
             session_controller->CloseSession();
             app_mainpercept->AddNewUserToRecognition(user_id);
             benefit_of_the_doubt = 0;	}
           benefit_of_the_doubt++; 
        }}
...
}
\end{lstlisting}

A partir de ahora, el nuevo usuario ya estaría registrado en el sistema, por lo que formará parte de los usuarios reconocibles. En las siguientes iteraciones el reconocedor debería ser capaz de identificarlo y en tal caso, la aplicación iniciará la sesión y cargará la escena que le fue previamente asociada al nuevo usuario.\\

app$\backslash$vox$\backslash$controllers$\backslash$ConfigurationController \\

Se añaden las nuevas opciones 'USE RECOGNITION', para indicar que se desea utilizar reconocimiento, y 'USE AUTOLOGIN' para permitir el inicio de sesión automático en la aplicación. Como con el resto de opciones de la aplicación, se obtienen del objeto ApplicationConfiguration, se parsean los datos y añaden al fichero de configuración 'config.ini' al ejecutar el método Save(). En el proceso opuesto, al ejecutar Load(), el fichero es cargado y se extrae la información para asignarlo a ApplicationConfiguration. 

\begin{lstlisting}[language=C++]            
bool ConfigurationController::Load()
{ ...
  else if ( tag == "[USE_RECOGNITION]" )
  { bool value;
    std::stringstream wop;
    config_file.getline(str, size);
    wop << str;
    wop >> value;
    app_config->SetUseRecognition(value);
  }
  else if ( tag == "[AUTO_LOGIN]" )
  { bool value;
    std::stringstream wop;
    config_file.getline(str, size);
    wop << str;
    wop >> value;
    app_config->SetAutoLogin(value);
  }
  ...
\end{lstlisting}

app$\backslash$vox$\backslash$ApplicationConfiguration \\

El siguiente paso es Añadir a AplicationConfigration las nuevas opciones de configuración, para obtenerlas y establecerlas. Se tratan de Getters y Setters comunes, idénticos al resto de las opciones de configuración. Se mostrará a modo de ilustración el caso de ShowCamCapture. Para completar la lista se añadirían ShowHomography, ShowFaceDetection, ShowForeground, ShowMotion, GetUseRecognition, GetAutoLogin. Que se implementarían de la misma manera.

\begin{lstlisting}[language=C++]            
virtual void ShowCamCapture(const bool &value)
{ show_capture = value; }
virtual bool IsShownCamCapture()
{ return show_capture; }
\end{lstlisting}

app$\backslash$vox$\backslash$controllers$\backslash$NavigationController \label{F_Fase4TerceraDemo_ControladorNavegacion}\\

La otra línea de trabajo en Application esta relacionada con la navegación. Se desea implementar una solución simple, a modo de introducción, a la navegación; mediante la cual el usuario pueda moverse por la escena usando exclusivamente una interfaz visual. Es decir, el usuario simplemente se moverá por el espacio de la instalación y a partir de técnicas de Visión por Computador, se situarán las cámaras de la escena 3D.\\

Este planteamiento conlleva en realidad tres problemas, que se describirán a continuación:

\begin{itemize}
\item Definición de la mecánica de navegación.
\item Localización del usuario dentro de la escena.
\item Modificación de las cámaras del motor gráfico.
\end{itemize}

Primero: se desea establecer una correspondencia directa, aunque no necesariamente fiel en todo momento, entre la situación del usuario dentro de la instalación con la vista 3D de la escena. Se supondrá el centro del espacio de la instalación identificado con el centro del espacio 3D. Si el usuario de desplaza en algún sentido las cámaras del motor gráfico harán lo mismo. Se usará el centro de masas de la presencia detectada como punto de referencia global del individuo detectado. Se usará además, un vector relativo al centro de masa que nos indique la posición de la cara. En caso de perder la localización de la cara se estimará que se conserva el vector relativo de la última detectada. Se ha optado por esta solución debido a que el centro de masas de la presencia nos ofrece una información más constante del sujeto, mientras que el desplazamiento relativo a la cara nos permitirá colocar las cámaras a la altura de la misma y poder realizar movimientos relativos de cabeceo. Por otro lado, es más probable no poder localizar correctamente la cara del individuo en todo momento. \\

\begin{lstlisting}[language=C++]            
void NavigationController::Iterate()
{ boost::try_mutex::scoped_try_lock lock(m_mutex);
  if ((lock) && (perception) && (production))
  { perception->GetSpaceBoundingBox(space_bounding_box_min, 
                                    space_bounding_box_max);
    presence_detected = perception->PresenceDetected();
    perception->GetHeadPosition(head_pos);
    perception->GetFeaturePosition("CENTER OF MASS", 
                                   presence_center_of_mass);
    com_to_head.x = head_pos.x - presence_center_of_mass.x;
    space_center.x = (space_bounding_box_max.x 
                    + space_bounding_box_min.x)/2;
    ...
\end{lstlisting}

Segundo: Se desea que el usuario pueda desplazarse más allá de los límites espaciales de la instalación. Es decir, es posible que el habitáculo en el que se coloque la instalación tenga unas dimensiones limitadas, por ejemplo, 4x4 metros. Sin embargo, las dimensiones del espacio 3D por el que pueda desplazarse son en principio, indefinidas. En este sentido se establece un umbral en las dimensiones del espacio, cercano al límite definido por el tamaño de la imagen, de forma que si el usuario traspasa este umbral se comenzará a aplicar un desplazamiento constante en la dirección del límite al que se ha acercado, en una cantidad proporcional a la distancia entre la situación del usuario y el umbral establecido. De esta forma, cuanto más se acerque a un borde mayor será la velocidad a la que las cámaras se desplazarán en ese sentido, cuando el usuario deje de traspasar el umbral los movimientos volverán ser exclusivamente locales.\\

\begin{lstlisting}[language=C++]  
  ...
  threshold_distance_to_minmax.x = 
    THRESHOLD*(space_bounding_box_max.x - space_bounding_box_min.x);
  double diff=abs(space_bounding_box_max.x - presence_center_of_mass.x);
  if ( diff < abs(threshold_distance_to_minmax.x))
    space_offset.x += OFFSET_STEP*diff;
  else
  { diff =abs(-1*space_bounding_box_min.x + presence_center_of_mass.x);
    if ( diff < abs(threshold_distance_to_minmax.x))
      space_offset.x -= OFFSET_STEP*diff; }
  ...
  final_cam_pos.x = presence_center_of_mass.x + space_offset.x 
                                              + com_to_head.x;
  production->SetCamerasPosition(final_cam_pos);
\end{lstlisting}

Tercero: Se desea que el movimiento sea suave. Hay que tener en cuenta que se trata de diversos componentes que se ejecutan en paralelo. Por un lado, están las cámaras del motor gráfico, que por defecto se actualizarán sesenta veces por segundo (por ejemplo, en caso de tener activada la sincronización vertical en un monitor de 60Hz). Sin embargo, por otro lado tenemos las cámaras web y los detectores que ofrecerán resultados son una frecuencia considerable. Desgraciadamente, ya sólo el acceso al dispositivo de captura de imágenes es considerablemente costoso (tanto que no es viable mantener la captura y el render en un mismo hilo sin bajar drásticamente la frecuencia de render). Además el proceso de las imágenes también es costoso. \\

Para resolver este último problema se decide usar el mecanismo conocido como Dead Reckoning, sólo que con algunas modificaciones para adaptarlo a nuestro caso. \\

El procedimiento conocido como Dead Reckoning \cite{DEADRECKONING} (deduced reckoning, o navegación por estimación), es usado extensamente en aplicaciones en las que no es posible saber en todo momento la localización real de un objeto, como en el caso de videojuegos en red o en robótica (un caso concreto: el uso de odometría, donde se usa dead reckoning para calcular trayectorias usando un cuentarrevoluciones). \\

Por poner ejemplos cercanos, nos centraremos en el caso de los juegos en red, donde la posición de los otros jugadores nos llega, con suerte cada 90 ó 200 milisegundos. Si comparamos con una frecuencia de render de referencia 60fps estamos hablando de que disponemos de un dato cada 9 o 20 fotogramas dibujados, con suerte. Si dibujáramos a los jugadores de esta forma no seria posible representar un movimiento continuo, y se apreciarían con toda probabilidad saltos constantes. Para resolver este problema, dead reckoning propone enviar más información que la posición actual, por ejemplo, estimaciones de velocidad, aceleración, etc. (en realidad puede incluirse mucha más información: golpe acertado, puntos de vida sustraídos, etc.). De forma que el otro extremo de la comunicación tenga una estimación de lo que debería pasar con el objeto entre un dato certero y otro. Cuanto mayor sea la diferencia de tiempo entre estos datos certeros, mayor es la probabilidad de que el objeto siga una ruta errónea. En tal caso se corregiría la posición del objeto. \\

Precisamente esto provoca un efecto común en aplicaciones de este tipo, concretamente cuando existen latencias altas en la red. Un ejemplo común y que no necesita demasiada latencia sucede en los juegos de tipo Shooter, donde es muy probable que cada jugador vea el cuerpo caído del otro en posiciones distintas. Sin embargo, cada jugador está en un ordenador distinto y la fidelidad de dónde cae cada cuerpo no es realmente relevante (especialmente si nunca ves el monitor de tu contrario). Otro efecto común es ver que los objetos se desplazan de forma constante durante un tiempo prolongado y se repente desaparecen y se colocan en la posición real. Dead reckoning no ofrece información fiel sobre la posición de los objetos. Sin embargo, como los usuarios no tienen la capacidad de contrastar no se considera relevante.

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/dead_reckoning.png}
\end{center}
\caption{ \label{F_Implementacion_TerceraDemo_deadreckoning} Implementación, Tercera Demo. Navegación - Dead Reckoning.}
\end{figure}

Como se ha podido apreciar, este mecanismo nos permite estimar posiciones intermedias entre datos certeros. Sin embargo, también se ha podido ver que existe un problema, especialmente para poder aplicarlo en nuestro sistema: nosotros sí veremos directamente que nuestro movimiento y el estimado por el dead reckoning pueden no ser fieles. La estimación se permitirá a nivel del navegador, en relación a atributos a considerar en el futuro pero no en relación a la velocidad y a la aceleración. Estás estimaciones se harán al nivel del motor gráfico. \\

La solución final a adoptar para resolver la navegación consiste en:

\begin{itemize}
\item NavigatorController mantiene diferentes componentes del movimiento de las cámaras: centro de masas, posición relativa de la cabeza, distancia sobrepasada al umbral.
\item Inicialmente sólo envía al motor gráfico una única coordenada, pero potencialmente se plantea que puede añadir más información sobre la estimación de su comportamiento.
\item Cada vez que el navegador establece la posición certera, el motor gráfico calcula la velocidad y aceleración correspondiente entre el último fotograma y el actual. El motor gráfico interpolará entre las posiciones certeras enviadas por el controlador de navegación usando y velocidad y aceleración. A su vez la velocidad sera también interpolada entre fotogramas a partir de la aceleración calculada entre datos certeros.
\end{itemize}

\subsubsection{Módulo de Persistencia}
scr$\backslash$ipersistence$\backslash$EntityPersistence \\

Los cambios ejecutados se reducen a obtener dos atributos de la entidad. Se trata de saber si la entidad es considerada interactiva, es decir si va a producir acciones o responder a las acciones de otros, o por el contrario no se va ver afectada. Por otro lado, averiguar si sólo va a responder a las acciones o si también será capaz de provocarlas (activa). \\

Estas opciones se encuentran codificadas en el atributo llamado psique que ya posee la entidad y que describe el comportamiento de la misma. Mediante el uso de máscaras binarias es posible extraer de forma rápida información del mismo.

\begin{lstlisting}[language=C++]            
bool EntityPersistence::IsInteractive()
{ return psique & IS_INTERACTIVE;   }
bool EntityPersistence::IsActive()
{ return psique & IS_ACTIVE;   }
\end{lstlisting}

\subsubsection{Módulo de GUI}
scr$\backslash$igui$\backslash$GUIUser \\

Se realizan cambios con el objetivo de mostrar de mostrar la cara detectada que está siendo objeto de estudio actualmente. En caso de que no se detecte ninguna cara o se hayan desactivado las opciones de reconocimiento se mostrará una imagen por defecto.\\

Para hacer esto se mantienen dos mapas de bits. El primero será la imagen por defecto, que se mostrará siempre que no se detecte una cara o no se quiera usar reconocimiento. La segunda, se mostrará en caso de que se desee usar reconocimiento y será la última imagen enviada por la aplicación para su reconocimiento.\\

La primera se cargará al construirse la interfaz, junto con el resto de componentes de la misma, en el constructor de la clase, pero aún no será visible.

\begin{lstlisting}[language=C++]            
GUIUser::GUIUser(...)
{ ...
  no_face_bitmap = new wxBitmap( bitmap.str(), wxBITMAP_TYPE_ANY);
  ... }
\end{lstlisting}

La segunda se cargará cada vez que sea enviada. La imagen se escalará para encajar en el recuadro, y tampoco será visible. En realidad sólo se está cargando o creando el objeto imagen, no el componente gráfico que lo dibuja. Esto se realizará manualmente en la función render().\\

En este caso, cuando nos lleva una nueva imagen de una cara, nos interesa forzar que se actualice la interfaz, ya que si no, no se vería el cambios realizado hasta que wxWidgets lo considerara necesario (por ejemplo, cuando detecta el área dañada al pasar otra ventana sobre ésta). Se usará el método Update para poner la interfaz en la cola de actualización. Sin embargo, esto tampoco sucederá al instante, sino que puede suceder en un período de tiempo prolongado. Para que la actualización sea inmediata es necesario usar el método Refresh justo antes de Update. De esta forma se forzará que la interfaz se redibuje al momento. 

\begin{lstlisting}[language=C++]            
void GUIUser::SetFace(char* data, const int &size_x, 
                      const int &size_y, 
                      const int &n_channels, const int &depth, 
                      const int &width_step)
{ if (data)
  { wxImage new_image = wxImage( size_x, size_y, (unsigned char*)data, true );
    face_bitmap = wxBitmap( new_image.Scale(100, 100, wxIMAGE_QUALITY_HIGH) );
    face_detected = true;
  } else face_detected = false;
  Refresh();
  Update();  }
\end{lstlisting}

Para terminar con esta interfaz, se añaden métodos para rellenar de forma automática los campos de inicio de sesión o de registro de un nuevo usuario. La aplicación actualizará con cierta frecuencia las caras que ve vayan detectando. En el caso de que el usuario fuera reconocido la aplicación rellenará los campos de la parte del login. Si en caso contrario, el usuario no ha sido reconocido durante un periodo de tiempo adecuado y la aplicación considera que debe añadirse uno nuevo, se rellenarán los campos del nuevo usuario. \\

Hay que tener en cuenta que si además de Reconocimiento, la aplicación tiene activado el login automático, ésta iniciará sesión y cargará la última escena creada, por lo que esta interfaz será sustituía por la interfaz de información del usuario (con la lista de escenas y propiedades).

\begin{lstlisting}[language=C++]            
void GUIUser::FillLoginUserGUI(const std::string username, 
                               const std::string userpasswd)
{ if (user_name)
    user_name->ChangeValue(username);
  if (user_passwd)
    user_passwd->ChangeValue(userpasswd);	    }
    
void GUIUser::FillNewUserGUI(const std::string username, 
                             const std::string userpasswd)
{ if (new_user_name)
    new_user_name->ChangeValue(username);
  if (new_user_passwd)
    new_user_passwd->ChangeValue(userpasswd);	
  if (new_user_passwd2)
    new_user_passwd2->ChangeValue(userpasswd);	}
\end{lstlisting}

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/gui_user_recon.png}
\end{center}
\caption{ \label{F_CapturaFase4TerceraDemo_GUIUser} GUI: Panel de Log in}
\end{figure}

scr$\backslash$igui$\backslash$GUIConfiguration \\

En el desarrollo de las demos anteriores ya se habían introducido la mayor parte de los paneles de interfaces gráficas y se habían conectado con la configuración de la aplicación. A través de estos paneles se podían configurar las cámaras y las ventanas de visualización que usará la aplicación.  En este caso, se se añadirán algunas características nuevas. \\

Se añaden dos nuevos paneles a la colección de paneles de la interfaz gráfica de configuración. El primero de ellos será el de opciones generales, que inicialmente contendrá el idioma de la aplicación, ofreciendo un cuadro desplegable que muestre los idiomas disponibles. Actualmente, no se ha implementado este apartado, aunque ya se ha hecho en otras ocasiones. El segundo, que si se ha implementado en esta fase, se corresponderá con las nuevas opciones de visualización, actualmente sólo relacionadas con el módulo de percepción. Al tratarse de meros indicadores para mostrar si se desea o no visualizar una determinada característica se usarán componentes de tipo checkbox. \\

Hay que tener en cuenta que activar estas opciones exigirán más trabajo para mostrar datos actualizados en cada iteración. Se debe tener en cuenta que no se mostrarán todos los resultados a la frecuencia a la que se obtienen. Aun así, activar estas opciones puede provocar un efecto negativo en el rendimiento de los componentes que se vean involucrados.

\begin{lstlisting}[language=C++]            
void GUIConfiguration::InitVisualizationPanel()
{ int width, height;
  GetClientSize(&width, &height);
  visulization_panel = new wxPanel(panel_book, wxID_ANY);
  if (app_config != NULL)
  { visualization_st = new wxStaticText(visulization_panel, wxID_ANY, 
                       _("These options may slow down your computer. "),...);
    cam_capture      = new wxCheckBox(visulization_panel, wxID_ANY, 
                       _("Cam capture"), wxPoint(col, row+20));                       
    homography       = new wxCheckBox(visulization_panel, wxID_ANY, 
                       _("Homography"), wxPoint(col, row+40));
    face_detection   = new wxCheckBox(visulization_panel, wxID_ANY, 
                       _("Face Detection"), wxPoint(col, row+60));
    foreground       = new wxCheckBox(visulization_panel, wxID_ANY, 
                       _("Foreground"), wxPoint(col, row+80));
    motion           = new wxCheckBox(visulization_panel, wxID_ANY, 
                       _("Motion"), wxPoint(col, row+100));	 } }
\end{lstlisting}

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/gui_configurepanel_visualization.png}
\end{center}
\caption{ \label{F_CapturaFase4TerceraDemo_GUIConfiguration_Viszualization} GUI: Panel de Configuración - Opciones de Visualización}
\end{figure}


scr$\backslash$igui$\backslash$MainGUI \\

EL módulo de GUI atenderá peticiones para alternar entre las interfaces de usuario así como para completar información de forma automática y ejecutar acciones. Anteriormente, la interfaz GUIUser respondía a los eventos que provocaba el usuario sobre los botones de Login y Register, que haciendo uso del controlador de la interfaz terminaba solicitando a la Aplicación las acciones correspondientes. \\

Como se había comentado en la sección de interfaces, la Aplicación dispone de los mecanismos subyacentes necesarios para iniciar una sesión y cargar escenas. Sin embargo, no era capaz de modificar las interfaces en consecuencia. Para ello, y por simplicidad se han añadido método para permitir estas acciones que se han encauzado con el flujo de ejecución ya existente. \\

Por otro lado, se han añadido nuevas opciones de configuración que el usuario puede activar a través de los paneles gráficos. Estas acciones también deben dirigirse a sus responsables, que en este caso será la aplicación. \\

De cualquier forma, MainGUI sólo encauzará peticiones a los componentes correspondientes. Se muestran a modo de ejemplo Calibrate(), FillLoginUserGUI y ShowFaceAtGUI. El resto de opciones se implementan de la misma forma:

\begin{lstlisting}[language=C++]            
void MainGui::Calibrate()
{ if (app != NULL)
  app->Calibrate(); }

void MainGui::FillLoginUserGUI(const std::string username, 
                               const std::string userpasswd)
{ if (main_frame)
   main_frame->FillLoginUserGUI(username, userpasswd); }
  
void MainGui::ShowFaceAtGUI(char* data, const int &size_x, 
                            const int &size_y, 
                            const int &n_channels, 
                            const int &depth, 
                            const int &width_step)
{ if (main_frame != NULL)
   main_frame->SetFace(data, size_x, size_y, 
                       n_channels, depth, width_step); }  
\end{lstlisting}


scr$\backslash$igui$\backslash$MainFrame \\

De forma similar, MainFrame sólo redirige las peticiones a los componentes correspondientes, en este caso se actualiza cada interfaz de usuario, en caso de ser necesario. Todas las opciones se implementan de forma similar, por lo que se muestran sólo algunas de ellas como ejemplo.

\begin{lstlisting}[language=C++]            
void MainFrame::SetFace(char* data,const int &size_x,const int &size_y, 
                        const int &n_channels, const int &depth, 
                        const int &width_step)
{ if (user_panel && user_panel->IsShown())
   user_panel->SetFace(data, size_x, size_y, n_channels, ...); }

void MainFrame::FillLoginUserGUI(const std::string username, 
                                 const std::string userpasswd)
{ if (user_panel)
   user_panel->FillLoginUserGUI(username, userpasswd); }  
\end{lstlisting}

\subsubsection{Módulo de Producción}\label{etapa4Implementacion:produccion}

scr$\backslash$iprod$\backslash$MainProd \\

Dentro de este módulo se trabajará en dos apartados distintos: Primero, se verán las modificaciones necesarias para permitir la manipulación de las cámaras y que el controlador de navegación pueda utilizarlo. En segundo lugar, se comprobará cómo añadir detección de colisiones entre las entidades de la escena.

\begin{itemize}
\item Navegación: \\

Como se describe en el apartado de Implementación que hace referencia al controlador de navegación, en la página \pageref{F_Fase4TerceraDemo_ControladorNavegacion}, dicho controlador establecerá la posición de las cámaras con frecuencia. También se comenta que como esta frecuencia será con toda probabilidad menor a la frecuencia de render, se ha tomado como solución realizar una interpolación entre los dos valores certeros inmediatamente anteriores, utilizando estimaciones de velocidad y aceleración para conseguir un movimiento suave. \\

En primer lugar, cada vez que el Controlador de Navegación establece una nueva posición certera se actualizan todos los valores. Se toma el intervalo de tiempo entre dos iteraciones, y a partir del valor actual y los valores previos, se integra para calcular velocidad y aceleración. \\

Los valores de pt0 y pt1 representan la ventana de estudio (o diferencial), pt0 será el instante inicial y pt1 el final (la posición certera actual). El valor de pti representa el último punto interpolado, que pasara a ser el nuevo valor de pt0. \\

A partir de estos dos valores de posición se calcula un nuevo valor de velocidad vel1, siendo vel0 de forma análoga el valor previo más reciente de velocidad. Se establece como velocidad actual la recién calculada. \\

Finalmente, a partir de vel0 y vel1 se calcula el nuevo valor de la aceleración.\\

\begin{lstlisting}[language=C++]            
void MainProd::SetCamerasPosition(const core::corePoint3D<double> &pos)
{ boost::mutex::scoped_lock lock(m_mutex);
  if(initialized)
  { double time = globalClock->get_real_time();
    double interval = time - last_time;
    pt0    = pti;
    pt1.x  = pos.x; 
    vel0   = vel;
    vel1.x = pt1.x - pt0.x;
    vel1.x = vel1.x/interval; 
    vel    = vel1;
    acc.x  = vel1.x - vel0.x; 
    acc.x  = acc.x/interval;
    ...  }}
\end{lstlisting}

Por otro lado, en cada iteración del bucle principal, en cada render, se vuelven a calcular los valores de posición y velocidad, en relación a los valores de velocidad y aceleración actuales respectivamente. Esto es lo que provee el efecto de inercia del movimiento de la cámara. Como detalle, se usa un factor de conversión de unidades espaciales.

\begin{lstlisting}[language=C++]            
void MainProd::DoDoStuff()
{ double time = globalClock->get_real_time();
  double interval = time - last_loop_t; 
  last_loop_t = time;
  vel.x = vel.x + interval*(acc.x);
  pti.x = pti.x + 10*interval*(vel.x);
  for(std::map<int, NodePath>::iterator 
           iter  = windowcamera_array.begin(); 
           iter != windowcamera_array.end(); iter++)
    iter->second.set_pos(pti.x/factor, pti.y/factor, pti.z/factor);  
\end{lstlisting}

Es evidente que esta solución implica un retardo en los valores aplicados a la posición de la cámara, dado que se basan inevitablemente en los valores de instantes anteriores. Sin embargo, la suavidad de movimiento conseguida y el efecto de inercia es interesante y además demuestra ofrecer un comportamiento aceptable. \\

\item Colisiones: \\

A continuación se hablará de la solución adoptada para la detección de colisiones entre entidades de la escena, que igualmente puede ser aplicado para el avatar del usuario. Si bien es cierto que se pueden usar motor de colisiones e incluso de físicas externos, se ha decidido usar el módulo de colisiones ya incorporado en el propio motor de juego de Panda3D. El motivo es que Panda3D ya ofrece una buena solución integrada y para el propósito que se desea estudiar cubre todas las necesidades. Además al usar un sistema ya integrado no será necesario mantener dos esquemas a la vez por separado: el de las entidades por un lado y el las colisiones por otro. \\

Existen varias formas de detectar colisiones: \\

\begin{itemize}
\item Las más potentes están basadas en las geometrías de los objetos. Aunque esto puede ofrecer ventajas evidentes en el detalle al que se pueden producir las colisiones, es una opción muy costosa que hay que usar con moderación. No se descarta su uso futuro para el caso del avatar del usuario pero por el momento no se considerará. \\

\item La otra solución es usar geometrías adicionales llamadas colisionadores, que se añaden como hijas al nodo de la entidad. Estas serán geometrías simples, y por defecto invisibles, que envolverán al objeto y simplificará el cálculo de las mismas. Por ejemplo, en el caso de usar esferas como colisionadores, bastará con calcular la distancia entre los centros de ambas geometrías y comprobar que sea mayor que la suma de sus radios. La mayor ventaja de esta solución es la rapidez de cómputo, que nos puede permitir tener muchas más entidades susceptibles de provocar colisiones que, por ejemplo, en el caso anterior. \\

Recordemos que la escena en los motores gráficos se suele representar como un árbol, en este árbol, los nodos pueden albergar geometrías o matrices de transformación. Por ejemplo, las entidades se pueden ver representadas como un nodo. En este punto, un nodo puede tener descendencia, y es de esta forma que las transformaciones realizadas sobre él se ven reflejadas en todo el subárbol. De esta manera, sólo con añadir la nueva geometría como hija de la entidad, su posición, así como todas las demás transformaciones, se aplicarán sobre la misma (si movemos la entidad, el colisionador se moverá con ella).
\end{itemize}

Por motivos de rendimiento y capacidad potencial de añadir mayor cantidad de entidades susceptibles de colisionar, se ha decidido utilizar la segunda solución como opción por defecto. En concreto se usarán esferas como geometrías de colisión. \\

Llegados a este punto estudiaremos el esquema de colisiones del motor gráfico. Además de las geometrías de colisión, el motor gráfico necesita un objeto que sea capaz de recorrer el grafo de la escena en busca de entidades susceptibles de colisionar. Un objeto que explora el grafo en busca de entidades se conoce como \textit{traverser}, en este caso será uno para colisiones. También sera necesario tener una cola de colisiones, donde el traverser insertará en cada fotograma todas las colisiones detectadas y la información asociada. \\

Un detalle a tener en cuenta es que, aunque Panda3D tiene una de las APIs mejor diseñadas que se han podido ver, y ofrece herramientas y resultados por encima de la media en el campo de los motores libres, se ha comprobado que en la versión disponible para C++ sufre de ciertas carencias. Entre estas, ha demostrado que algunos componentes no están blindados para su uso en sistemas multihilos. Una de las recomendaciones es ejecutar la mayor parte del código posible en el hilo del módulo de producción. También se ha comprobado que algunos módulos no funcionan correctamente en los modos de Debug o Release. Por ejemplo, la detección de colisiones dan problemas en modo Debug.\\

Para crear el explorador y la cola de colisiones se ejecutan las siguientes líneas de código:

\begin{lstlisting}[language=C++]            
void MainProd::DoMainLoop()
{ ...
  #ifndef _DEBUG
  if (!collision_handler_queue)
   collision_handler_queue = new CollisionHandlerQueue();
  if (!collision_traverser)
   collision_traverser = new CollisionTraverser();
  #endif
\end{lstlisting}

Hemos hablado de entidades y de dotarlas de capacidad para detectar colisiones. Sin embargo, no todas las entidades serán susceptibles de provocarlas o recibirlas. Es posible que se desee tener entidades en la escena que no colisionarán, u otras que sólo responderán a las colisiones provocadas por determinados objetos, pero que no sean capaces de provocar las colisiones. Llegado aquí, es importante recordar de nuevo que el cálculo de colisiones es computacionalmente elevado y depende geométricamente del número de objetos que se consideran capaces de generar colisiones, ya que es necesario comprobar cada uno con todos los demás. Si todas las entidades provocaran colisiones seria de orden exponencial. \\

La solución que se suele adoptar, y que Panda3D también incorpora es separar los colisionadores en 2 tipos: Por un lado, están los colisionadores capaces de provocar una colisión, lo que llaman 'fromObjects'; y por otro lado, están todos las demás, incluídos los 'fromObjects'. A esta segunda colección la llaman 'intoObjects'. \\

En cada iteración el traverser consultará cada elemento que exista en 'fromObjects' con todos los elementos que existan en 'intoObjects'. Evidentemente interesa mantener el número de objetos del primer grupo lo más bajo posible. En su forma más común, sólo existirá un objeto en fromObjects, que será el propio avatar del usuario, pero otros objetos son susceptibles de ser fromObjects, como por ejemplo la munición disparada por un arma. Hay que tener en cuenta que si dos objetos de 'intoObjects' que no pertenecen a 'fromObjects' colisionan, no se detectará. \\

Para añadir una geometría de colisión a una entidad de la escena, basta con crear un objeto de tipo colisionadores y añadirlo como hijo al nodo de la entidad.

\begin{lstlisting}[language=C++]            
 CollisionSphere *coll_solid = new CollisionSphere(0,0,0,radius);
 CollisionNode *coll_node = new CollisionNode(nombre);
 coll_node->add_solid(coll_solid);
 NodePath coll_nodePath = entity->GetNodePath()
                                ->attach_new_node(coll_node);
\end{lstlisting}

Con esto, la geometría de colisión ya formará parte de los objetos en 'intoObjects'. Si además se desea insertarlo en la colección 'fromObjects', bastará con añadirlo como tal en el collision-traverser:

\begin{lstlisting}[language=C++]            
 collision_traverser->add_collider(coll_nodePath, 
                                   coll_handler_queue);
\end{lstlisting}

Dentro de la catalogación de entidades que se adopta en el proyecto 'intoObjects' se corresponderá con las entidades Interactivas, y 'fromObjects' from las entidades Activas. Para simplificar la creación de geometrías de colisión, y asegurar que se ejecutan siempre en el hilo del módulo de producción, la carga de entidades (que se ejecuta en el hilo principal) añadirá las nuevas entidades a un vector que se comprobará en el método de comprobación de colisiones. Será aquí donde se creen las geometrías y se inserten en fromObjects o intoObjects en caso de necesidad.\\

\begin{lstlisting}[language=C++]            
void MainProd::CheckCollisions()
{ collision_traverser->traverse(pandawin_array[1]->get_render());
  for (int i = 0; i < coll_handler_queue->get_num_entries(); i++)
   cout << "Entry: " << coll_handler_queue->get_entry(i) << "\n";
  for (std::vector< Prod3DEntity * >::iterator iter =
            entity_collidable_array_to_register.begin(); 
            iter != entity_collidable_array_to_register.end();
            iter++)
  { if( coll_traverser && (*iter)->IsInteractive ) //intoObject
    { NodePath* np = (*iter)->GetNodePath();
      PT(BoundingSphere)bs=DCAST(BoundingSphere, np->get_bounds());
      int radius = bs->get_radius();
      CollisionSphere *coll_solid = new CollisionSphere(0,0,0,radius);
      CollisionNode *coll_node = new CollisionNode(nombre);
      coll_node->add_solid(coll_solid);
      NodePath coll_nodePath = (*iter)->GetNodePath()
                                      ->attach_new_node(coll_node);
      coll_nodePath.show(); //shows the collider's geometry
      entity_collider_array[(*iter)] = collision_node;
      if ( (*iter)->IsActive() ) //also a fromObject
       coll_traverser->add_collider(coll_nodePath, coll_handler_queue);
... }
\end{lstlisting}
\end{itemize}

\subsubsection{Módulo de Percepción}\label{Etapa4_Impl:Percepcion}

Llegamos al último módulo en que se han realizado cambios durante esta fase. Derivado de los cambios realizados en el apartado de implementación de las interfaces, se implementarán los  métodos correspondientes tanto a nivel de MainPercept como de PerceptVideo. El objetivo de estos métodos es dar acceso a las nuevas funcionalidades que aportarán los detectores que se desarrollarán. 

scr$\backslash$ipercept$\backslash$MainPercept \\

Se añaden los métodos que permitirán acceder a los datos producidos por los diversos detectores, así como para poder  configurar los mismos. MainPercept no añadirá ninguna lógica especial, sólamente encauzará la petición a los componentes responsables, en este caso PerceptVideo.

\begin{lstlisting}[language=C++]            
namespace core{ namespace ipercept
{ class MainPercept : public core::IPercept
  { public:
    ...
    virtual void Calibrate(const bool &value);
    virtual void CalculateHomography();
    virtual void TrainBackground();
    virtual bool MainPercept::PresenceDetected();
    virtual bool MainPercept::FaceDetected();
    virtual void GetHeadPosition(corePoint3D<double> &result);
    virtual void GetFeaturePosition(const std::string &feature, 
                                    corePoint3D<double> &result);
    virtual void GetSpaceBoundingBox(corePoint3D<double> &min, 
                                     corePoint3D<double> &max);
    virtual char * GetCopyOfCurrentFeature(const std::string &feature, 
                   int &size_x, int &size_y, int &n_channels, 
                   int &depth, int &width_step, 
                   const bool &switch_rb = false);
    virtual int  RecognizeCurrentFace();
    virtual void AddNewUserToRecognition(const int &user_id);
    virtual bool IsFacePoolReady();
\end{lstlisting}

A modo de ilustración se muestra el caso de la implementación del método Calibrate(). El resto se realiza de forma similar.

\begin{lstlisting}[language=C++]            
void MainPercept::Calibrate(const bool &value)
{ if (perceptVideo_module != NULL)
  perceptVideo_module->Calibrate(value); }
\end{lstlisting}

scr$\backslash$ipercept$\backslash$PerceptVideo \\

Recordemos que PerceptVideo es el componente principal del módulo de Percepción, responsable de las tareas relacionadas con el procesado de imágenes. Se encarga de obtener imágenes de los dispositivos de captura, así como de mantener y gestionar los distintos procesos o detectores que se incluyan. Desde este punto de vista, PerceptVideo también encauzará las peticiones a los componentes responsables, pero además también realiza operaciones de forma genérica y necesita mantener cierta gestión. \\

Se separarán los componentes en elementos de proceso general y detectores/reconocedores. Los primeros realizan preproceso o postproceso sobre las imágenes para realizar correcciones o permiten transformar datos sobre ellas, a las que llamaremos Herramientas. Los segundos están destinados a obtener características concretas de las imágenes realizando un estudio sobre las mismas, a las que llamaremos Detectores. \\

En Herramientas localizamos la Captura de Imágenes (ya implementada en fases anteriores), la Calibración y el Cálculo de las matrices de homografía. Para cada cámara conectada al sistema existirá una instancia de los siguientes componentes: Captura de Imágenes, Calibración, Cálculo de Homografías, Detector de Caras, Presencia y Movimiento. Por otro lado, sólo existirá un único Reconocedor de Caras. \\

\begin{itemize}
\item Calibración: \\

Se trata de corregir la deformación que sufre la imagen debido a las características del dispositivo de captura. En la siguiente figura puede verse la expresión de transformación de perspectiva, que convertiría un punto del espacio 3D al espacio 2D, y que se describe según la implementación de OpenCV.

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/formula_calibrate_intrinsics.png}
\end{center}
\caption{ \label{F_Formula_IPercept_Calibrate_Intrinsics} PerceptVideo: Transformación de perspectiva. }
\end{figure}

En ella puede verse cómo convertir un punto (X,Y,Z) en el espacio tridimensional, al correspondiente (u,v) del espacio imagen, 2D. El primer detalle que nos interesa es la matriz A, llamada matriz de parámetros intrínsecos, y que efectivamente describe los parámetros intrínsecos de la cámara. los valores de fx, fy representan la distancia focal, siendo cx, cy los valores del punto principal del foco (comúnmente, el centro de la imagen). Siempre que la lente sea fija o no se use el zoom, estos parámetros permanecerán constantes. \\

La matriz $R|t$, representa los parámetros extrínsecos y es la conjugación de transformaciones de Rotación-Traslación, representa la transformación afín entre un sistema de coordenadas y el otro.  \\

La expresión anterior puede expresarse también como:

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/formula_calibrate_2.png}
\end{center}
\caption{ \label{F_Formula_IPercept_Calibrate_2} PerceptVideo: Transformación de perspectiva 2. }
\end{figure}

Sin embargo, las lentes reales añaden cierta distorsión que principalmente es radial, pero también existe una componente tangencial. Añadiendo estos coeficientes la expresión quedaría de la siguiente forma:

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/formula_calibrate_intrinsics2.png}
\end{center}
\caption{ \label{F_Formula_IPercept_Calibrate_2} PerceptVideo: Transformación de perspectiva 3 - coeficientes de distorsión }
\end{figure}

Los valores k1, k2, k3 son los coeficientes de distorsión radiales, y p1, p2 los coeficientes de distorsión tangenciales. Al igual que la distancia focal, son valores propios de la cámara que no van a cambiar y se incluyen en el conjunto de coeficiente intrínsecos de una cámara, \cite{CALIBCV}. \\

La tarea de la calibración consiste en calcular estos valores, para posteriormente aplicarlos y corregir la distorsión que produce la cámara sobre la imagen. Como son valores estáticos que no cambiarán, se pueden guardar en ficheros de configuración que se cargarán al inicio de la aplicación.

\begin{lstlisting}[language=C++]            
PerceptVideo::PerceptVideo(IApplicationConfiguration *app_config_)
{ ...
  intrinsics[i+1] = (CvMat*)cvLoad( intrinsics_filename );
  distortion[i+1] = (CvMat*)cvLoad( distortion_filename );
  capture_img[i+1] = cvQueryFrame(capture_cam_array[i+1]);
  undistort_mapx[i+1] = cvCreateImage( cvGetSize(capture_img[i+1]),
                                       IPL_DEPTH_32F, 1);
  undistort_mapy[i+1] = cvCreateImage( cvGetSize(capture_img[i+1]), 
                                       IPL_DEPTH_32F, 1);
  if( (intrinsics[i+1]) && (distortion[i+1]) )
   cvInitUndistortMap( intrinsics[i+1], distortion[i+1], 
                       undistort_mapx[i+1], undistort_mapy[i+1] );  
\end{lstlisting}

Sin embargo, el cálculo de los parámetros es un proceso elaborado que requiere estudiar una sucesión de imágenes capturadas y el uso de un patrón (tablero de ajedrez), para ser capaz de obtener puntos de referencia y estudiar su relación. A continuación se mostrará un resumen. \\

En primer lugar se establecen los parámetros del patrón a detectar. Se trata de un tablero de ajedrez de nueve filas por seis columnas. Esta forma nos permitirá no solo poder detectar un patrón reconocible sino también poder definir la orientación del mismo, ya que no sería simétrico.\\

Dentro de un bucle se pedirá al usuario que muestre el tablero y se irán añadiendo los puntos del espacio imagen y los puntos del objeto estudiado, por cada tablero detectado con éxito. \\

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/calibrating_chessboard.png}
\end{center}
\caption{ \label{F_IPercept_Calibrate_Chessboard} PerceptVideo: Marcadores Tablero }
\end{figure}

\begin{lstlisting}[language=C++]  
if( corner_count == board_n )
{ step = successes*board_n;
  for( int i=step, j=0; j < board_n; ++i, ++j ){
   CV_MAT_ELEM( *image_points, float, i, 0 ) = corners[j].x;
   CV_MAT_ELEM( *image_points, float, i, 1 ) = corners[j].y;
   CV_MAT_ELEM( *object_points, float, i, 0 ) = j/board_w;
   CV_MAT_ELEM( *object_points, float, i, 1 ) = j%board_w;
   CV_MAT_ELEM( *object_points, float, i, 2 ) = 0.0f; }
  CV_MAT_ELEM( *point_counts, int, successes, 0 ) = board_n;
  successes++; }
\end{lstlisting}

Se inicializan las matrices de valores intrínsecos con una distancia focal igual a 1 milímetro. A continuación se calculan los coeficientes y finalmente se almacenan en un fichero de configuración, para que la próxima vez que se inicie la aplicación sean cargados. Finalmente, se crean los mapas de corrección de distorsión que serán necesarios para corregir la misma en las imágenes capturadas.

\begin{lstlisting}[language=C++]            
 CV_MAT_ELEM( *intrinsic_matrix, float, 0, 0 ) = 1.0;
 CV_MAT_ELEM( *intrinsic_matrix, float, 1, 1 ) = 1.0;
 cvCalibrateCamera2( object_points2, image_points2, 
                     point_counts2, cvGetSize( image ), 
                     intrinsic_matrix, distortion_coeffs, 
                     NULL, NULL, CV_CALIB_FIX_ASPECT_RATIO ); 
intrinsics[iter->first] = intrinsic_matrix;
distortion[iter->first] = distortion_coeffs;
intrinsics_filename << app_config->GetRootDirectory() 
                    << "Intrinsics" << iter->first << ".xml";
cvSave( intrinsics_filename.str().c_str(), intrinsic_matrix );
undistort_mapx[iter->first] = cvCreateImage( cvGetSize( image ), 
                                             IPL_DEPTH_32F, 1 );
undistort_mapy[iter->first] = cvCreateImage( cvGetSize( image ), 
                                             IPL_DEPTH_32F, 1 );
cvInitUndistortMap( intrinsic_matrix, distortion_coeffs, 
                    undistort_mapx[iter->first], 
                    undistort_mapy[iter->first] ); 
\end{lstlisting}

Antes de analizar una imagen en el Módulo de Percepción, se le aplica la corrección de distorsión. En cada iteración se captura una imagen por cada cámara web conectada. Inmediatamente después se le aplica la trasformación de forma que el procesado futuro se haya con las imágenes corregidas. Para aplicar la transformación, basta con aplicar los mapas de corrección de distorsión, como se muestra a continuación. Siendo 't' la imagen de estudio distorsionada de origen e 'image' la imagen destino. De hecho, PerceptVideo mantiene un vector con la última imagen capturada por la cámara correspondiente. Esta misma imagen es la que se usará como base en los trabajos posteriores, y como se ha comentado ya tendrá la corrección aplicada.

\begin{lstlisting}[language=C++]
cvRemap( t, image, undistort_mapx[iter->first], 
                   undistort_mapy[iter->first] );  
\end{lstlisting}

Antes de concluir este apartado se comenta la forma en la que se visualizarán los datos de interés de los componentes de este módulo de Percepción. Todos los componentes cuyos resultados nos interese visualizar insertarán ventanas en un vector de ventanas de depuración. Cuando el usuario indica que desea mostrar alguna en cuestión, esta venta se crea y se volcarán las imágenes en ellas, y si no se destruyen. Por motivos de eficiencia es en cada iteración, dentro de la función ShowDebugWindows() donde se recorren las ventanas que se desean mostrar, se solicitan las imágenes a los componentes correspondientes y se actualizan en pantalla. Hay que tener en cuenta que es posible que la petición a los componentes no sea atendida, debido a que se hace uso de hilos no bloqueantes para la sincronización, en tal caso la imagen se conservará como la del fotograma anterior.


\begin{lstlisting}[language=C++]
void PerceptVideo::ShowDebugWindows()
{ for (std::map<int, CvCapture *>::iterator 
       iter = cam_array.begin() ...)
  { IplImage * image = capture_img[iter->first];
    if(show_cam_capture)
    { std::map< std::string, CamWindow* >::iterator cam_iter =
                       camWindow_array.find(window_name.str());
      cam_iter->second->ShowImage(image);  }//already undistorted
\end{lstlisting}

\item Cálculo de Homografías:\\

Una homografía es una matriz de transformación invertible que nos permite convertir coordenadas de un plano de proyección real al plano de proyección de estudio. Establecerá una coalineación entre un punto de un plano en el espacio 3D a un punto en el espacio imagen. La gran ventaja es que la misma matriz puede ser usada en ambos sentidos.

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/homography_diagram.png}
\end{center}
\caption{ \label{F_IPercept_Calibrate_Homography_Diagram} PerceptVideo: Homografía \cite{HOMOGF}}
\end{figure}

Mediante el uso de homografías es posible estudiar la situación de un objeto en planos no frontales a la cámara corrigiendo la deformación de la perspectiva de la vista. Sin embargo, hay que tener en cuenta que deformará la imagen capturada y que debe usarse teniendo en cuenta esta consideración. \\

El análisis se hará siempre en el espacio imagen. Una vez realizado, se convertirán las coordenadas de imagen del punto de estudio en cada caso, con el espacio transformado. \\

Por el momento, la solución adoptada para el alcance actual de este proyecto no contempla visión en estéreo, que será una línea interesante que se propone como trabajo futuro. En cambio a través del panel de configuración de las cámaras es posible asignar qué plano de estudio corresponde a cada cámara. La intención es hacerlo de una forma más genérica, pero actualmente se podrá asignar un plano que corresponderá a cada cara del cubo que envuelve el espacio de la instalación. \\

Se usará la homografía para corregir la perspectiva dado que probablemente el plano de estudio no coincida completamente con el de la imagen (por ejemplo al colocar una cámara frontal a cierta altura). \\

Según las condiciones asumidas en tabla comentada en \ref{F_Typical_Assumptions_Etapa4}, página \pageref{F_Typical_Assumptions_Etapa4}, las cámaras permanecerán estáticas durante el desarrollo de la instalación. Teniendo en cuenta esta característica, una vez calculas las matrices de homografía de las distintas cámaras será posible almacenarlas en un fichero de configuración que ahorrará tener que volver a calcularlas en próximas ejecuciones. \\

\begin{lstlisting}[language=C++]
 homographies_filename << app_config->GetRootDirectory() 
                       << "Homography" << i+1 << ".xml";
 homography[i+1] = (CvMat*)cvLoad( homographies_filename );
\end{lstlisting}

El cálculo de la homografía se realiza de forma similar al caso anterior. Se entrará en un bucle donde se localizará un patrón de ajedrez, cuyas características han sido definidas. Hay que tener en cuenta que es relevante poder identificar la orientación. \\


En cada iteración se buscará el patrón de tablero de ajedrez y se dibujarán marcadores. Además de los marcadores tradicionales se añaden unas circunferencias para que el usuario sea consciente de la orientación del plano. \\

\begin{lstlisting}[language=C++]
while( successes < n_boards )
{ if( frame++ % board_dt == 0 ) {
 int found = cvFindChessboardCorners( image, board_sz, corners, 
             &corner_count,  CV_CALIB_CB_ADAPTIVE_THRESH | 
             CV_CALIB_CB_FILTER_QUADS );
 cvCvtColor( image, gray_image, CV_BGR2GRAY );
 cvFindCornerSubPix( gray_image, corners, corner_count, 
               cvSize( 11, 11 ), cvSize( -1, -1 ), 
               cvTermCriteria( CV_TERMCRIT_EPS+CV_TERMCRIT_ITER, 
               30, 0.1 ));
 cvDrawChessboardCorners(image,board_sz,corners,corner_count,found);				
 origin.x = corners[0].x; origin.y = corners[0].y;
 xpoint.x = corners[board_w-1].x; xpoint.y = corners[board_w-1].y;
 ypoint.x = corners[(board_h-1)*board_w].x; 
 ypoint.y = corners[(board_h-1)*board_w].y;
 xypoint.x = corners[(board_h-1)*board_w + board_w-1].x; 
 xypoint.y =  corners[(board_h-1)*board_w + board_w-1].y;
 cvCircle(image,origin,20,red,6);
\end{lstlisting}

A continuación se establece la correspondencia entre los puntos del espacio imagen (esquinas del tablero) y espacio transformado (valores definidos internamente). Se crea la matriz que albergará la homografía, y se utiliza la función de OpenCV cvFindHomography(), que calculará los valores de la misma a partir de la relación de puntos entre los dos espacios.

\begin{lstlisting}[language=C++]
 CvPoint2D32f objPts[4], imgPts[4];
 H = cvCreateMat( 3, 3, CV_32F);
 CvMat _pt1, _pt2;
 _pt1 = cvMat(1, 4, CV_32FC2, &objPts[0] );
 _pt2 = cvMat(1, 4, CV_32FC2, &imgPts[0] );
 cvFindHomography(&_pt1, &_pt2, H, CV_RANSAC, 5);
 if(homography[iter->first])
   cvReleaseMat(&homography[iter->first]);
 homography[iter->first] = H; 
\end{lstlisting}

Finalmente, se compondrán las coordenadas obtenidas por las distintas cámaras. Para que esto pueda ser viable, es necesario usar un mecanismo de forma que la coordenada del centro de referencia del patrón sea siempre el centro del espacio de la instalación. \\

Para aplicar la homografía basta con multiplicar las coordenadas por la matriz de transformación. Por lo general, el uso será siempre calcular las coordenadas transformadas a partir de las coordenadas del espacio imagen. \\

A continuación, se muestra cómo realizar esta operación, donde imgPts contiene los valores del punto que se tiene en espacio imagen y objPts la variable donde se guardará el resultado.

\begin{lstlisting}[language=C++]
 pt1 = cvMat( 3, 1, CV_32F, &objPts );
 pt2 = cvMat( 3, 1, CV_32F, &imgPts );
 if (homography[index])
 { cvMatMulAdd(homography[index], &pt2, 0, &pt1); }
\end{lstlisting}

Para terminar con esta herramienta se ilustra un ejemplo de cómo aplicar la matriz de homografía al conjunto de la imagen.

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/homography_example.png}
\end{center}
\caption{ \label{F_IPercept_Calibrate_Homography_Diagram} PerceptVideo: Homografía - Ejemplo}
\end{figure}

\end{itemize}

scr$\backslash$ipercept$\backslash$video$\backslash$SimpleFaceDetection \\

A continuación se describirá la solución que vendrá por defecto en el proyecto. Se trata de la implementación del Detector de Caras que utilizará los métodos proporcionados directamente por la librería de OpenCV. El detector de caras de OpenCV se basa en un clasificador Haar, basado en los trabajos de Paul Viola y mejorado por Rainer Lienhart \cite{Viola01cvpr} \cite{LIENHART}. \\

Aunque no es el objetivo analizar ni mejorar estos algoritmos se considera interesante comentar en qué consiste su funcionamiento. \\

Con el fin de mejorar la eficiencia en los procesos de detección de objetos, es común utilizar un proceso de detección de características del objeto de estudio. Esto es, reducir la información de la que se dispone a un conjunto de atributos. Por ejemplo, en el caso de caras humanas, una codificación de los contrastes existentes en la imagen y sus relaciones espaciales. A estas características se las conoce como Haar, ya que se procesan de forma similar a los coeficientes que se usan en las transformadas wavelets de Haar. \\

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/haar_features.png}
\end{center}
\caption{ \label{F_IPercept_SimpleFAceDetection_HaarFeatures} PerceptVideo: Características Haar \cite{LIENHART} \cite{KBERGGREN} \cite{AHARVEY}.}
\end{figure}

Para ello, será necesario disponer de un clasificador. Este clasificador se entrenará con imágenes positivas, es decir, imágenes que representan al objeto de estudio; y a continuación con imágenes negativas, otras imágenes arbitrarias donde no aparezca el sujeto. Una vez que el clasificador ha sido entrenado será posible contrastar una nueva imagen (del tamaño de la imagen de entrenamiento), de forma que el clasificador será capaz de distinguir si corresponde con el objeto que se busca. En el caso práctico en el que se dispone de una imagen de una escena, que puede contener un sujeto, el procedimiento consiste en utilizar una ventana deslizante que avanza por el espacio de la imagen, utilizando el clasificador para comprobar si dicha ventana (sub-imagen) contiene al sujeto. De hecho, es el clasificador sobre el que se realiza el escalado, en lugar de la imagen, ya que es más eficiente de esta forma. \\

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/scale_window_classifier.png}
\end{center}
\caption{ \label{F_IPercept_SimpleFaceDetection_Scale} PerceptVideo: SimpleFaceDetection - Escalado convolución}
\end{figure}

Otro detalle a tener en cuenta, es que en realidad el clasificador consiste en una colección de una serie de clasificadores más simples que se aplican a las distintas subregiones de interés. Por este motivo se suele llamar clasificador en cascada a este tipo de clasificadores. \\

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/cascade.png}
\end{center}
\caption{ \label{F_IPercept_SimpleFaceDetection_Cascade} PerceptVideo: SimpleFaceDetection - Clasificador en cascada \cite{Viola01cvpr}.}
\end{figure}

En el caso que nos ocupa se usarán los clasificadores que ya provee OpenCV, por lo que no será necesario realizar la fase de entrenamiento. Lo primero que hará un detector será cargar este clasificador.

\begin{lstlisting}[language=C++]
 cascade_name << "./haarcascade_frontalface_alt2.xml";
 cascade = (CvHaarClassifierCascade*)cvLoad(cascade_name,0,0,0);
\end{lstlisting}

El detector se ejecutará en un hilo aparte y permanecerá dormido hasta que el módulo de Percepción de Vídeo le envíe una nueva imagen para estudiar. Cuando esto sucede se aplicará el clasificador con el objetivo de detectar al sujeto dentro de la imagen. \\

Para ello, el primero paso consiste en inicializar algunos atributos y preparar las imágenes que se van a usar. Entre estas tareas de preprocesado se convierte la imagen de estudio a blanco y negro y se ecualiza para estandarizar el brillo y el contraste, por motivos de simplicidad y eficiencia, además de para reducir el impacto que diferencias en la iluminación pueden introducir.

\begin{lstlisting}[language=C++]
 cvCvtColor(img, gray, CV_BGR2GRAY);
 cvResize(gray, small_img, CV_INTER_LINEAR);
 cvEqualizeHist(small_img, small_img);
\end{lstlisting}

A continuación se aplica el detector y se extraen las ocurrencias detectadas dentro de la imagen. Adicionalmente se dibuja un marco alrededor de la cara detectada.

\begin{lstlisting}[language=C++]
if(cascade)
{ CvSeq * faces = cvHaarDetectObjects(small_img, cascade, 
                       cvCreateMemStorage(0), haar_scale, 
                       min_neighbors, haar_flags, min_size);
  int x, y, w, h, n;
  if (faces)
  for( int i = 0; i < (faces ? faces->total : 0); i++ )
  {  IplImage *aux_img = face_img;
	 face_img = Sub_Image(img, cvRect( faceRec_a.x, faceRec_a.y,
	            faceRec_b.x-faceRec_a.x, faceRec_b.y-faceRec_a.y ));
				if (aux_img) cvReleaseImage(&aux_img);
...
\end{lstlisting}

Se muestra un ejemplo del funcionamiento del detector.

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/simplefacedetection_example.png}
\end{center}
\caption{ \label{F_IPercept_SimpleFaceDetection_example} PerceptVideo: SimpleFaceDetection - Detección de caras}
\end{figure}

De la misma forma que todos los detectores ofrece métodos para la obtención de datos, así como de imágenes. Estos datos son, de forma genérica, las coordenadas del centro del objeto detectado, y las coordenadas de las esquinas que definen el rectángulo que lo enmarca. Estos atributos se obtienen en cada iteración cuando clasificador devuelve los datos del objeto detectado, actualizándose en ese momento. Se muestra a continuación cómo se obtiene el caso de las coordenadas del centro a modo de ilustración. El resto de peticiones se atienden de forma similar.

\begin{lstlisting}[language=C++]
void SimpleFaceDetection::GetFaceCenterPos(corePoint2D<int> &pos)
{ boost::try_mutex::scoped_lock lock(m_mutex);
  pos.x = faceCenterPos.x;
  pos.y = faceCenterPos.y; }
\end{lstlisting}

En concreto, devolverá la imagen completa con la cara resaltada, así como recortes de la cara en cuestión. 

scr$\backslash$ipercept$\backslash$video$\backslash$Encara2FaceDetection\label{Encara2Details} \\

En este apartado veremos la aplicación del detector  \cite{Castrillon05}, desarrollado por Modesto Castrillón et al. Los sistemas de detección de caras pueden clasificarse de varias formas. Una de estas clasificaciones los separan según el tipo de conocimiento que utilizan: implícito o explícito. \\

Los detectores que usan conocimiento implícito se centran exclusivamente en la imagen. Entrenan un clasificador con un conjunto de imágenes, teniendo en cuenta un conjunto de restricciones de escala y orientación. Es una solución de fuerza bruta, lenta pero robusta, que sin embargo, ignora información que podría acelerar la detección. \\

El segundo grupo de detectores se centran en el uso de información explícita acerca de la estructura y apariencia de determinadas características obtenidas a través del conocimiento de la experiencia humana (por ejemplo, proporción ojos-boca). Estos casos pueden ofrecer una detección rápida en entornos con determinadas restricciones. \\

El enfoque de Encara2 es unificar estos dos tipos de información. Primero, selecciona posibles candidatos utilizando conocimiento explícito, para luego aplicar un análisis basado en conocimiento implícito más rápido. \\

De igual forma que el clasificador estudiado en la implementación anterior, se trata de un clasificador en cascada.

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/cascade_encara.png}
\end{center}
\caption{ \label{F_IPercept_Encara2FaceDetection_Cascade} PerceptVideo: Encara2FaceDetection - Clasificador en cascada}
\end{figure}

Aunque no es el objetivo analizar ni mejorar esta solución al problema de la detección de caras, se considera interesante comentar brevemente su funcionamiento. El clasificador tomará una hipótesis y avanzará por los clasificadores para confirmarla o rechazarla:

\begin{itemize}
\item Seguimiento: Si existe una detección previa se buscan características en zonas cercanas (ojos y comisuras de la boca). Si las posiciones detectadas son próximas se supera el test.
\item Selección de una Posible Cara: se realiza una búsqueda de zonas de la imagen rectangulares por aproximación de color, que puedan contener piel.
\item Detección de Características Faciales: Se elimina de forma heurística elementos de la imagen que no sean de interés (por ejemplo, cuello) y envuelve la posible imagen de cara dentro de una elipse para ignorar el resto de la imagen. En este módulo también se hace una detección temprana en busca de característica que responden a las interrelaciones de elementos geométricos o de apariencia (por ejemplo, ojos).
\item Normalización: Para reducir el problema de la dimensionalidad al buscar caras a distintas escalas, el tamaño del rostro detectado se normaliza a una dimensión de 59x65 píxeles, con 25 píxeles de distancia interocular.
\item Confirmación: A modo de confirmación o descarte, para eliminar posibles falsos positivos se realiza un análisis basado en conocimiento implícito: Se realiza un análisis de componentes principales (PCA) sobre los ojos, se realizan pruebas sobre el conjunto de la imagen y finalmente se estima la localización de la boca y nariz.
\end{itemize}

La aplicación de esta librería muestra una mejora notable tanto en la rapidez como en el caso de detecciones correctas de caras al procesar vídeo. Modificaciones realizadas sobre esta librería, permite además obtener otras características interesantes como la edad o el sexo. \\

Su aplicación es similar al caso anterior, aunque más simplificado. Primero se crea una nueva instancia del detector.

\begin{lstlisting}[language=C++]
 ENCARAFaceDetector = new CENCARA2_2Detector(ENCARAdataDir,320,240);
\end{lstlisting}

De igual forma que resto de detectores, el hilo permanece dormido hasta que PerceptVideo le asigna una nueva imagen a estudiar. Esto se lleva a cabo mediante la activación de un flag cada vez que PerceptVideo envía una nueva imagen, que evitará que el hilo vuelva a dormirse y en su lugar procese la nueva imagen enviada. \\

En la siguiente iteración que se ejecute se solicitará al detector que procese la imagen y se obtiene la mayor cara detectada, en caso de haber alguna. A partir de ella, se obtendrá la información que nos interesa: posición del centro de la cara y coordenadas del rectángulo que la enmarca. También es posible obtener otras características ya comentadas, pero que por el momento no se usarán. \\

\begin{lstlisting}[language=C++]
bool Encara2FaceDetection::Apply()
{ ...
  ENCARAFaceDetector->ApplyENCARA2(h,2);
  CFacialDataperImage * facial_data = ENCARAFaceDetector->GetFacialData();
  int face_index = facial_data->GetLargestDetectedFace();
  if (facial_data && (face_index != -1))
  { facial_data->Faces[face_index]->Gender;
    faceRec_a.x = facial_data->Faces[face_index]->x1;
    faceCenterPos.x = (faceRec_a.x + faceRec_b.x) / 2;
    ENCARAFaceDetector->PaintFacialData(h,CV_RGB(0,255,0));
\end{lstlisting}

A continuación se muestra un ejemplo de detección con este componente. \\

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/encara_sample.png}
\end{center}
\caption{ \label{F_IPercept_Encara2FaceDetection_Sample} PerceptVideo: Encara2FaceDetection}
\end{figure}


scr$\backslash$ipercept$\backslash$video$\backslash$FaceRecognition \\

Este componente no será un detector sino un reconocedor. El problema que nos ocupa no es localizar un posible sujeto dentro de una imagen, sino dado un sujeto, ser capaz de identificarlo. \\

Para resolver este problema se ha hecho uso de las herramientas proporcionadas por la librería OpenCV. Para ello, se utilizará el algoritmo clásico de Análisis de Componentes Principales (PCA) \cite{TURKPENTLAND}. PCA es la solución más simple y rápida para el reconocimiento de objetos. Sin embargo, tiene ciertas debilidades que es necesario tener en cuenta: variaciones en traslación, escala, fondo, así como cambios en la iluminación puede provocar que el reconocer no sea capaz de identificar una cara ya registrada. También demuestra problemas con cambios como presencia/ausencia de accesorios, como gafas o barba. \\

Sin embargo, es una buena aproximación y cubre las necesidades que se tienen en este proyecto. \\

Al igual que en el detector de caras por defecto, se usará uno de los clasificadores que provee OpenCV.

\begin{lstlisting}[language=C++]
 cascade_name << "./haarcascade_frontalface_alt2.xml";
 cascade = (CvHaarClassifierCascade*)cvLoad(cascade_name,0,0,0);
\end{lstlisting}

Algunos de los atributos más relevantes de este componente son:

\begin{itemize}
\item Vector de caras: Imágenes de las caras de los usuarios a reconocer.
\item Vector de caras principales (eigenfaces): Imágenes de diferencias usada para reconocer.
\item Cara promedio: Media de todas las caras en la colección.
\item Índice: Utilizado para mantener una relación entre una cara y un identificador de usuario.
\end{itemize}

El sistema deberá registrar a cada usuario sobre el que se desee realizar reconocimiento. Si no se dispusiera de información de cada usuario no sería posible realizar identificación. Es un clasificador que también requiere entrenamiento, pero sólo será necesario cada vez que se añada un nuevo usuario. \\

Cuando aparece un nuevo usuario que se desea añadir al registro, se introducirá una serie de imágenes del mismo. Estas imágenes habrán sido tomadas por PerceptVideo, que mantiene una colección de caras recientes. El primer paso es preprocesarlas. Como se explicaba en el caso anterior, por simplicidad, velocidad y para disminuir el impacto de efectos de la iluminación, las imágenes son preprocesadas: se convierten a escala de grises, se escalan a un tamaño de estudio por defecto y se ecualizan para estandarizar el brillo y el contraste. Estos procedimiento son comunes y ya han sido descritos. 

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/preprocess.png}
\end{center}
\caption{ \label{F_IPercept_FaceRecognition_Preprocess} FaceRecognition: Preprocess}
\end{figure}

Se comentará un detalle en relación a la escala. Se utilizarán método distintos para agrandar frente a encoger, debido a que cada método ofrece mejores resultados en ese caso, y peores en el contrario. \\

\begin{lstlisting}[language=C++]
 cvCvtColor( zimage, face_gray, CV_BGR2GRAY );
 
 if (face_width > size.width && size.height > size.height)
  cvResize(face_gray, scaled, CV_INTER_LINEAR);
  //CV_INTER_CUBIC or CV_INTER_LINEAR, good for enlarging
 else
  cvResize(face_gray, scaled, CV_INTER_AREA);
  //good for shrinking, but bad at enlarging.

  equalized = cvCreateImage( desired_size, IPL_DEPTH_8U, 1);
  cvEqualizeHist( scaled, equalized );
\end{lstlisting}

PCA implica dos consideraciones: por un lado podrá contrastarse una imagen dada contra el clasificador y obtener la probabilidad de acierto/similitud con alguna de las caras almacenadas. Por otro lado, debe existir esta base de datos para poder aplicar  PCA. \\

Ya se ha comentado que cuando se registra un nuevo usuario se añade una colección de imágenes suyas. El siguiente paso es volver a entrenar el conjunto completo, por lo que se purga a información actual y se vuelve a calcular. \\

Para poder diferenciar entre una cara y otra, el algoritmo calcula una media entre todas las imágenes añadidas a la colección, la anteriormente mencionada cara promedio. A partir de esta imagen se calcularán las caras principales, de forma que la primera de ellas mostrará las diferencias más dominantes frente a la cara media, la segunda representa diferencias menos dominantes, y así sucesivamente. Observando las caras principales, puede apreciarse que las de primer orden mantienen características reconocibles, mientras que las de orden mayor son cada vez más ruidosas.

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/eigenfaces.png}
\end{center}
\caption{ \label{F_IPercept_FaceRecognition_Eigenfaces} FaceRecognition: Caras principales}
\end{figure}

De la misma forma que una imagen se puede descomponer en sus caras principales, también es posible recomponerla. En este procedimiento es en lo que se basa el algoritmo de PCA para reconocer una imagen, obtener el conjunto de las caras principales que en proporción se asemejan más a la cara que se desea reconocer. \\

En resumidas cuentas tenemos dos procesos, un primero donde recolectamos datos y entrenamos el clasificador. Para ello, se calcularán las componentes y vectores principales (eigen-values y eigen-vectors), y a continuación se aplicará la descomposición de cada cara del catálogo para obtener todos los coeficientes de descomposición.

\begin{lstlisting}[language=C++]
 cvCalcEigenObjects( face_vector.size(), (void*)face_vector_pp,
                	(void*)eigen_vector_pp, CV_EIGOBJ_NO_CALLBACK, 
                	0, 0, &term_criteria, average_image, 
                	eigen_values->data.fl);
 ...
 for(int i=0; i < face_vector.size(); i++)
 { float *coeffs = projected_trainning_faces->data.fl;
   coeffs +=  i*offset;
   cvEigenDecomposite( face_vector[i], eigen_vector.size(), 
                       eigen_vector_pp, 0, 0, average_image, coeffs);
\end{lstlisting}

Para cada consulta que se realice, volverá a seguirse algunos de estos pasos. Primero se preprocesará la imagen y a continuación se descompondrá a partir de los valores calculados durante el entrenamiento. Una vez se tenga la imagen proyectada se comparará para obtener la imagen candidata más similar; y si encuentra alguna que supere el umbral de similitud establecido se buscará en el índice que nos identifica una cada con un id de usuario.

\begin{lstlisting}[language=C++]
int FaceRecognition::RecognizeFromImage(char* data, ...)
{ cvEigenDecomposite(equalized, eigen_vector.size(), 
                    eigen_vector_pp, 0, 0, average_image, 
                    projected_test_face);
  candidate = FindBestCandidate(projected_test_face, likeness);
  return (candidate != -1) ? 
                   facepic_to_person_indexes->data.i[candidate] 
                   : -1;
...

int  FaceRecognition::FindBestCandidate(float *projected_test_face_, 
                      float &likeness, const bool &use_mahalanobis)
{ double min_distance = DBL_MAX;
  int candidate = -1;
  for(int trainning_face = 0; 
      trainning_face < face_vector.size(); trainning_face++)
  { double distance = 0;
    for(int eigen = 0; eigen < eigen_vector_size; eigen++)
    { double aux_dis = projected_test_face[eigen] 
                     - projected_trainning_faces->data
                     .fl[trainning_face*eigen_vector_size+eigen];
    if (use_mahalanobis)
         distance += aux_dis * aux_dis / eigen_values->data.fl[eigen];
    else distance += aux_dis * aux_dis;
   }
   if ( distance < min_distance )
   { min_distance = distance;
     candidate = trainning_face; }}

   likeness = 1.0f - sqrt( 
              min_distance / (float)(face_vector.size() 
                                   * eigen_vector_size) ) / 255.0f;
   return ((likeness > 0.75f) ? candidate : -1);
}                   
\end{lstlisting}

Llegado a este punto sólo nos resta guardar todos los datos a un fichero que se pueda recuperar de disco, que cargará al inicio de la aplicación y se que guardará cada vez que se vuelva a entrenar el clasificador. Este fichero estará guardado en formato XML, donde se guardarán todos los datos necesarios, como un par etiqueta-valor:

\begin{lstlisting}[language=C++]
 int face_vector_size = 
     cvReadIntByName(file_storage, 0, "face_vector_size", 0);
 int eigen_vector_size= 
     cvReadIntByName(file_storage, 0, "eigen_vector_size", 0);
 facepic_to_person_indexes = 
    (CvMat *)cvReadByName(file_storage,0,"facepic_to_person_indexes",0);
...	
 for(int i = 0;  i < face_vector_size; i++)
 { face_vector.push_back((IplImage *)cvReadByName(file_storage, 0, 
                                      index, 0));	}
\end{lstlisting}

scr$\backslash$ipercept$\backslash$video$\backslash$PresenceDetection \\

El detector de presencia funciona de forma similar al resto de los detectores. Se ejecutará en un hilo aparte y permanecerá dormido mientras no le sean enviadas imágenes nuevas que procesar. Una vez le sea enviada una nueva imagen procederá a su análisis para detectar elementos que no formen parte del fondo. \\

Tras su implementación se ha comprobado que es uno de los detectores que más problemas ha dado. Es costoso computacionalmente, tanto en proceso como en memoria. Además también requiere entrenamiento, pero se verá afectado por cambios en las condiciones ambientales más difíciles de controlar. Sin embargo, se considera válido al tratarse de una instalación donde el entorno permanece controlado. Revisando la tabla de condiciones que se asumen \ref{F_Typical_Assumptions_Etapa4}, página \pageref{F_Typical_Assumptions_Etapa4}, sólo habrá un individuo cada vez en la instalación y el fondo aunque no será uniforme si será eminentemente estático. Por otro lado, las condiciones de iluminación también serán controladas. \\

Para poder definir un fondo de imagen, es necesario crear un modelo que sea capaz de discriminar si un píxel forma parte del fondo o del sujeto en primer término. Para ello se usa un Modelo de Composición de Gaussianas (Gaussian Mixture Model). GMM es un tipo de modelo de densidad probabilístico expresado como una suma ponderada de componentes gaussianos de densidad. Se utilizan comúnmente para obtener la distribución de probabilidad de medidas continuas de características. Puede verse más fácilmente en la Figura \ref{F_IPercept_GaussianMixtureModel}.

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/gmm.png}
\end{center}
\caption{ \label{F_IPercept_GaussianMixtureModel} FaceRecognition: Gaussian Mixture Model \cite{DReynolds}}
\end{figure}

De cualquier forma, utilizaremos este modelo para entregar el detector durante una serie de iteraciones y que sea capaz de clasificar si un píxel determinado forma parte del fondo. Durante este entrenamiento no debe aparecer ningún sujeto en el espacio de la instalación. \\

El esquema es bastante simple de utilizar. Por una parte se crea y se le asigna a una serie de parámetros de configuración. A continuación bastará con actualizar el modelo en cada iteración con la imagen capturada. A partir del modelo podemos obtener la imagen de figura, es decir, aquello que no forma parte del fondo.

\begin{lstlisting}[language=C++]
 bg_trainning_model = 
  cvCreateGaussianBGModel(latest_bkg ,background_params);
 ...
bool PresenceDetection::Apply()
{...
 int train_flag = (train) ? -1 : 0;
 cvUpdateBGStatModel( image, bg_trainning_model, train_flag); 
 background_model->foreground;
\end{lstlisting}

Para obtener la imagen de figura, se comparará la imagen capturada en cada fotograma con el modelo de fondo y se discriminarán los píxeles que forman parte del fondo o del primer término. La instalación está dirigida a que sólo existe un elemento dentro de ella, el usuario, por lo que se establecerá una relación directa. Finalmente, existe presencia si se detecta una cantidad considerable de píxeles clasificados como tal. \\

Finalmente, a partir de la imagen de figura pueden obtenerse una cantidad interesante de características mediante el uso de Momentos. 

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/moments.png}
\end{center}
\caption{ \label{F_IPercept_PresenceDetection_Moments} PresenceDetection: Momentos \cite{JOHANNESK}.}
\end{figure}

Con el momento de orden 0, p = q = 0, (M00) puede verse inmediatamente que se obtiene el área de la imagen: para cada fila y se suman todos los píxeles con valor 1, y luego se acumulan todas estas secciones obteniendo el área. Por otro lado, aunque no se ve de forma tan directa, con los momentos de orden 1, pueden obtenerse las coordenadas del centro de masas del objeto (M10 daría la componente x, y M01 la componente y). Hay que tener en cuenta que es aconsejable aplicar erosión antes de calcular los momentos para eliminar ruido que pueda estar presente en la imagen.

\begin{lstlisting}[language=C++]
 cvErode(background_model->foreground, eroded, 0, 3);
 cvMoments(eroded, &foreground_moments);
 double area = foreground_moments.m00;
 if(area)
 { presenceCenterPos.x = foreground_moments.m10/area;
   presenceCenterPos.y = foreground_moments.m01/area;		}
\end{lstlisting}

A continuación se muestra un ejemplo de la aplicación de este componente.

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/presencedetection_example.png}
\end{center}
\caption{ \label{F_IPercept_PresenceDetection_Example} PresenceDetection: Example}
\end{figure}

\vspace{20 mm}

scr$\backslash$ipercept$\backslash$video$\backslash$MotionDetection \\

Finalmente, el último de los detectores implementado es un detector de movimiento. De la misma forma que los componentes mostrados anteriormente, existe una instancia por cada cámara que estudie la instalación y por motivos de rendimiento se ejecutará en un hilo paralelo a al ejecución del resto. En el caso de que no tenga nuevas imágenes que procesar, el hilo permanecerá dormido, y cuando PerceptVideo le envíe una nueva imagen procederá a su análisis. \\

En este caso, a diferencia de los anteriores, no será necesario disponer de ningún tipo de datos de entrenamiento ya que el análisis se realizará en una ventana temporal correspondientes a las últimas imágenes más recientes. \\

Para realizar este estudio se seguirá un modelo MHI (Motion History Image) \cite{DAVIS97}, es decir se mantendrá un histórico de las imágenes que representan el movimiento a partir de una silueta. También se calcularán los gradientes de movimientos, que indican la dirección de movimiento de cada píxel.

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/silueta_mhi.png}
\end{center}
\caption{ \label{F_IPercept_MotionDetection_MHI} MotionDetection: Silueta - Motion History Image - Motion History Gradients}
\end{figure}

Para calcular estas siluetas y obtener el histórico, se creará una imagen limpia con valores cero y se aplicará el método UpdateMHI(). Se le pasan como parámetros la imagen de estudio y la imagen donde se desea almacenar el resultado. El tercer parámetro indica el umbral que se desea utilizar para filtrar las diferencias entre fotogramas.

\begin{lstlisting}[language=C++]
void MotionDetection::Process()
{ if( !motion )
  { motion=cvCreateImage(cvSize(width,height),8,3);
    cvZero( motion );
    motion->origin = image->origin;  }
    UpdateMHI( image, motion, 30 ); 
\end{lstlisting}

A continuación se obtendrá una marca de tiempo actual, que se usará cada vez que se desee actualizar el historial. También se mantiene un buffer donde almacenar la secuencia de imágenes. En cada iteración se inserta la nueva imagen capturada al final del historial y se calcula la diferencia con la imagen del fotograma anterior, se filtra con el umbral, y se actualiza el historial de imágenes y el de gradientes. 

\begin{lstlisting}[language=C++]
void  MotionDetection::UpdateMHI( 
      IplImage* img, IplImage* dst, int diff_threshold )
{ double timestamp = (double)clock()/CLOCKS_PER_SEC;
  cvAbsDiff( buf[idx1], buf[idx2], silh ); //diff between frames
  cvThreshold( silh, silh, diff_threshold, 1, CV_THRESH_BINARY );
  cvUpdateMotionHistory( silh, mhi, timestamp, MHI_DURATION );
  cvCalcMotionGradient( mhi, mask, orient, MAX_TIME_DELTA, 
                        MIN_TIME_DELTA, 3 );
  
\end{lstlisting}

A partir de este análisis es posible recuperar las distintas áreas de interés, de forma que se puede obtener un análisis de movimiento a nivel global o local de cada segmento analizado. Sin embargo, esto entra dentro de la consideración de técnicas avanzadas de detección que se estudiará en el desarrollo de la siguiente fase. \\

Se muestra a continuación ejemplos de resultado del análisis de movimiento utilizando este componente.

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/motion_example.png}
\end{center}
\caption{ \label{F_IPercept_MotionDetection_Example} MotionDetection: Example}
\end{figure}

\section{Validación y Publicidad}
\subsection{Validación}
\begin{itemize}
\item Comprobación de uso de recursos de la máquina mediante las herramientas del sistema. En la máquina en la que se desarrolla la aplicación muestra consumir una media de un 19\% de CPU, con 27 subprocesos asociados y una media de 200Mb de memoria con el escenario por defecto de prueba. Al cargar los distintos escenarios se ven cambios dependiendo de la complejidad de los mismos, lo cual es esperado. También es esperado el aumento de consumo de la CPU debido a los nuevos detectores añadidos. Sin embargo, se aprecia que la carga de trabajo se mantiene balanceada entre las CPUs del equipo. 
\item Respecto a a la comprobación de la ejecución y cierre correctos, así como pérdidas de memoria, se han detectando dos bugs de relevancia:\\
\\
El primero consistía un assert espúreo localizado en el módulo de detección, en especial el detector de movimiento, sólo durante de la carga de la aplicación. En concreto durante la creación de un hilo de boost. No se pudo averiguar el motivo ni encontrar documentación relacionada. Sin embargo, durante la depuración se hicieron cambios, incluyendo nuevos fuentes y modificando las opciones de compilación. Tras realizar estos cambios no se ha podido volver a reproducir el error. \\
\\
El segundo se trata una pérdida de memoria de tamaño moderado, pero no localizado. Probablemente relacionado con los detectores o el sistema de colisiones. Deben localizarse y resolverse. Sin embargo, con el fin de no retrasar el avance del proyecto y como se ha conseguido alcanzar el resto de objetivos, se propone resolverlo durante el desarrollo de la siguiente fase.
\item Uso de herramientas para medición de fotogramas por segundo para comprobar el rendimiento de la ventana de visualización. No se aprecian cambios en la tasa de fotogramas por segundo que se mantiene a 60fps para dos ventanas de visualización. Teniendo en cuenta que 60 es el límite máximo impuesto por la sincronización vertical del monitor. Frente a los primeros esquemas de detectores, la solución finalmente adoptada ofrece una buena tasa de resultados por segundo. 

\end{itemize}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Etapa 5: Interacción Avanzada y Creación de Contenidos}\label{Etapa5}

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/blocks_full_etapa5.jpg}\label{F_DiagramaCompleto}
\end{center}
\caption{ \label{F_BloquesResumen_Etapa5} Componentes sobre los que se va a trabajar.}
\end{figure}

A lo largo de las demos anteriores se perseguía el objetivo de ofrecer las herramientas necesarias para la percepción y creación de elementos tridimensionales, incluyendo un inicio de interacción. Con estos mecanismos ya en marcha se procederá a la creación de contenidos, de forma que el usuario ya pueda crear entornos según sus acciones. Para ello, un usuario podrá crear elementos, destruirlos o provocar reacciones con los mismos, creando de esta forma su propio entorno.
\\

Primero, separaremos el trabajo a realizar en dos campos: Interacción avanzada y Creación de contenidos.
\\

\begin{itemize}
\item \textbf{Interacción Avanzada:} Se añaden nuevos mecanismos de interacción que vayan más allá de la navegación. Ya no se usará sólo la posición del usuario, sino también información relativa a sus movimientos, de forma que pueda interactuar con los elementos existentes en la escena. Se estudiará la colisión de su cuerpo con los objetos de la escena, así como se recopilará la información obtenida por el módulo de percepción para realizar cálculos estadísticos en relación a su actividad, como la excentricidad de su cuerpo (si mantiene una postura muy redondeada o más recta de pie). También se estudia cómo construir una malla 3D de su cuerpo, así como otras representaciones que nos permitan calcular colisiones y obtener datos con ese punto de la colisión como velocidad o dirección de movimiento. Estas acciones afectarán a qué tipo de contenidos se crean en el mundo.
\item \textbf{Creación de contenidos:} Hasta el momento se han usado escenas de prueba para comprobar el funcionamiento de la aplicación y sus capacidades. Sin embargo, la intención de la aplicación que se desea construir es crear el contenido en tiempo de ejecución, a partir de las acciones del usuario. De esta forma, se habilitarán mecanismos para crear, modificar o destruir elementos del entorno. Además según su comportamiento se creará un perfil, que llamaremos 'Psique' que servirá como punto principal para la composición. Afectará al audio y a los objetos que se crean. 
\end{itemize}

Un punto de especial interés en la creación de contenidos, es que se desea crear elementos de vida artificial. No todos los objetos creados serán entidades pasivas, esperando la interacción del usuario. Algunos de ellos tendrán un comportamiento propio, siendo capaces de moverse y perseguir sus propios objetivos.
\\

Se incluye también en la creación de contenidos el apartado de la composición musical. La composición procedimental es un campo de gran interés. Aunque no es el objetivo realizar un estudio profundo sobre la misma, se ha decidido añadir ciertas componentes. Basándose en la psique, que refleja el histórico de las acciones realizadas, se establece la estructura armónica de la composición, su ritmo y estilo. Se definen tres elementos que conformarán la composición: una base rítmica, una melodía de fondo y unas decoraciones. 
\\

Con la finalización de esta fase, el SDK y la aplicación se pueden considerar terminada, de cara a los objetivos que se querían conseguir, cerrando el ciclo de las técnicas que se deseaban explorar en el desarrollo del trabajo. Sin duda, tanto el SDK como la aplicación pueden enriquecerse y existen diversas líneas de trabajo futuro muy interesantes. Por ejemplo: crear composiciones musicales procedimentales más complejas, hacer uso de análisis y síntesis de audio o extender la interfaz con el motor gráfico para añadir más capacidades. Con la aparición del dispositivo XBOX Kinect, sería interesante implementar un nuevo módulo de percepción que use esta tecnología, y comprobar su integración en el sistema. En cualquier caso, nos detendremos en este apartado al llegar a la sección de trabajo futuro.

\section{Consideraciones Previas}

El proyecto ha sufrido un paréntesis en el que el desarrollo se ha detenido. Debido a esto y al reconocimiento de que el alcance ideado inicialmente podía ser demasiado extenso, se han tomado decisiones para acotar la dimensión del mismo. De esta forma, se ha planteado simplificar tanto los mecanismos de interacción avanzada como de creación de contenidos, y las mejoras se añaden como posible trabajo futuro. También se relajará la exigencia y concentración sobre el estudio de múltiples fuentes y salidas para esta propuesta de instalación, centrándonos en el uso de una única cámara y ventana de visualización, y se limitará el trabajo relacionado con audio a su generación en el Módulo de Producción.

\section{Análisis}
\subsection{Análisis de Requisitos de Hardware}
Aunque la arquitectura del SDK soporta un número indeterminado de cámaras y ventanas de visualización, se relajará la exigencia y concentración sobre este apartado para la consecución de la propuesta de instalación, centrándonos en el uso de una única cámara y ventana de visualización.

\subsection{Análisis de Requisitos de Usuario}
A partir de ahora el usuario no sólo se moverá por el entorno, sino que interactuará con el mismo. Para ello, será necesario obtener, además de los datos generales como su posición absoluta o movimiento global, estudiar su cuerpo. El usuario tendrá una representación dentro de la escena, no necesariamente visible, dentro del entorno 3D que reflejará sus acciones. \\

Aunque es realmente interesante el estudio del reconocimiento postural, se ha optado por una solución más simple. Crearemos una representación del usuario basada en una nube de puntos simplificada, nos permitirá representar al usuario así y proyectar su interacción con la escena.\\

Se usará la sustracción de fondo para obtener un modelo del usuario. Añadiendo los datos de detección de movimiento obtendremos la velocidad de movimiento de un punto concreto del avatar. Sin embargo, no habrá distinción explícita de extremidades ni postura. Aunque se considera un apartado muy interesante para trabajo futuro. \\

La interacción del usuario se puede clasificar en dos secciones:

\begin{itemize}
\item \textbf{Interacción basada en colisión:} A partir del modelo del usuario podemos proyectar su presencia en profundidad. Cada elemento del modelo del avatar del usuario conserva datos de forma que es posible conocer, por ejemplo, la dirección y velocidad de ese segmento. Si la velocidad de esa colisión tiene una magnitud baja se considerará que el avatar está en contacto con la Entidad que toca y, en nuestra instalación, decidiremos que esa acción provocará una reacción y creará un enjambre de entidades. Sin embargo, la intensidad del movimiento es elevada lo consideraremos un golpe, la entidad reaccionará de una forma más enérgica, creará otro tipo de enjambre diferente y morirá. 
\item \textbf{Interacción basada en comportamiento:} Las acciones del usuario provocarán efectos personalizados. Se utilizan para ellos dos conceptos nombrados como 'Psique' y 'Energía'. El valor de psique puede moverse entre 0-Bondad, 1-Neutral y 2-Maldad. Crear entidades moverá al usuario hacia el valor 0-Bondad, golpearlas para destruirlas la moverá hacia el valor 2-Maldad. Por otro lado, se prepara un esquema para poder obtener valores estadísticos del comportamiento del usuario a partir de la información obtenida por el módulo de percepción. Algunos de estos datos como el centro de masas, volumen, excentricidad de la forma de su cuerpo, o número de segmentos de su cuerpo, pueden permitir clasificar el tipo actividad que realiza el usuario durante una sesión dentro de la instalación. En este caso usaremos la excentricidad y el número de segmentos móviles para calcular un valor de 'Energía' que irá entre el valor 0-Calma y 1-Excitado.
\end{itemize}

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/casosdeuso_cuartademo.png}
\end{center}
\caption{ \label{F_Casos_de_Uso_Etapa5} Diagrama de Casos de Uso. Interacción avanzada y Creación de contenidos.}
\end{figure}


\subsection{Selección de Herramientas}\label{Etapa5Herramientas}

Un cambio de interés ha sido el de la herramienta de gestión de proyectos. Para esta fase final se ha utilizado la herramienta MOOVIA \cite{MOOVIA}. MOOVIA es una red social y gestor de proyectos ágiles online que permite la gestión colaborativa de proyectos, muy integrada con el uso de documentos compartidos de Google Drive. Permite crear componentes, etapas, fases o sprints de desarrollo, y crear listas de tareas que planificar y sobre las que realizar su seguimiento. Tiene un acercamiento importante a las metodologías de desarrollo ágiles ofreciendo interfaces de tipo kanban donde ver el estado actual de una tarea (ToDo, Doing, Issue, Done), así como diagramas de BurnDown (gráfica de quemado de trabajo que muestra el trabajo pendiente por realizar) o diagramas de Gantt basados en las etapas.
\\

Como se ha usado sólo en la etapa final del proyecto, su uso se ha centrado en la manipulación de tareas. En cualquier caso se trata de una herramienta muy potente y flexible, por lo que su uso es muy recomendado.
\\

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/moovia_gestionproyectos.png}
\end{center}
\caption{ \label{F_MOOVIA_Etapa5} MOOVIA. Herramienta online para la gestión de proyectos www.moovia.com}
\end{figure}

Otra herramienta que ha sido sustituida ha sido el software de control de versiones. Para la última Etapa se ha creado un repositorio público de Github que puede accederse desde esta url: https://github.com/Adrasl/OX/
\\

Por otro lado, se revisita el módulo de Percepción para extender la detección de movimientos. El análisis que se añade es el cálculo de flujo óptico entre dos imágenes con el que poder estimar la dirección e intensidad de movimiento de un segmento del cuerpo del usuario dado un punto de su cuerpo.
\\

También se visita el módulo de Producción para añadir la capacidad de gestionar colas para añadir y retirar entidades a la escena, así como para llamar a sus métodos de actualización (cuya implementación es desconocida para este módulo), crear un mecanismo de colisiones entre entidades y contra el usuario, así como algunos ejemplos sencillos de postproceso.
\\

En el módulo de Producción se actualiza la librería de terceros Panda3D a la versión 1.8.1, que aporta la mayor ventaja de haber mejorado la estabilidad, corrigiendo algunos módulos que no eran seguros en ejecución multihilo al compilar en Release. Por suerte, los cambios remitidos durante la versión 1.6.1 que evitaban conflictos durante el lincado fueron aceptados en revisión en su momento y forman parte ya de la distribución actual, por lo que no fue necesario volver a modificarlos para recompilarlos.
\\

Antes de terminar este apartado se considera de interés recordar algunas herramientas para el análisis y síntesis de audio. Aunque no se usan en este proyecto se proponen al lector como herramientas destacadas para trabajos futuros o proyectos de similar estilo. Estas herramientas destacadas son Marsyas \ref{SUBSUBSEC_MARSYAS}, página \pageref{SUBSUBSEC_MARSYAS} y Chuck \ref{SUBSUBSEC_CHUCK}, página \pageref{SUBSUBSEC_CHUCK}.
\\

En resumen:
\\

\begin{itemize}
\item Gestión de Proyectos: MOOVIA
\item Control de versiones: Github (https://github.com/Adrasl/OX/)
\item Visión por Computador - Flujo Óptico: OpenCV
\item Motor gráfico - Gestión de entidades: Colas de gestión.
\item Motor gráfico - Gestión de colisiones: Panda3D
\item Motor gráfico - Postproceso: Panda3D
\item Creación de una malla 3D a partir de una nube de puntos: Marching Cubes.
\item Acumuladores estadísticos: Boost accumulators
\end{itemize}


\section{Diseño}
\subsection{Diseño de la Aplicación}

Se añaden elementos al módulo iPercept, IProd, ICog y se hacen modificaciones en IApplicacion, IPersistence. También se añaden elementos a la aplicación.

\begin{itemize}
\item core: Conjunto de interfaces de la aplicación.
\begin{itemize}
\item IPercept: Interfaz para el módulo de Percepción. Se añaden métodos para acceder al análisis de la forma de la presencia del usuario.
\begin{itemize}
\item IPerceptVideo: Ajustes en la interfaz para acceder a las funcionalidades de grabación y carga de vídeos, así como funcionalidades nuevas ofrecidas por los detectores añadidos. 
\item IPresenceDetection: Interfaz para ofrecer más información sobre la detección de presencia de área, orientación y excentricidad.
\item IMotionDetection: Interfaz para añadir información de flujo óptico a la detección de movimiento.
\end{itemize}
\item IProd: Interfaz para el módulo de Producción. Se añaden métodos para añadir y retirar entidades a la escena actual. Se añaden métodos para crear y actualizar la presencia del usuario en la escena. Se añaden métodos para detectar colisiones entre entidades y de entidades contra el usuario. Se añaden métodos para añadir o quitar audios a la escena (absolutos) o a las entidades (relativos), y modificar su tono y amplitud. Se añaden métodos para cambiar el color de fondo y la intensidad y color de la niebla de la escena. Se añaden métodos para añadir ejemplos de postproceso tras el render de la imagen.
\begin{itemize}
\item IEntity: Interfaz abstracta para poder manipular entidades sin necesidad de conocer su implementación.
\end{itemize}
\end{itemize}
\item igui: Ajustes para añadir opciones de configuración de las cámaras que permitan grabar y cargar vídeos grabados, así como cambios estéticos de las interfaces.
\item ipercept: Se realizan cambios para implementar las capacidades comentadas en IPercept. 
\begin{itemize}
\item PerceptVideo: Grabación y carga sincronizada de múltiples cámaras. Acceso a las funcionalidades añadidas en PresenceDetection y MotionDetection. Creación y simplificación de una nube de puntos que represente al usuario, donde cada punto guarda además información sobre el mismo como posición, tamaño y velocidad de movimiento.
\item PresenceDetection: Uso del cálculo de momentos de orden 1 y 2 para obtener área, orientación y excentricidad.
\item MotionDetection: Uso de flujo óptico para calcular intensidad y dirección de movimiento sucedido en la imagen.
\end{itemize}
\item iprod: Se realizan cambios para implementar las capacidades comentadas en IProd. Para mantener un pipeline coherente y consistente se hace uso de colas para gestionar las entidades que se encuentran en ella. Sus casos más destacados son las colas usadas para la actualización de las entidades, la detección de colisiones, y las usadas para añadir y borrar entidades de la escena.
\begin{itemize}
\item MeshFactory: Creación de un modelo3D a partir de una nube de puntos basado en Marching Cubes (Metaballs).
\item Prod3DEntity: Se extiende para heredar de IEntity y se añaden elementos como fuentes de Audio, animaciones del modelo, así como los métodos OnStart, OnUpdate y OnCollision para que el pipeline de producción permita, de forma agnóstica, que las entidades actualicen su estado.
\end{itemize}
\item vox: 
\begin{itemize}
\item ApplicationConfiguration: Ajustes necesarios para guardar las opciones de reconocimiento y login automático.
\item NavigatorController: Lo renombramos a RunningSceneController y extendemos su responsabilidad de forma que se encargue de gestionar el contenido de la escena en curso.
\item ContentCreationController: Controlador que implementa la creación de contenidos durante una sesión en curso teniendo en cuenta información sobre la interacción del usuario.
\item OXStandAloneEntity: Extensión de IEntity que implementa un tipo de entidad que se muestra por sí misma e independiente. Representa un tipo de entidad básica.
\item OXBoidsEntity: Extensión de IEntity que implementa un tipo de entidad que pertenece a una especie gregaria y cuyo comportamiento se define por su conocimiento del entorno y del de los integrantes de su propia especie.
\item Application: Ajustes para automatizar el inicio de sesión haciendo uso del módulo de percepción.
\end{itemize}
\item icog:
\begin{itemize}
\item CommonSwarmIndividual: Se ofrece en el módulo de icog una implementación de una IA genérica que puede servir para crear comportamientos de individuos de una especie, que contiene los componentes básicos de Boids (separación, cohesión y alineamiento) y extiende algunos componentes adicionales (límites, aleatoriedad, evasión y atracción). Con esta IA puede representarse comportamientos como el de una nube de moscas, como un banco de peces o bandada de aves.
\end{itemize}
\end{itemize}

\subsubsection{Breve Descripción de los Módulos}

Durante esta Etapa se trabajará principalmente en los módulos de Producción y Percepción de SDK, así como en la Aplicación en sí para crear la propuesta de instalación. 
\\

Se añade la capacidad de crear un modelo 3D a partir de la sustracción del usuario (Presencia) obtenida del módulo de Percepción. También se añadirá un esquema de colisiones Entidad-Entidad y Avatar-Entidad, donde para el usuario además se añade información sobre la velocidad de movimiento del jugador en ese punto de su cuerpo.
\\

El propio motor de juego nos da herramientas para la detección de colisiones, y el módulo de Percepción es capaz de darnos la segmentación del usuario y la velocidad en cada píxel de la imagen, como ya se ha descrito en los apartados anteriores. De esta forma, el apartado más interesante de esta sección es la construcción de una malla 3D que represente al usuario, a partir de una nube de puntos. Para ello se ha usado el algoritmo de Marching Cubes que, dado un punto y una vecindad crea primitivas geométricas predefinidas o las descarta. Este problema es muy costoso en cómputo y muy sensible a la resolución, por ello se propone una simplificación. 
\\

El primer paso es configurar un grid, una rejilla tridimensional que definirá el tamaño y posiciones de los posibles puntos en el espacio del modelo 3D. El siguiente paso consiste en reducir la cantidad de datos a estudiar (la nube de puntos), con el fin de acelerar y obtener una cantidad mayor de mallas por segundo. Para ello se ha usado un clasificador espacial R-Tree y un algoritmo de deconstrucción, de forma que dado un punto de la nube se sustraerá un volumen del mismo, como si diéramos bocados. De esta forma es posible reducir la nube de puntos original a una colección de elementos más reducida, sin afectar en exceso a la resolución de la forma o pérdida de detalles (dependiendo siempre de la configuración de los parámetros y la configuración del grid en el Marching Cubes). Una de las debilidades de este algoritmo de deconstrucción es que los puntos internos de la forma se siguen teniendo en cuenta. Se conoce la posibilidad de mejorar este algoritmo usando uno de esculpido en su lugar, de forma que sólo se tenga en cuenta los puntos de la superficie. Se estima que esto aceleraría sensiblemente la simplificación de la nube de puntos. Esta alternativa se plantea como mejora futura.
\\

Por último, nos queda el apartado de creación de contenidos. La aplicación contendrá colecciones de audios y primitivas geométricas, clasificadas de forma que sea posible asignarlas a la Ambientación y a los Elementos que se crearán, y que de esta forma sean acordes a la Psique definida para este mundo. Desde este punto de vista nos centraremos en:

\begin{itemize}
\item \textbf{Ambientación y Creación de Contenidos:} Se trata de apartados de la escena 3D que no está formado por elementos: sonido ambiental, color de fondo, niebla y efectos de post-producción. Se elegirán configuraciones de forma aleatoria dentro de una colección según la psique actual del usuario. El mundo creará de forma aleatoria elementos interactivos.
\item \textbf{Interacción con los Elementos:} Para simplificar, sólo existirá un tipo de interacción, la colisión usuario-elemento. Se realizará una colisión contra la malla 3D del usuario. Al estar creando continuamente una malla con la forma del usuario esta colisión no será de tipo Cuerpo-Rígido, sino que las geometrías serán capaces de atravesarse y lo que se desea detectar es si una geometría se superpone a otra. A partir de esta colisión se calculará la velocidad de movimiento de ese segmento y, según esa velocidad, concluiremos si el usuario está en contacto estático, moviendo suavemente o golpeando un Elemento. Con este lenguaje de tres elementos (contacto, mover, golpear) y el valor de la velocidad cada Elemento definirá un comportamiento propio. Por defecto, golpear destruirá el Elemento, contacto y mover podrán disparar una acción del elemento o crear otros nuevos.
\end{itemize}

\vspace{100 mm}

\subsubsection{Diseño en UML - Resumen}

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/DiagramadeclasesUML_CuartaDemo.png}
\end{center}
\caption{ \label{F_DiagramadeClasesUML_CuartaDemo} Diagrama de Clases UML, Cuarta Demo. Resumen.}
\end{figure}

Se revisan algunas secciones generales para extender las interfaces con nuevos métodos. En IProd se crea la interfaz IEntity que servirá para manipular entidades a alto nivel sin necesidad de conocer su implementación. En la aplicación se renombra NavigationController a RunningSceneController, ya que no se dedicará sólo a la navegación sino al control de la escena actualmente en curso. También se crea ContentCreationController, encargado de crear el contenido de la escena según la interacción observada del usuario y las Entidades OXStandAloneEntity y OXBoidEntity. Se detallan algunos de los tipos generales creados de uso común para la herramienta, como Observer y Subject para ofrecer el patrón Observador-Observable, corePoint3D, coreSound o Image, representaciones de estos datos de uso común entre módulos pero cuyos detalles de implementación queremos mantener ajenos.
\\

A este nivel no se observan más cambios, la mayor parte del trabajo se ha centrado en el módulo de Producción y en la aplicación principal, detallados en las secciones siguientes. 

\subsubsection{Diseño en UML - Núcleo de la Interfaz}
\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/DiagramadeclasesUML_CuartaDemo_core.png}
\end{center}
\caption{ \label{F_DiagramadeClasesUML_CuartaDemo_core} Diagrama de Clases UML, Cuarta Demo. Core.}
\end{figure}
En IPerceptVideo se añaden los métodos SetCameraRecording que activa o desactiva la grabación de forma sincronizada de todas las cámaras y SetUseRecording, que activa o desactiva el uso de los ficheros de vídeo grabados para alimentar al sistema. También se añaden métodos para obtener información sobre área, orientación, excentricidad de la forma observada, o cantidad de segmentos, dirección o intensidad de velocidad en la detección de movimiento. Se crea la interfaz IEntity para poder manipular diferentes implementaciones de entidades, destacando los métodos OnStart y OnUpdate para la actualización de estado durante su ejecución en el pipeline de producción. 

\subsubsection{Diseño en UML - Módulo de Persistencia}

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/DiagramadeclasesUML_CuartaDemo_ipersistence.png}
\end{center}
\caption{ \label{F_DiagramadeClasesUML_CuartaDemo_ipersistence} Diagrama de Clases UML, Cuarta Demo. Persistencia.}
\end{figure}

Por considerarse de interés general se añaden atributos a la clase IEntityPersistence. Algunos de estos datos son el tiempo de vida de la entidad, si es una entidad que hace uso de colisiones o por el contrario no interviene en ellas. 
\\

También se añaden junto con las coordeanas de su posición, los atributos de velocidad y aceleración. Por último se añaden métodos que permitan cambiar varios atributos a la vez, que típicamente son actualizados en conjunto, lo que ahorran accesos e interrupciones entre hilos al hacer las asignaciones juntas en un único bloque. 
\\

Antes de pasar al siguiente apartado, comentar que aunque la mayor parte de la aplicación usa el propio puntero a una entidad como identificador único, también se ha considerado añadir un método para obtener el ID que identifica a cada instancia en todas las clases. Éste es un mecanismo más explícito y puede evitar confusiones al trabajar a diferentes niveles.
\\
 
\vspace{140 mm}
 
\subsubsection{Diseño en UML - Módulo de GUI}

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/DiagramadeclasesUML_CuartaDemo_igui.png}
\end{center}
\caption{ \label{F_DiagramadeClasesUML_CuartaDemo_igui} Diagrama de Clases UML, Cuarta Demo. GUI.}
\end{figure}

En esta sección, los cambios en la interfaz se reducen a añadir los métodos SetCameraRecording y SetUseRecording en GUIGenericController, para indicar que se desea grabar de las cámaras o utilizar los vídeos grabados.

\subsubsection{Diseño en UML - Módulo de Aplicación}
\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/DiagramadeclasesUML_CuartaDemo_vox.png}
\end{center}
\caption{ \label{F_DiagramadeClasesUML_CuartaDemo_vox} Diagrama de Clases UML, Cuarta Demo. Aplicación.}
\end{figure}

Éste es el componente donde más elementos se han añadido. Teniendo gran parte de las herramientas disponibles este componente hará uso de ellas para crear una propuesta de instalación que sirva como ejemplo.
\\

\begin{itemize}
\item Application: Añadimos los métodos SetCameraRecording y SetUseRecording para indicar que deseamos grabar de forma sincronizada de todas las cámaras o por lo contrario alimentar al sistema con imágenes obtenidas de ficheros de vídeo en lugar de usar las imágenes en vivo de las cámaras. También añadimos los métodos AddNewentityIntoCurrentWorld y RemoveEntityFromCurrentWorld para indicar que deseamos añadir una nueva entidad a la escena actual o que queremos eliminarla.
\item NavigatorController: Extendemos su responsabilidad y lo renombramos a RunningSceneController. Mantiene la responsabilidad de navegar dentro de la escena y añade la responsabilidad de crear y mantener la representación del usuario, extrayendo esta tarea del hilo de ejecución del render de la escena mejorando así el rendimiento gráfico, así como es el encargado de ejecutar y actualizar la creación de contenidos haciendo uso del controlador ContentCreationController.
\item ContentCreationController: Controlador que implementa la creación de contenidos durante una sesión en curso teniendo en cuenta información sobre la interacción del usuario. Mantiene un registro de las entidades que existen actualmente en la escena, tiene acceso a la información obtenida por percepción y decide qué contenidos crear en consecuencia. Cabe destacar que como debe estar actualizado con los cambios sucedidos en la escena se trata de un observador que observa dos objetos: El controlador de sesión que indicará cuándo hay un cambio de escena, y las Entidades que le indicarán cuando mueren o son destruidas. Como Observador tiene un método Notified donde recibe un punto a la entidad notificadora, así como varios algunos atributos indicadores, para que los Observables puedan comunicar mensajes usando esta misma interfaz.
\item OXStandAloneEntity: Esta clase es una implementación de Prod3dEntity (que a su vez lo es de IEntity). Implementa un tipo de entidad que se muestra por sí misma y de forma independiente. Representa un tipo de entidad básica. Cabe destacar sus métodos OnStart y OnUpdate llamados al comienzo de cada ciclo del pipeline de producción y que permite que cada entidad sea capaz de actualizar su estado según indique su implementación. También tiene los métodos OnCollisionCall y OnUserCollisionCall, llamadas cuando se detecta una colisión de esta entidad contra otra entidad o contra la representación del usuario. Por último mencionar que conserva datos como tiempo de vida, métodos para cambiar el tono o volumen de los audios que reproduce, etc.
\item OXBoidsEntity: Igualmente es una implementación de Prod3DEntity (que a su vez lo es de IEntity). Implementa un tipo de entidad que pertenece a una especie gregaria y cuyo comportamiento se define por su conocimiento del entorno y del de los integrantes de su propia especie. Hereda también de CommonSwarmIndividual, una implementación básica que se ofrece en el módulo iCog del SDK.
\end{itemize}

\subsubsection{Diseño en UML - Módulo de Percepción}
\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/DiagramadeclasesUML_CuartaDemo_ipercept.png}
\end{center}
\caption{ \label{F_DiagramadeClasesUML_CuartaDemo_perception} Diagrama de Clases UML, Cuarta Demo. Percepción.}
\end{figure}
\begin{itemize}
\item PerceptVideo: Se añaden los métodos SetCameraRecording y SetUseRecording para grabar o usar vídeo y métodos para obtener datos como dominancia lateral (en qué vista tiene mayor área), orientación (inclinación) o excentricidad (si la forma es más lineal o redondeada).
\item PresenceDetection: GetPresenceArea, GetPresenceOrientation y GetPresenceEccentricity.
\item MotionDetection: Destacan GetMotionElements que devuelve los segmentos de imagen donde se ha detectado movimiento, y GetMotionAtCoords que dada unas coordenadas ofrece una velocidad de movimiento.
\end{itemize}

\subsubsection{Diseño en UML - Módulo de Producción}
\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/DiagramadeclasesUML_CuartaDemo_iprod.png}
\end{center}
\caption{ \label{F_DiagramadeClasesUML_CuartaDemo_production} Diagrama de Clases UML, Cuarta Demo. Producción.}
\end{figure}

Este es otro apartado donde más trabajo se ha realizado durante esta fase, aunque su reflejo en el diseño de la interfaz es menor en comparación.
\\

Se destacan los siguientes apartados:

\begin{itemize}
\item MeshFactory: Esta clase se ha creado con el fin de crear una malla3D a partir de una nube de puntos. Para ello se ha hecho uso del algoritmo de Marching cubes que predefine un espacio discretizado en vóxeles (unidad espacial con forma de cubo) y un catálogo de primitivas geométricas. Dada una configuración de vecindad de los puntos de la nube de puntos se crea una malla 3D combinando estas primitivas. 
\\

Destacar que este algoritmo es uno de los clásicos y más extensamente utilizados para reconstrucción de superficies a partir de datos volumétricos, usado especialmente para representaciones 3D de metabolas o metasuperficies. En este caso se hacen modificaciones en el cálculo de la distancia para conseguir un acabado más destacado o exagerado de metabola, de forma que si dos puntos alejados están lo suficientemente cerca, teniendo en cuenta sus tamaños, se crea una superficie más suavizada que los conecta, en lugar de dejar el hueco que existe entre ellos.
\\

También comentar que la creación de esta malla no se realiza en el hilo del render, sino que es el hilo del controlador RunningSceneController, el encargado de crear la representación gráfica del usuario para evitar interrupciones y pérdida de rendimiento en el render.

\item Prod3DEntity: Destacar que extiende de la clase Subject, ya que será observada por ContentCreationController a quién notificará de sus cambios. Se añaden también atributos como fuentes de audio, tiempo de vida, controladores de animación y se extiende la interfaz para añadir más métodos para obtener o asignar valores a sus atributos. 
\\

Se añaden especialmente métodos para obtener o asignar atributos en bloque que suelen ser accedidos juntos, de forma que pueden hacerse en una sola llamada, lo que permite reducir el número de peticiones y bloqueos de la entidad.

\item MainProd: Finalmente se añade a MainProd una serie de atributos y métodos para dar cabida a las nuevas funcionalidades que se van a desarrollar. Los más destacados son los controladores de animación de la escena para permitir que los objetos 3D que tengan asociados una animación sean capaces de reproducirlo.
\\

También se añade acceso para cambiar el color de fondo de la escena o el color e intensidad de la niebla, un contenedor de audios no relativos para reproducir sonidos de fondo que siempre se escuchen a igual intensidad, independientemente de la posición del usuario, y, finalmente, una serie de efectos de postproceso predefinidos como Invert, Bloom, Blur o Cartoon para añadir de forma simple un acabado diferente al render.
\end{itemize}

\subsubsection{Diseño en UML - Módulo de Inteligencia Artificial}
\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/DiagramadeclasesUML_CuartaDemo_icog.png}
\end{center}
\caption{ \label{F_DiagramadeClasesUML_CuartaDemo_icog} Diagrama de Clases UML, Cuarta Demo. IA (Cog).}
\end{figure}

Este módulo tiene la intención de ofrecer componentes de IA básicos que el desarrollador pueda usar directamente o extender. En este caso añadimos una implementación básica del comportamiento de un individuo que forma parte de un enjambre. Está inspirado en la implementación clásica de Boids, desarrollada por Craig Reynolds \cite{CRAIGREYN}.

\begin{itemize}
\item CommonSwarmIndividual: Se ofrece en el módulo de icog una implementación de una IA genérica que puede servir para crear comportamientos de individuos de una especie, que contiene los componentes básicos de Boids (separación, cohesión y alineamiento) y extiende algunos componentes adicionales (límites, aleatoriedad, evasión y atracción). 
\\

Cada individuo tiene conocimiento de su entorno, aunque para evaluar su estado actual usará sólo información que esté dentro de campo de visión. Teniendo en cuenta todas sus necesidades (separación, cohesión, alineación, límites, evasión, atracción y randomicidad) y el factor de peso de cada una de ellas, el individuo modificará su comportamiento expresado como posición, velocidad y aceleración para cumplir esos objetivos. 
\\

Con esta IA pueden representarse comportamientos como el de una nube de moscas, un banco de peces o una bandada de aves. También puede extenderse o combinarse para crear otros tipos de comportamientos o ejemplos de inteligencias artificiales más complejas como los agentes inteligentes.

\end{itemize}

\section{Implementación}

Los módulos afectados en la presente fase son los siguientes:

\begin{itemize}
\item core: Añadida la clase IEntity, modificadas las interfaces de IPerceptVideo.
\item igui: Añadidas interfaces gráficas para grabar la captura de imagen en vídeo o cargar vídeo en lugar de usar imagen en vivo. Cambios estéticos y ocultación de interfaces de elementos que se dejan como trabajo futuro.
\item ipercept: Se implementa la captura sincronizada de múltiples pantalla o la carga de múltiples vídeos. Se añade flujo óptico al detector de movimiento, y se añade cálculo de área, orientación y excentricidad al detector de presencia.
\item ipersistence: Se añaden campos a los objetos usuario, mundo y entidad y se añaden algunos métodos para mejorar su uso.
\item iprod: Se añade capacidad para crear una malla 3D a partir de una nube de puntos, se plantea un esquema de proyección de la representación del usuario para interactuar con objetos de la escena. Se prepara esquema para la carga, destrucción y actualización de entidades en la escena. Se añade algunos elementos de personalización de la escena como música de fondo, color de fondo, color e intensidad de niebla y efectos de postproceso.
\item vox: Se convierte el NavigatorController en RunningSceneController. Se crea el ContentCreationController. Se crean dos implementaciones de entidades y se crea de esta forma una propuesta de instalación a partir del kit de desarrollo creado.
\item icog: Implementación de un modelo de IA que simula el comportamiento de enjambres.
\end{itemize}

\subsubsection{Núcleo de la Interfaz}

En esta ocasión los cambios realizados en el núcleo de la Interfaz son menores, añadiendo principalmente algunos métodos a clases ya existentes para acceder a más información. Al tratarse sólo de una interfaz podrá verse información de mayor interés en los apartados correspondientes, donde se implementan estas funcionalidades. Se listan los cambios a continuación:
\\

src$\backslash$core$\backslash$IPercept$\backslash$IPerceptVideo\\

Como se ha comentado en los apartados anteriores, se añaden los métodos SetCameraRecording destinada a activar o desactivar la grabación de forma sincronizada de todas las cámaras y SetUseRecording, para activar o desactivar el uso de los ficheros de vídeo grabados para alimentar al sistema. Así como métodos para obtener información sobre dominancia, orientación, excentricidad, número de elementos móviles, velocidad, o la nube de puntos que representa al usuario.

\begin{lstlisting}[language=C++]
...
 virtual bool SetCameraRecording(const bool &value)
 virtual bool SetUseRecording(const bool &value, const std::string &url)
 virtual void GetMainLateralDominance(corePoint3D<double> &result)
 virtual void GetMainOrientation(corePoint3D<double> &result)
 virtual void GetMainEccentricity(corePoint3D<double> &zero_means_round)
 virtual void GetFeatureWeightedPositions(const std::string &feature, 
              std::map<int, std::vector<corePDU3D<double>>> &result, 
              const float &scale=1)
 virtual std::vector<MotionElement> GetMotionElements()
...
\end{lstlisting}

src$\backslash$core$\backslash$IPercept$\backslash$video$\backslash$IPresenceDetection\\

Se añaden métodos para obtener información sobre área, orientación, excentricidad de la forma observada a partir de la imagen de sustracción de fondo. El área está medido en píxeles, la orientación en radianes (partiendo del eje de ordenadas, en sentido contrario a las agujas del reloj) y la excentricidad es un valor que variará entre 0, que indica una forma circular, y 1 que indica una forma de línea.

\begin{lstlisting}[language=C++]
...
 virtual void GetPresenceArea(double &area_inpixel_units)
 virtual void GetPresenceOrientation(double &radians_counterclockwise)
 virtual void GetPresenceEccentricity(double &cero_means_round)
...
\end{lstlisting}

src$\backslash$core$\backslash$IPercept$\backslash$video$\backslash$IMotionDetection\\

Más información añadida es la cantidad de segmentos en movimiento, y la dirección e intensidad de movimiento dadas unas coordenadas.

\begin{lstlisting}[language=C++]
...
 virtual std::vector<MotionElement> GetMotionElements()
 virtual corePoint3D<float> GetMotionAtCoords(corePoint2D<int> coords)
...
\end{lstlisting}
\vspace{20 mm}

src$\backslash$core$\backslash$IProd$\backslash$IEntity\\

Se crea la interfaz IEntity para poder manipular diferentes implementaciones de entidades. A partir de ahora Prod3DEntity implementará esta interfaz. Igualmente, cuando la aplicación desee crear entidades más complejas que Prod3DEntity (que contiene sólo elementos básicos para su representación), heredarán de ésta última para extender sus capacidades. Destacan los métodos OnStart y OnUpdate, para la actualización de estado durante su ejecución en el pipeline de producción. También se añaden métodos que permiten acceder a varios atributos en bloque, lo que reduce el número de interrupciones necesarias en la sincronización de hilos. Se comentan algunos de los métodos:

\begin{itemize}
\item OnStart(): Llamado una única vez tras cargar la entidad en la escena.
\item OnUpdate(): Llamado cada vez, al comienzo de la ejecución de un nuevo render de imagen. Es el lugar adecuado para evaluar el comportamiento que debe realizar como la gestión de sus animaciones, trasladarla, o aplicar los cambios que una IA asociada quisiera hacer.
\item OnCollisionCall(): Llamado cada vez, al comienzo de la ejecución de un nuevo render, antes de que se llame a ningún Update de ninguna entidad, sólo si el motor gráfico indica que dicha entidad ha incurrido en una colisión con otra entidad.
\item OnUserCollisionCall(): Actúa de igual manera que el caso anterior, pero en la situación especial de que se trate de la representación del usuario, indicando en este caso la posición y velocidad del elemento del avatar con el que colisionó.
\item IsReadyToDie(): Las entidades no pueden retirarse en cualquier momento ya que el motor gráfico puede seguir usándola y provocar fallos si la entidad es destruida de imprevisto. Todas las tareas que añadan o eliminen objetos en la escena se realizan a través de unas colas de carga y borrado, que añaden o borran las entidades en el momento adecuado. Con este método una entidad puede indicar que debe evitarse su uso y que ya puede ser retirada del grafo de la escena, así ésta será retirada de forma segura al comienzo del próximo render.
\end{itemize}

\begin{lstlisting}[language=C++]
class _COREEXPORT_ IEntity
{public:
  virtual ~IEntity(){}
  virtual void DeletePersistence()
  ...
  virtual void SetPositionVelocityAcceleration(...)
  virtual void GetPositionVelocityAcceleration(...)
  virtual void OnStart()
  virtual void OnUpdate();
  virtual void OnCollisionCall(IEntity *otherEntity) 
  virtual void OnUserCollisionCall(core::corePDU3D<double>collisionInfo)
  virtual bool IsReadyToDie()
  virtual bool IsCollidable()
  virtual void SetCollidable(const bool &value)
};
\end{lstlisting}

src$\backslash$core$\backslash$types\\

Lo más destacado de los cambios realizados en esta sección son las clases Subject y Observer, que aportan una implementación muy básica del patrón de diseño Observador-Observable. El problema que resuelve este patrón es el de mantener notificada a una entidad de los cambios sufridos por otra entidad, sin necesidad de que consulte continuamente por ella.

\begin{itemize}
\item Subject: Se trata de la entidad Observable. Contiene un lista de Observadores. Cuando esta entidad es actualizada basta con llamar a su método Notify(), donde recorrerá su listado de observadores para notificarles de que ha sufrido un cambio. Como extensión al patrón de diseño básico, se añaden algunos campos de información que sirvan como intercambio de mensajes.

\begin{lstlisting}[language=C++]
class Subject {
 std::vector< class Observer* > observers;
 public:
 int ObserversCount() 
 void attach(Observer *observer) 
 void detach(Observer *observer) 
 void detach_all() 
 virtual void Notify(void* callinginstance=NULL, 
                     const std::string &tag="", 
                     const int &flag=0) 
 { ... (*iter)->Notified(callinginstance, tag, flag); }
}
\end{lstlisting}

\item Observer: Se trata de la entidad Observadora. Debe implementar el método Notified(), y una vez conectada a la entidad Observable será notificada cada vez que la Entidad Observable lo indique. Qué código implementa la clase Observadora es completamente transparente para la Observable. En este caso, la clase base no ejecuta ningún código al ser Notificada.

\begin{lstlisting}[language=C++]
class Observer {
 public:
  virtual void Notified(void* callinginstance=NULL, 
                        const std::string &tag="", 
                        const int &flag=0) {}
};
\end{lstlisting}


\end{itemize}

\subsubsection{Módulo de Persistencia}
Se han añadido modificaciones que añaden nuevos campos a los objetos. También se han añadido métodos que permiten leer o escribir varios campos en una sola llamada. Finalmente, se añade un método GetID() que nos devuelve el ID asignado al objeto en la base de datos. Estos cambios son muy simples y no aporta mayor detalle que lo visto en secciones anteriores. En cualquier caso, a modo de ilustración sobre este apartado, puede consultarse la sección \ref{Etapa3detallesImp:ModuloPersistencia}, en la página \pageref{Etapa3detallesImp:ModuloPersistencia}.

\subsubsection{Módulo de GUI}

Como añadido en la vista de Configuración, sección Cámaras, se añade un botón que permite iniciar o detener la grabación sincronizada de múltiples cámaras. 
\\

Igualmente se añade un campo de texto donde podemos escribir el nombre de un fichero de vídeo que será usado como prefijo, al que el sistema añadirá el texto 'cam' y el número que indica un índice de cámara comenzando a partir de 1. Este 'basename' es usado a la hora de cargar ficheros de vídeo, uno por cada cámara para alimentar al sistema, que deben estar en el mismo directorio de la aplicación donde se generan. 
\\

Finalmente, hay un botón de selección que permite alternar entre el uso de la captura en vivo desde las cámaras o la carga de dichos ficheros para usar la imagen de vídeo en su lugar.

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/etapa5_grabarousarvideo_gui.png}
\end{center}
\caption{ \label{F_GUIEsteticos_Etapa5} Cambios en la sección de configuración que permiten grabar la captura de imagen en vivo en un vídeo por cada cámara o usar vídeos ya grabados en lugar de la captura en vivo}
\end{figure}

Aparte de lo comentado, los cambios realizados en esta sección están limitados a la sustitución de elementos estéticos como son las imágenes de fondo de las diferentes vistas. Para ello ha bastado con sustituir la imagen de fondo en el directorio de recursos gráficos de GUI. 
\\

También se ocultan algunos elementos de la interfaz que inicialmente fueron añadidos pero que pretendían dar acceso a funcionalidades que han quedado fuera del alcance del proyecto. 

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/etapa5_captura_gui.png}
\end{center}
\caption{ \label{F_GUIEsteticos_Etapa5} Retoques gráficos en las vistas de la aplicación}
\end{figure}

\subsubsection{Módulo de Percepción}
Durante esta fase se extienden las capacidades de Percepción definidas durante el desarrollo de la Etapa 4, y que puede consultarse en la sección \ref{Etapa4_Impl:Percepcion} a partir de la página \pageref{Etapa4_Impl:Percepcion}. 
\\

De esta forma, se revisitan los componentes PresenceDetection y MotionDetection para añadir más elementos a su estudio.
\\

src$\backslash$ipercept$\backslash$video$\backslash$PresenceDetection\\

Este componente obtenía, mediante sustracción de fondo una imagen que representaba la presencia de un usuario en el interior de la zona de estudio. Como puede verse en la etapa anterior, hacíamos uso del cálculo de los momentos de orden cero y uno para obtener el área y centro de masas de la imagen obtenida por el módulo de detección de presencia. 
\\

Siguiendo la misma línea, en esta ocasión, usaremos además los momentos de orden dos, para obtener la orientación y la excentricidad de la forma detectada como presencia. La orientación está medida en radianes (tomando el sentido contrario a las agujas del reloj y resolviendo la ambigüedad de la cabeza o cola de la elipse, partiendo del eje de ordenadas y tomando el semieje mayor de la elipse). Para un mejor entendimiento del uso de los momentos para obtener este tipo de información se recomienda leer el artículo 'Simple Image Analysis by Moments' de Johannes Kilian \cite{JOHANNESK}, del que se han obtenido los cálculos necesarios.
\\

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/etapa5_momentosorientacion_percepcion.png}
\end{center}
\caption{ \label{F_GUIEsteticos_Etapa5} Cálculo de la orientación de un blob a partir de los momentos}
\end{figure}


\begin{lstlisting}[language=C++]
presence_orientation = 0.0;
double mu20_minus_mu02 = (*foreground_moments).mu20 - (*foreground_moments).mu02;
double mu11 =(*foreground_moments).mu11;
double base_presence_orientation=0.5*atan(2*mu11/mu20_minus_mu02);
if (mu20_minus_mu02==0.0)
{ if (mu11>0) presence_orientation = M_PI * 0.25;
  if (mu11<0) presence_orientation = M_PI * -0.25;			
} else if (mu20_minus_mu02>0.0)
{ if ((mu11>0)||(mu11<0))presence_orientation=base_presence_orientation;
} else
{ if (mu11>0) presence_orientation=base_presence_orientation+(M_PI*0.5);
  if (mu11<0) presence_orientation=base_presence_orientation-(M_PI*0.5);
}
\end{lstlisting}

La excentricidad es un valor que variará entre cero y uno. Un valor cercano a cero indica que la forma estudiada es similar a una circunferencia. Por el contrario un valor cercano a uno indica una forma de línea. En  nuestro caso, un ejemplo claro que da un valor cercano a uno es cuando la presencia se encuentra en una pose recta de pie o acostada. 

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/etapa5_momentosexcentricidad_percepcion.png}
\end{center}
\caption{ \label{F_GUIEsteticos_Etapa5} Cálculo de la excentricidad de un blob a partir de los momentos}
\end{figure}

\begin{lstlisting}[language=C++]
presence_eccentricity = 1.0;
double mA=pow((*foreground_moments).mu20-(*foreground_moments).mu02,2);
double mB=pow((*foreground_moments).mu11, 2)*4.0;
double mC=pow((*foreground_moments).mu20+(*foreground_moments).mu02,2);
if (mC>0.0) presence_eccentricity=(mA-mB)/mC ;
\end{lstlisting}

\vspace{5 mm}

src$\backslash$ipercept$\backslash$video$\backslash$MotionDetection\\

El cambio realizado en este detector es que se añade cálculo de flujo óptico para ofrecer más información de movimiento. 
\\

El flujo óptico es una técnica que nos indica el movimiento aparente de un objeto en una imagen. Como puede verse en la figura \ref{F_FlujoOpticoKaneda_Etapa5} se trata de encontrar la distancia h que hace que F(x)=G(x)=F(x+h), o que lleva a una diferencia mínima entre G(x) y F(x+h). En esa misma figura, a la izquierda se muestra este problema de forma esquemática, mientras que a la derecha se ilustra el cálculo de un caso simple de una muestra monodimensional (si nos centramos en el estudio del eje x). En nuestro caso utilizamos una implementación que ofrece  OpenCV basada en el algoritmo de Lucas Kanade \cite{LUKASK81}. 

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/etapa5_problemofopticalflow_perception.png}
\end{center}
\caption{ \label{F_FlujoOpticoKaneda_Etapa5} Ilustración del problema del flujo óptico, Lukas Kanade \cite{LUKASK81}}
\end{figure}

Una de las mejoras que aportaba el algoritmo de Lukas Kanade era que permitía realizar menos comparaciones de imágenes para ofrecer una correspondencia, utilizando información extrínseca, en este caso discriminar por la proximidad lineal en las transformaciones del patrón que se busca, que frecuentemente son cercanas y permiten acelerar el cálculo.

\begin{lstlisting}[language=C++]
...
 velx = cvCreateImage(size,IPL_DEPTH_32F,1);
 vely = cvCreateImage(size,IPL_DEPTH_32F,1);	
if (previous_frame == NULL)
{ previous_frame = cvCreateImage(size,IPL_DEPTH_8U,1);
  cvCvtColor(img,previous_frame,CV_RGB2GRAY);
} else
{ cvReleaseImage(&previous_frame);
  previous_frame = current_frame;
}
current_frame = cvCreateImage(size,IPL_DEPTH_8U,1);
cvCvtColor(capture, current_frame, CV_RGB2GRAY);		
...
 velx_Mat = cvCreateMat(size.height, size.width, CV_32FC1); 
 vely_Mat = cvCreateMat(size.height, size.width, CV_32FC1);

//Calculating dense Optical Flow (Lucas Kanade)
block_size.height = block_size.width = 3;
if (previous_frame && current_frame)
 cvCalcOpticalFlowLK(previous_frame,current_frame,block_size,velx,vely);
cvConvert(velx, velx_Mat);
cvConvert(vely, vely_Mat);
\end{lstlisting}
\vspace{5 mm}

De esta forma, dadas dos imágenes (actual y previa) y un tamaño de ventana para agrupar píxeles para la búsqueda de patrones similares, esta función nos ofrece, por cada píxel dos matrices, una dando el desplazamiento estimado en el eje de ordenadas y otra para el eje de abscisas.

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/etapa5_opticalflow_perception.png}
\end{center}
\caption{ \label{F_FlujoOptico_Etapa5} Detección de Movimiento: mostrando la imagen de histórico de movimientos a la derecha, y una muestra de flujo óptico tomada cada 50 píxeles a la izquierda, donde un punto implica que no hay movimiento y una línea refleja el movimiento detectado.}
\end{figure}

\vspace{7 mm}
src$\backslash$ipercept$\backslash$PerceptVideo\\

Llegados a este punto tenemos una colección de detectores que estudian imágenes y son capaces de ofrecer una información enriquecida. El principal objetivo que queremos conseguir con ella, es que una aplicación pueda obtener, además de esa información de forma puntual, una representación de la presencia del usuario con sus datos. Para terminar este apartado, veremos cómo creamos un modelo del usuario a partir del cual poder crear una malla3D para su visualización en producción, o una representación a modo de proyección para colisionar con objetos que se encuentren delante.
\\

Desde este punto de vista, y aunque es realmente interesante el estudio del reconocimiento postural, se ha optado por una solución más simple que cubran los objetivos del presente proyecto y que entren dentro de su alcance. En cualquier caso, me gustaría hacer referencia a algunos destacados artículos sobre esta materia. Sería muy interesante poder continuar estas líneas de estudio como posible trabajo futuro.
\\

Algunos de los trabajos analizados se basan en el estudio de un modelo de vóxeles del usuario obtenido a partir de múltiples cámaras como puede leerse en el trabajo 'Articulated Body Posture Estimation from Multi-Camera Voxel Data' de Ivana Mikic \cite{IVANAM}. Otros, en la detección de partes del cuerpo centrando el estudio en las articulaciones del mismo para estimar la pose del esqueleto del usuario como puede leerse en 'Estimating Human Body configurations using Shape Context Matching' de Greg Mori \cite{GREGMJM}. 

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/postural.png}
\end{center}
\caption{ \label{F_ReconocimientoPostural_Etapa5} Ilustraciones de los trabajos mencionados sobre reconocimiento postural de Ivana Mikic et al. \cite{IVANAM} (izquierda) y Greg Mori et al. \cite{GREGMJM} (derecha)}
\end{figure}

En la misma línea, no quisiera dejar atrás hacer referencia al trabajo de Juergen Gall et al. que puede leerse en el artículo 'Motion Capture Using Joint Skeleton Tracking and Surface Estimation', donde se propone un método para adaptar una plantilla de una malla 3D y un esqueleto predefinidos a la captura en vivo de una imagen, y con la que parecen conseguir resultados excelentes incluso en acciones que suceden a gran velocidad \cite{JUERGEN}.
\\


\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/etapa5_postural_percepcion.png}
\end{center}
\caption{ \label{F_ReconocimientoPosturalJuergen_Etapa5} Ajuste de una malla3D dada y esqueleto a modo de plantilla sobre una captura de imagen \cite{JUERGEN}}
\end{figure}

En nuestro caso crearemos una representación del usuario basada en una nube de puntos que, simplificada, nos permitirá representar al usuario así como proyectar su interacción con la escena y obtener información del usuarios sobre esas interacciones.\\

Se usará la sustracción de fondo para obtener un modelo del usuario. Añadiendo los datos de detección de movimiento obtendremos la velocidad de movimiento de un punto concreto del avatar. Es decir, aunque hablemos de puntos, cada punto es una estructura de datos que contiene más información que su posición, como, por ejemplo, su velocidad. En cualquier caso, con la solución planteada, no habrá distinción explícita de extremidades ni postura. Aunque se considera un apartado muy interesante para trabajo futuro. 
\\

Uno de los principales puntos a resolver es conseguir una representación lo bastante ligera como para generar una alta tasa de resultados por segundo. Para ello obtenemos la intersección proyectada entre los blobs de las diferentes fuentes de vídeo. Aún así, el mayor problema de trabajar con nubes de puntos es la gran cantidad de datos que hay que manejar. A modo ilustrativo si tuviéramos un blob de 10x10 píxeles tendríamos 100 elementos para estudiar, si llevamos esto a 500x500 píxeles la cuenta sube a 250.000 elementos.
\\

Por este motivo, dada una nube de puntos en bruto a partir del blob de presencia obtenido por el detector de presencia, la simplificación de la nube de puntos sigue los siguientes pasos:

\begin{itemize}
\item Bajar la resolución: El primer paso consiste en escalar la imagen. No necesitamos una representación a resolución completa.
\item Reducción de la nube de puntos original: Se ha planteado una posible solución que ofrecería una representación en forma de nube de puntos con pesos asignados, similar a la original, donde cada punto puede tener un tamaño mayor a los otros. 
\\

Para conseguir esto se siguen los siguientes pasos:
\begin{itemize}
\item Clasificación espacial de la nubes de puntos mediante un árbol de búsqueda R-Tree. 
\\

Este árbol balanceado da especial importancia a la agrupación de sus nodos en contenedores rectangulares mínimos que engloben las dispersiones locales de objetos. Permite realizar búsquedas rápidas dentro de una colección de elementos a los que se les puede asignar un tamaño, y sobre la que se pueden realizar consultas mediante vecindades usando radios de distancia. 
\\

R-Tree no garantiza buenos rendimientos en los peores casos, pero demuestra ser muy eficiente en situaciones reales y es muy usado actualmente en las extensiones para búsquedas eficientes de entidades geolocalizadas dada una localización y una vecindad.

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/etapa5_rtree.png}
\end{center}
\caption{ \label{F_RTree_Etapa5} Representaciones en forma de árbol de nodos, 2D y 3D de un R-Tree \cite{RTREE}.}
\end{figure}

\item Simplificación mediante escarbado: Dada esta representación se toma un nodo aleatorio y se calcula un porcentaje de llenado para vecindades, empezando por un tamaño mayor y escalando hacia tamaños más pequeños hasta encontrar uno que satisfaga la condición de llenado. Si no satisface la condición con un tamaño mínimo el nodo es descartado. De esta forma podemos pasar de una representación del usuario que, como en el caso de la fingura \ref{F_Escarbado_Etapa5} puede contener contiene miles de puntos a una representación simplificada que para una forma similar a la mostrada puede estar alrededor de 30 puntos de diferentes tamaños.

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/etapa5_simplificacionnubepuntos_perception.png}
\end{center}
\caption{ \label{F_Escarbado_Etapa5} Simplificación de una nube de puntos mediante escarbado.}
\end{figure}

\end{itemize}
\end{itemize}

\begin{lstlisting}[language=C++]
void PerceptVideo::ObtainPresenceVolumeAsWeightPoints(std::map<int, 
  std::vector<corePDU3D<double>>>&weighted_points, 
  const float &scale)
...
for (int i = n_steps; i >= 0; )
{ bool candidate_step_found = false;
  Rect3F search_rect(new_point.position.x-search_delta_window,
    new_point.position.y-search_delta_window,
    new_point.position.z-search_delta_window, 
    new_point.position.x+search_delta_window,
    new_point.position.y+search_delta_window,
    new_point.position.z+search_delta_window);
  search_results.clear();
  int overlapping_size=spatial_index.Search(search_rect.min,
    search_rect.max,RegisterPointIDIntoSearchResults,NULL);
  float n_deltas=search_delta/delta;
  third_delta=(is3D)?n_deltas:1.0;
  float success_criteria = 0.95;
  
  if((overlapping_size>(success_criteria*pow(n_deltas,((is3D)?3:2))))&&
    (overlapping_size>1)) 
  { new_point = relative_points[iter_isp->first];
    image_space_points[iter_isp->first].y;
    if (weighted_points.find(search_delta)!= weighted_points.end())
      weighted_points[search_delta].push_back(new_point);
    else
    { std::vector<corePDU3D<double>> new_vector;
      new_vector.push_back(new_point);
      weighted_points[search_delta] = new_vector;
    }
    for (std::vector<int>::iterator diter = search_results.begin();
         diter != search_results.end(); diter++)
    { spatial_index.Remove(search_rect.min, search_rect.max, *diter);
      image_space_points.erase(*diter);
      candidate_step_found = true;
    }
  }
  else if (( i==0 ) || overlapping_size == 1 )
  { spatial_index.Remove(search_rect.min,search_rect.max,iter_isp_id);
    image_space_points.erase(iter_isp_id); 
  } ...
\end{lstlisting}

Recordar que PerceptVideo actúa como controlador principal del análisis de vídeo. También recordar que, aunque la arquitectura está preparada para trabajar con múltiples fuentes (creando los detectores necesarios que funcionarán en paralelo por cada fuente), en el alcance de este proyecto nos centraremos en el uso de una única fuente.\\

Se recuerda también que la solución adoptada para la orientación y combinación de múltiples cámaras consiste en una orientación predefinida de tipo Cave (un cubo, con sus seis lados: derecha, izquierda, frontal, trasero, superior, inferior). Para coordinar los datos se alinean los centros de cada imagen y se proyecta la información en profundidad, utilizando (por normal general) la intersección, unión o promedio de ambos. Igualmente se plantea revisitar esta sección como trabajo futuro, tanto para mejorar la eficiencia de la solución actual como para extenderla y permitir combinar fuentes de vídeo en cualquier orientación.
\vspace{20 mm}

\subsubsection{Módulo de Producción}

Ésta es una de las secciones donde más trabajo se ha realizado durante esta fase. Podemos distinguir tres apartados: Representación del Avatar, Extensión de funcionalidades asociadas a la gestión de entidades en la escena, y Extensión de capacidades del controlador principal de producción para personalizar el entorno.
\\

Se destacan los cambios realizados en las siguientes clases:
\\

\begin{itemize}
\item MeshFactory: Como comentábamos en la sección de diseño, esta clase se ha creado con el fin de crear una malla3D a partir de una nube de puntos. Para ello se ha hecho uso del algoritmo de Marching cubes que predefine un espacio discretizado en vóxeles (unidad espacial con forma de cubo) y un catálogo de primitivas geométricas. Con esta técnica pueden conseguirse resultados como metabolas o meta superficies, o incluso (simplificándolo con menos primitivas) construir terrenos virtuales procedimentales modificables en tiempo de ejecución como en el juego Minecraft \cite{MINECRAFT}.
\\

En Marching Cubes \cite{MCUBESW} se discretiza el espacio en un número de vóxeles, a mayor cantidad de divisiones, mayor resolución tendrá la malla resultante. Para cada vóxel se estudian sus ocho esquinas, para evaluar si deben contribuir a que exista una superficie o no, mediante un valor definido por el usuario, donde generalmente se usa un valor por vecindad o por distancia con los puntos cercanos de la nube de puntos que se estudia. En la figura \ref{F_MarchingCubesPrimitives_Etapa5} se muestran algunos ejemplos de primitivas, donde se ve la configuración de caras y, resaltadas, las esquinas del vóxel que cumplen con esa condición y por lo tanto las que determinan qué primitiva se crea. Estudiando las diferentes configuraciones posibles de un cubo, variando sus 8 esquinas, tenemos 256 posibles primitivas. Muchas de las 256 en realidad son iguales, sólo que en una orientación diferente.

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/etapa5_marchingcubesprimitives_production.png}
\end{center}
\caption{ \label{F_MarchingCubesPrimitives_Etapa5} Marching Cubes: Primitivas geométricas y las esquinas que las definen \cite{MCUBESW}.}
\end{figure}

De esta forma, dada la configuración de una nube de puntos, definido un número de divisiones para discretizar el espacio, y una función de evaluación para considerar si una esquina de un vóxel debe contribuir o no a que exista una superficie, se crea una malla 3D combinando las primitivas mencionadas. En la siguiente figura \ref{F_MarchingCubesSampless_Etapa5} vemos dos representaciones. A la izquierda, vemos un ejemplo muy simple de una malla 3D creada a partir de estas primitivas y de los vóxeles involucrados donde encaja. En la secuencia de la derecha, al estilo de unas metabolas, vemos una misma entidad, definida por una una nube de puntos en la que podemos ver cómo, sólo variando la cantidad de subdivisiones del espacio, el modelo resultante tiene mayor o menor detalle.
\\

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/etapa5_marchingcubessamples_production.png}
\end{center}
\caption{ \label{F_MarchingCubesSampless_Etapa5} Marching Cubes, construcción de metasuperficies y voxelización del espacio \cite{PAULB94}}
\end{figure}

El modelo de datos es exactamente el mismo en los cinco casos, es la misma nube de puntos. Subdividiendo el espacio en más o menos vóxeles se puede conseguir una representación con mayor o menor resolución. Notar también que en estas muestras de render no se usan técnicas de shading como Phong para interpolar la iluminación dentro de una cara. Dicho de otra forma, el color en estas imágenes es exactamente igual para todos los píxeles que caen dentro de una misma cara, en todos los casos. En esta figura, las formas más suavizadas se ven así porque la geometría del mallado 3D tiene una cantidad muy elevada de caras.


Destacar que este algoritmo es uno de los clásicos y más extensamente utilizados para reconstrucción de superficies a partir de datos volumétricos, usado especialmente para representaciones 3D de metabolas o metasuperficies. 
\\

En este proyecto se hacen modificaciones en la función de evaluación de cada vértice, para tener en cuenta un cálculo de distancia y el tamaño (o peso) de la representación simplificada de la nube de puntos del avatar del usuario. 
\\

Se ajustan los valores para conseguir un acabado de metabola mejor conectado, de forma que si dos puntos están alejados pero según sus tamaños lo suficientemente cerca, se crea una superficie que los conecta, en lugar de dejar el hueco que existe entre ellos. 
\\

El código asociado a este algoritmo es extenso. Si se desea estudiar en detalle cómo resolver una implementación de este algoritmo, se recomienda la lectura de Paul Bourke, 'Polygonising a scalar field: Also known as 3D Contouring, Marching Cubes, Surface Reconstruction' \cite{PAULB94}. En el siguiente bloque sólo se pretende mostrar algunos de sus elementos principales.

\begin{lstlisting}[language=C++]
NodePath* MeshFactory::CreateVoxelized(source_weighted_data)
{ 
 int grid_overlapping_size_whole=aux_spatial_grid_index.Search(grid_search_rect_whole.min, grid_search_rect_whole.max,
    RegisterGridPointIDIntoSearchResults_callback);              
 GeomVertexWriter vertex, normal, color, texcoord;
 vertex = GeomVertexWriter(vdata, "vertex");
 normal = GeomVertexWriter(vdata, "normal");
 color = GeomVertexWriter(vdata, "color");
 texcoord = GeomVertexWriter(vdata, "texcoord"); 
 for (iter=global_rTree_search_results.begin(); 
      iter!=global_rTree_search_results.end();iter++)
 {for (jX = jXini; jX <= jXend; jX++)
  for (jY = jYini; jY <= jYend; jY++)
  for (jZ = jZini; jZ <= jZend; jZ++)
  { 
    point_str << jX << ":" << jY << ":" << jZ;
    v_iter=visited.find(point_str.str());
    if (v_iter == visited.end())
    { //OBTAIN THE TRIANGLES OF THE VOXEL PRIMITIVE
      int nTriangles=(this->*vMarchCube)(jX*fStepSize,jY*fStepSize,
                                         jZ*fStepSize,fStepSize,
                                         v_vertex,v_color,
                                         v_edge_normal);
      for ( int i = 0; i < nTriangles; i++)
      {vertex.add_data3f(vert[i*3].x,  vert[i*3].y,  vert[i*3].z  );
       vertex.add_data3f(vert[i*3+1].x,vert[i*3+1].y,vert[i*3+1].z);
       vertex.add_data3f(vert[i*3+2].x,vert[i*3+2].y,vert[i*3+2].z);
      }
      nVertices+=nTriangles*3;
    }
  }
  PT(GeomTriangles) tris;
  tris = new GeomTriangles(Geom::UH_static);
  for (int i = 0; i<nVertices; i = i+3)
  { tris->add_vertex(i+0);
    tris->add_vertex(i+1);
    tris->add_vertex(i+2);
    tris->close_primitive();
  }
}	
//DISTANCE
float MeshFactory::DistanceToWeightedPointsInRange(...)
{ for (int i=0;i<RTree_search_results.size();i++)
  { iter =source_weighted_points_indexed.find(search_results[i]);
    iter2=weight_index.find(iter->first);
    fResult+=(metaball_weightfactor*(float(iter2->second)))/
             (fDx*fDx+fDy*fDy+fDz*fDz);
}
//MARCHINGCUBES on a single cube
int  MeshFactory::vMarchCube1(...)
{ for(iVertex=0;iVertex<8;iVertex++)
  CubeValue[iVertex]=(this->*DistanceToWeightedPointsInRange)(...);
 //Find if vertices are inside or outside the surface
 for(iVertexTest=0;iVertexTest<8;iVertexTest++)
  if(CubeValue[iVertexTest]<=fTargetValue)
   iFlagIndex|=1<<iVertexTest;
 //If entirely inside or outside, ignore it
 iEdgeFlags = CubeEdgeFlags[iFlagIndex];
 if(iEdgeFlags==0) return 0;
 //Triangles
 for(iTriangle=0;iTriangle<5;iTriangle++)
 {if(TriangleConnectionTable[iFlagIndex][3*iTriangle]<0) break;
  for(iCorner=0;iCorner<3;iCorner++)
  {iVertex=TriangleConnectionTable[iFlagIndex][3*iTriangle+iCorner];
   vertex[nTriangles*3+iCorner].x=asEdgeVertex[iVertex].x;
   vertex[nTriangles*3+iCorner].y=asEdgeVertex[iVertex].y;
   vertex[nTriangles*3+iCorner].z=asEdgeVertex[iVertex].z;
  }
 }
}
\end{lstlisting}
\vspace{5 mm}

Comentar que, para mejorar el rendimiento, la creación de esta malla 3D no se realiza en el hilo del render. En su lugar se realiza en el hilo del controlador RunningSceneController, que es el encargado de gestionar la escena actualmente en curso. Este controlador obtiene la nube de puntos pesada, crea un mallado 3D y sólo una vez creado le pide al hilo de render que actualice el modelo del usuario. De esta forma conseguimos evitar interrupciones innecesarias y pérdida de rendimiento en el hilo de render.
\\

Como resultado, en la figura \ref{F_MarchingCubesAvatar_Etapa5} se muestra a modo de ejemplo la malla3D construida para la silueta detectada. La nube de puntos simplificada que representa esa figura y con la que se ha creado la malla 3D tiene apenas 30 puntos aproximadamente.
\\

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/etapa5_marchingcubesavatar_production.png}
\end{center}
\caption{ \label{F_MarchingCubesAvatar_Etapa5} Marching Cubes para construir una representación 3D del avatar.}
\end{figure}

\item Prod3DEntity: Las modificaciones que se han añadido en Prod3DEntity persiguen dos objetivos. Por un lado, añadir riqueza a la representación de una entidad, añadiendo atributos como velocidad, fuentes de audio, tiempo de vida, etc. Por otro lado, extender sus capacidades de gestión dentro del kit de desarrollo. 
\\

Para ello, el primer cambio ha sido hacer que Prod3DEntity herede de la clase abstracta IEntity, de forma que la entidad pueda ser manipulada por otros componente con idependencia de su implementación. También se añaden métodos para obtener o asignar atributos en bloque que suelen ser accedidos juntos, de forma que pueden hacerse en una sola llamada, lo que permite reducir el número de peticiones y bloqueos de la entidad.
\\

Finalmente, se añaden métodos pensando en el flujo de ejecución y actualización de las entidades, destacando el método virtual OnStart() pensado para que el desarrollador pueda redefinir el comportamiento que va a tener una entidad nada más ser introducida en la escena, e igualmente OnUpdate() para personalizar su comportamiento permitiendo evaluar o a hacer modificaciones al comienzo de cada ciclo de ejecución de cada fotograma, o los métodos OnCollisionCall() y OnUserCollisionCall(), que le permitirán reaccionar a las colisiones contra otras entidades o contra el usuario, llamados durante la evaluación de colisiones.

\begin{lstlisting}[language=C++]
class Prod3DEntity : public core::IEntity
{...
  virtual bool IsCollidable(){boost::mutex::scoped_lock lock(m_mutex);return collidable;}
  virtual bool IsReadyToDie();
  virtual void SetCollidable(const bool &value)
  virtual float GetTimeToLive()
  virtual void  SetTimeToLive(const float &value)
  virtual void SetPositionVelocityAcceleration(...,...,...);
  virtual void GetPositionVelocityAcceleration(...,...,...);
  void SetPositionOrientationScale(...,...,...);
  void GetPositionOrientationScale(...,...,...);
  virtual void DeletePersistence();

  virtual void OnStart(){};
  virtual void OnUpdate(){};
  virtual void OnCollisionCall(IEntity *otherEntity){}; 
  virtual void OnUserCollisionCall(core::corePDU3D<double> collisionInfo){};
  virtual void StartAnimations();
...
};  
\end{lstlisting}

\item MainProd: 
Por un lado, se añaden métodos para la gestión de las entidades, por ejemplo para poder solicitar la carga y eliminación de entidades de la escena actual de forma segura. Estas acciones no se resuelven en el momento de la petición, ya que para poder realizarse de forma segura deben ejecutarse en un punto concreto del ciclo de render. 
\\

Por ejemplo, la carga de nuevas entidades se resuelve mediante el encolado de las peticiones, durante la cual también se añaden modificaciones para reproducir animaciones del modelo 3D o añadir sonido como extensión a la implementación anterior. Para el caso de retirar las entidades no se usa una cola en sí, ya que el motivo para eliminar una entidad puede tener diferentes orígenes. En su lugar, cuando se visitan las entidades para ejecutar sus Updates, se comprueba primero si tiene activado un atributo que indica que la entidad está lista para ser retirada. Así en lugar de actualizarla se procede a su eliminación. También se añaden métodos para gestionar su ejecución durante el tiempo que existan dentro de la escena, y sus reacciones al colisionar con otras entidades.
\\

En relación a la representación del avatar, aunque la representación del usuario como una figura 3D representación es muy interesante y se conserva en el kit de desarrollo, si lo que se desea es proyectar una presencia en profundidad de forma que al moverse el usuario pueda golpear objetos que se encuentran delante suyo a una determinada distancia, es más conveniente usar otras soluciones.
\\

Así, partiendo de la misma nube de puntos de simplificada, para esta instalación en lugar de crear una representación de su superficie en forma de malla 3D creamos, por cada punto, un colisionador con forma de cápsula proyectada en profundidad cuyo radio es proporcional al tamaño del punto.
\\

Estos colisionadores se crean de la misma forma que fue comentada en la sección \ref{etapa4Implementacion:produccion}, página \pageref{etapa4Implementacion:produccion}. La solución en este caso puede ser mejorada tiendo en cuenta la proyección de perspectiva, pero para la aplicación que se desea y con una configuración estándar de la cámara, y limitando las colisiones a una distancia no muy alejada los resultados conseguidos son satisfactorios. Mejoras en este campo se plantean como trabajo futuro. A modo de ejemplo, en la siguiente figura \ref{F_ColisionadoresAvatar_Etapa5} se muestra la proyección de los colisionadores a partir de los puntos que forman la representación del avatar del usuario.

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/etapa5_colisionesavatar_production.png}
\end{center}
\caption{ \label{F_ColisionadoresAvatar_Etapa5} Proyección de la representación del avatar.}
\end{figure}

En el cuadro siguiente pueden verse las modificaciones necesarias:

\begin{lstlisting}[language=C++]
void MainProd::Iterate()
{current_timestamp = (double)clock()/CLOCKS_PER_SEC;
 CheckCollisions(); 
 UpdateEntities(); 
 ProcessProd3DEntitiesToBeLoadedQueue();
 for (unsigned int i=1; lock&&(i<=num_windows); i++)
  CheckDesiredBackgroundAndFogRanges()) 
 framework.do_frame(graphic_thread);				 
}

void MainProd::CheckCollisions() 
{for (int i=0;i<collision_handler_queue->get_num_entries();i++)
 {collision_entry=collision_handler_queue->get_entry(i);
  if(collision_entry)
  {into_object=collision_entry->get_into();
   from_object=collision_entry->get_from();
   if(into_object&&from_object)
   { found=avatar_collider_array.find(into_collision_node);
     if (found!=avatar_collider_array.end()) 
     {avatar_collision_info=avatar_colliders[into_collision_node];
      entities_colliders[from_object]->OnUserCollisionCall(
                                       avatar_collision_info);
     }else
      entities_colliders[from_object]->OnCollisionCall(
                                       entities_colliders[
                                       into_object]);
}

void MainProd::UpdateEntities() 
{for (iter=scene_entities.begin();iter!=scene_entities.end();)
 if((*iter)&&((*iter)->IsReadyToDie()))
 {entity_to_remove=(IEntity*)(*iter);
  PrivateRemoveEntityFromCurrentWorld(entity_to_remove);
 }else (*iter)->OnUpdate();
}

void MainProd::AddNewEntitiesIntoCurrentWorld(new_entities_after_seconds)
{for (iter=new_entities_after_seconds.begin();...)
  if((iter->first)&&(current_world))
  {prod3d_ent=(Prod3DEntity *)(iter->first);
   desired_timestamp=(double)clock()/CLOCKS_PER_SEC+iter->second;
   entity_array_to_be_loaded_afterseconds[prod3d_ent]=desired_timestamp;
}}   

void MainProd::LoadEntityIntoScene(Prod3DEntity * entity)
{scene_entities.push_back(entity);
 scene_entities_nodepaths[entity]=new_model;
 scene_nodepaths_entities[new_model]=entity;
 entity->SetNodePath(scene_entities_nodepaths[entity]);
 entity->StartAnimations();
 if (entity->IsCollidable())
 entity_collidable_array_to_register.push_back(entity);
}
\end{lstlisting}

La otra línea de trabajo, desde este apartado, es la de ofrecer algunos mecanismos en el módulo de Producción para la personalización de la escena. Se añaden métodos para cambiar el color de fondo, el color o la intensidad de la niebla, los audios de ambiente que deben sonar de forma absoluta (no relativos al usuario) o algunos ejemplos simples de postproceso. El motor gráfico ofrece una serie de efectos de postproceso predefinidos básicos y con poca capacidad de personalización como Invert, Bloom, Blur o Cartoon para añadir de forma simple un acabado diferente al render. Es posible definir efectos propios o incluso shaders específicos para cada tipo de entidad. Este es un apartado muy interesante dentro del mundo de los gráficos por computador. Sería interesante ahondar en este apartado en el futuro.
\\

En el siguiente bloque puede verse un ejemplo de estos cambios:

\begin{lstlisting}[language=C++]
void MainProd::EnableSimpleBloomEffect(const bool &enable)
{for (iter=pandawindows_array.begin();...)
  if (enable && !enable_simpleBLOOMEffect)
  {CCommonFilters::SetBloomParameters bloom_paramns; 
   bloom_paramns.blend=LVector4f(0.33,0.34,0.33,0.0);
   bloom_paramns.mintrigger=0.90f;
   bloom_paramns.maxtrigger=1.0f;
   bloom_paramns.desat     =0.8f;
   bloom_paramns.intensity =0.9f;
   bloom_paramns.size      ="medium";
   ccommonfilters_array[iter->first]->set_bloom(bloom_paramns);
   }else if(!enable)ccommonfilters_array[iter->first]->del_bloom();}

std::string  MainProd::AddBackgroundSound(file_name)
{ new_buffer->LoadFromFile(file_name)
 core::corePoint3D<double> position;
 position.x = 0; position.y = 0; position.z = 0;
 sf::Sound *new_sound = new sf::Sound();
 new_sound->SetBuffer(*new_buffer);
 new_sound->SetLoop(true);
 new_sound->SetPosition(position.x, position.y, position.z);
 new_sound->SetRelativeToListener(false);
 new_sound->SetMinDistance(10.f);
 new_sound->SetAttenuation(0.75f);
 new_sound->SetPitch(1.0f);
 new_sound->SetVolume(100.f);
 new_background_melody(position,new_sound,new_buffer,...);
 music_melody_samples[file_name]=new_background_melody;
 new_sound->Play();
}
void SetPitchBackgroundSound(id,value);
void SetAmplitudeBackgroundSound(id,value);
std::vector<std::string> GetBackgroundSounds();
void RemoveBackgroundSound(id);
void RemoveAllBackgroundSound();
\end{lstlisting}

\end{itemize}

\subsubsection{Módulo de Inteligencia Artificial}\label{Section_IACommonSwarmIndividual_Etapa5}

\begin{itemize}
\item CommonSwarmIndividual: 
Se ofrece en el módulo de icog una implementación de una IA genérica que puede servir para crear comportamientos de individuos de una especie, que contiene los componentes básicos de Boids (separación, cohesión y alineamiento) y extiende algunos componentes adicionales (límites, aleatoriedad, evasión y atracción). 
\\

La solución adoptada está inspirada en la implementación clásica del Boids, un programa creado por Craig Reynolds \cite{CRAIGREYN}. En este modelo todas las entidades tienen conocimiento del mundo en el que se encuentran, aunque la cantidad de datos que usan estará limitada por su campo de visión. Según la información del mundo que tienen, su comportamiento se centra en evaluar tres componentes que definirán el siguiente cambio de posición del individuo.
\\

Estos componentes clásicos son los siguientes:

\begin{itemize}
\item Cohesión: Los individuos tienden a acercarse entre sí. 
\item Separación: Los individuos tienden a mantener una distancia mínima entre ellos.
\item Alineación: Los individuos tienen a moverse en la misma dirección y velocidad.
\end{itemize}

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/etapa5_boids_icog.png}
\end{center}
\caption{ \label{F_Boids_Etapa5} De izquierda a derecha: Cohesión, Separación y Alineación \cite{CRAIGREYN}.}
\end{figure}

Se extiende el módulo básico con los siguientes componentes:

\begin{itemize}
\item Randomicidad: Los individuos tienden a moverse de forma aleatoria.
\item Evasión: Los individuos tienden a evadir algún elemento.
\item Atracción: Los individuos tienen a sentirse atraídos por algún elemento.
\item Límites: Los individuos tienden a no traspasar ciertos límites del espacio.
\end{itemize}

Según se asigne mayor o menor peso la especie definida por este comportamiento tenderá a darle más importancia a una componente o a otra. 
\\

Cada individuo tiene conocimiento de su entorno, aunque para evaluar su estado actual usará sólo información que esté dentro de su campo de visión. Teniendo en cuenta todas sus necesidades (separación, cohesión, alineación, límites, evasión, atracción y randomicidad) y el factor de peso de cada una de ellas, el individuo modificará su comportamiento expresado como posición, velocidad y aceleración para cumplir esos objetivos. Para ello cada componente definirá un vector de fuerza, que se será multiplicado por su peso y sumado con el resto para promediar.
\\

Cada vector de fuerza se calcula de la siguiente forma:
\begin{itemize}
\item Randomicidad: Un vector con valores aleatorios en los tres ejes, limitando a la aceleración máxima permitida.
\item Cohesión: El vector de fuerza se calcula como la diferencia entre la media de las posiciones de mis vecinos y el propio vector de posición del individuo. 
\item Separación: Por cada vecino del que se encuentre más cerca que una distancia umbral, se suman cada vector de diferencia, normalizados y multiplicados por la distancia deseada.
\item Alineación: El vector de fuerza se calcula como la media de las velocidades de los vecinos.
\item Evasión: Se calcula como la Separación, sólo con los objetos correspondientes (por defecto, cualquier entidad que no sea de su propia especie).
\item Atracción: Se calcula como la media de las diferencias de la posición del individuo y los objetos atractores que estén cerca.
\item Límites: Se calcula como la separación pero estableciendo las contantes que limitan la parte de la escena donde se desea mantener a la población.
\end{itemize}

Para acelerar los cálculos de medias se usan acumuladores estadísticos que van construyendo los valores de forma incremental a medida que se introducen muestras en la colección.
\\

Por otro lado, ContentCreationController mantiene una estructura de datos llamada Ecosistema, que agrupa todas las entidades actualmente presentes en la escena en curso, y es el encargado de coordinar los cambios para estas entidades. En cada iteración actualizará dicho catálogo. En la sección de Aplicación se verá cómo se mantiene actualizado y coherente. 
\\

Otro punto interesante a comentar es que no es necesario evaluar estas condiciones en todos y cada uno de los fotogramas. Aprovechando la proximidad temporal, al implementar una Entidad que incluya esta IA, dicha entidad podría llamar al método de evaluación cada cierto intervalo de tiempo, por ejemplo cada 0.1 segundos. De esta forma se consigue un comportamiento igualmente satisfactorio sin necesidad de evaluar el estado de todas las entidades en todos los fotogramas.
\\

Para resumir, con esta IA, variando las distancias y factores de peso, pueden representarse comportamientos de diferentes tipos de agrupaciones de individuos que se mueven como un conjunto. También puede extenderse o combinarse para formar parte de otros tipos de comportamientos o ejemplos de inteligencias artificiales más complejas como agentes inteligentes.

En el siguiente bloque puede verse a modo de ilustración las secciones principales de este componente:

\begin{lstlisting}[language=C++]
void CommonSwarmIndividual::Think()
{overlapping_size=RTree_spatialIndexes.Search(search_rect.min,
                  search_rect.max,Register_SearchResults_callback);
 ResetAccumulators();
 for (result_iter=RTree_search_results.begin()...)
 {if(other_id!=me_id)
  //SAME SPECIES
  {if (other_type==me_type)
   {if (use_world_limits&&outlimits) worldlimits=world_max-distance;
    if (std::abs(distance_separation)<separation_distance)
    {separation_position=(distance_separation>0.0f)?
                         other_position-separation_distance:
                         other_position+separation_distance;
    accumulators_separation(separation_position); }
    accumulators_cohesion(other_position); 
    accumulators_alignment(other_velocity);
   //DIFFERENT SPECIES 
   } else 
   {if(std::abs(distance_separation)<avoidance_distance)
    {separation_position=other_position-avoidance_distance;
     accumulators_avoidance(separation_position);}
    if(attraction)
     accumulators_attraction(other_position);
 }}}
 separation=CalculateSeparation();
 alignment=CalculateAlignment();
 cohesion=CalculateCohesion();
 attraction=CalculateAttraction();
 avoidance=CalculateAvoidance();
 randomness=RandomFloat(-1.0f*max_acceleration,max_acceleration);
 separation*=separation_factor;
 alignment*=alignment_factor; 
 cohesion*=cohesion_factor; 
 attraction*=attraction_factor; 
 avoidance*=avoidance_factor; 
 worldlimits*=worldlimits_factor;
 AddForce(randomness);
 AddForce(separation);
 AddForce(alignment);
 AddForce(cohesion);
 AddForce(attraction);
 AddForce(avoidance);
 AddForce(worldlimits);
 CheckPDULimits();
 csi_pdu.velocity+=csi_pdu.acceleration;
 csi_pdu.acceleration=0.0f;
 CheckPDULimits();
 csi_pdu.position+=csi_pdu.velocity;
}

void CommonSwarmIndividual::AddForce(force)
{csi_pdu.acceleration+=force;CheckPDULimits();}   
 
core::corePoint3D<float>CommonSwarmIndividual::CalculateSeparation()
{corePoint3D<float> result,position;
 int n_datum=extract_result<tag::count>(accumulators_separation);
 if((n_datum>0)&&csi_entity)
 {double mean=extract_result<tag::mean>(accumulators_separation);
  csi_entity->GetPosition(position.x,position.y,position.z);
  result=mean-position;}
 return result; }
\end{lstlisting}

\end{itemize}

\subsubsection{Módulo de Aplicación}

Finalmente, en el componente aplicación hacemos una muestra de creación de contenidos usando el kit de desarrollo, utilizando las nuevas funcionalidades.
\\

\begin{itemize}
\item Colección de contenidos: Con la Aplicación Blender \cite{BLENDER} se crean algunas figuras geométricas básicas, a las que se añaden una animación por defecto mediando el uso de key-shapes. Key-shapes son deformaciones claves de la malla 3D, de forma que pueden usarse para interpolar o mezclar deformaciones durante una animación. Esta técnica es muy usada, por ejemplo, para la sincronización de la forma de los labios según los sonidos que pronuncia un personaje, o en animación facial para representar las diferentes posiciones de cejas, párpados, mejillas, labios, etc. Para el caso de los Audios se ha recopilado desde FreeSound \cite{FREESOUND} una colección de muestras cuya licencia permitiera el uso de forma gratuita. Se han clasificado en Melodía, Base Rítmica y Decoraciones, una segunda clasificación por GOOD, NEUTRAL e EVIL, y una tercera por CALM y EXCITED. Para editar las muestras de Audio se usado la Aplicación Audacity \cite{AUDACITY}, un editor y grabador de audio libre.
\\

Para esta aplicación se han definidos las siguientes primitivas:
\begin{itemize}
\item Variación de un cubo que se convierte en una estrella.
\item Variación de un cubo que se convierte en una superficie plana.
\item Variación de un cubo que se convierte en una superficie plana.
\end{itemize}

Los audios:
\begin{itemize}
\item Base: Un clip de audio usado como base rítmica principal.
\item Melodías: Dos melodías por cada valor de psique: dos agradables, dos neutrales y dos más fuertes y agresivas.
\item Decoraciones: Un sonido de sónar para las acciones positivas y un sonido electrónico discordante para las negativas.
\end{itemize}

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/etapa5_aplicaciones_app.png}
\end{center}
\caption{ \label{F_Aplicaciones_Etapa5} Blender \cite{BLENDER}, FreeSound \cite{FREESOUND} y Audacity \cite{AUDACITY}.}
\end{figure}

\item Application: Añadimos los métodos SetCameraRecording y SetUseRecording para indicar que deseamos grabar de forma sincronizada de todas las cámaras o por lo contrario alimentar al sistema con imágenes obtenidas de ficheros de vídeo en lugar de usar las imágenes en vivo de las cámaras.  
\\

Se trata únicamente de conectar las peticiones realizadas por los controladores de GUI al componente de Percepción de Vídeo para que active o desactive la grabación, o active o desactive el vídeo de la captura de vídeo.
\\

También ofrecemos los métodos AddNewentityIntoCurrentWorld y RemoveEntityFromCurrentWorld que conectan con MainProd para indicar que deseamos añadir una nueva entidad a la escena actual o que queremos eliminarla.

\item RunningSceneController: Anteriormente llamado NavigatorController, extendemos su responsabilidad. De esta forma, el controlador mantiene la responsabilidad de navegar dentro de la escena y añade la de crear y mantener la representación del usuario, y actualizar y crear contenidos, haciendo uso del controlador ContentCreationController. 
\\

Los cambios realizados en este componente son menores y pueden verse a modo de resumen en este bloque:

\begin{lstlisting}[language=C++]
void RunningSceneController::Iterate()
{...
 perception->GetFeatureWeightedPositions("PRESENCE VOLUME",presence_volume,scale);
 if (app&&perception&&production)
 {if (presence_volume.size()>0) 
  {graphic_node=production->CreateGraphicNode(presence_volume);
   production->SetAvatar(graphic_node);}
  production->SetUserPosition(final_cam_pos); }
 ...
 if (contentCreationController)
  contentCreationController->Update();
}
\end{lstlisting}


\item ContentCreationController: Este controlador implementa la creación de contenidos durante una sesión en curso teniendo en cuenta información sobre la interacción del usuario. Mantiene un registro de las entidades que existen actualmente en la escena y tiene acceso a la información obtenida por percepción y decide qué contenidos crear en consecuencia. 
\\

Una de sus primeras tareas al iniciar la ejecución es crear el catálogo de recursos de audio.

\begin{lstlisting}[language=C++]
sound_filename_base << iapp_config->GetSoundDirectory();
sound_filename_MG1 << sound_filename_base.str() << "MG0008.wav";
sound_filename_MG2 << sound_filename_base.str() << "MG0010.wav";
sound_filename_MN1 << sound_filename_base.str() << "MN0001.wav";
sound_filename_MN2 << sound_filename_base.str() << "MN0003.wav";
sound_filename_ME1 << sound_filename_base.str() << "ME0004.wav";
sound_filename_ME2 << sound_filename_base.str() << "ME0002.wav";

std::vector<std::string> good_melodies;
std::vector<std::string> neutral_melodies;
std::vector<std::string> evil_melodies;

good_melodies.push_back(sound_filename_MG1.str());
good_melodies.push_back(sound_filename_MG2.str());
neutral_melodies.push_back(sound_filename_MN1.str());
neutral_melodies.push_back(sound_filename_MN2.str());
evil_melodies.push_back(sound_filename_ME1.str());
evil_melodies.push_back(sound_filename_ME2.str());

psique_melody[IA_Karma::GOOD]   = good_melodies;
psique_melody[IA_Karma::NEUTRAL]= neutral_melodies;
psique_melody[IA_Karma::EVIL]   = evil_melodies;
\end{lstlisting}

Otro de sus cometidos principales, durante toda la ejecución, es estudiar el comportamiento del usuario. Para ello se hace uso de una implementación de acumuladores estadísticos ofrecida por la librería Boost. Estos acumuladores permiten añadir muestras y extraer datos estadísticos de forma eficiente, construyendo soluciones de forma incremental. 
\\

Como puede verse en el siguiente bloque algunos de los datos que acumulamos son el número de muestras, media, mediana y varianza de la posición de la cabeza, centro de masas, dominancia lateral, orientación, excentricidad, cantidad en cada instante de segmentos móviles, colisiones positivas (tocar entidades) e interacciones negativas (golpear entidades). Para la implementación actual se hace uso de las interacción positivas y negativas para definir el color de fondos, y junto a este valor, la media del número de segmentos en movimiento, junto con la dispersión media de la excentricidad para calcular un valor de energía, que junto con el valor anterior decidirán los audio de la melodía principal que se reproducen.

\begin{lstlisting}[language=C++]
void ContentCreationController::Update()
{...
 //Feed accumulators
 accumulators_head_pos(head_pos);
 accumulators_presence_center_of_mass(head_pos);
 accumulators_main_lateraldominance(main_lateraldominance);
 accumulators_main_orientation(main_orientation);
 accumulators_main_eccentricity(main_eccentricity);
 accumulators_main_eccentricity.y(main_eccentricity);
 accumulators_main_eccentricity.z(main_eccentricity);
 accumulators_motion_NumElements(motion_elements.size());
 ...
 factor=0.8f*(n_motionElements/max_motion_mean_value)+
        0.2f*(dispersion_eccentricity/max_eccentricity_dispersion);	
 energy= factor*((float)IA_Energy::EXCITED); 
 ...
 if (change_music)
 {app_mainprod->RemoveAllBackgroundSound(5.0f);
  music_id=app_mainprod->AddBackgroundSound(current_melody, 5.0f);
  {boost::mutex::scoped_lock lock(m_mutex);
   background_sound=music_id;
   music_timestamp=current_timestamp;
   must_change_music=false;
   std::string decoration_sound_filename;
   if ((int)psique<psique_energy_decoration.size())
   {decorations_size=psique_energy_decoration
      [(int)floor(psique+0.5f)][(int)floor(energy+0.5f)].size();
    decoration_sound_filename=(*((psique_energy_decoration
      [(int)floor(psique+0.5f)][(int)floor(energy+0.5f)].begin())
      +(rand()%(psique_energy_decoration[(int)floor(psique+0.5f)]
      [(int)floor(energy+0.5f)].size()))));
   app_mainprod->AddBackgroundSound(decoration_sound_filename,5.0f);
   }ResetStatisticalAccumulators();}}
\end{lstlisting}

Por otro lado, es responsable de mantener actualizado el catálogo de entidades actualmente presentes en la escena. Para ello, en cada iteración consulta todas las entidades de la escena actualmente abiertas. Sin embargo, para permanecer actualizado ante cambios que pueden suceden entre iteraciones, hacemos que esta clase herede de la clase genérica Observer, y hacemos que observe a los dos tipos de objetos que pueden provocar cambios entre iteraciones, que son: 

\begin{itemize}
\item El controlador de sesión que indicará cuándo hay un cambio de escena. Cuando esto sucede, se debe reiniciar el estado actual, así como actualizar la escena y usuario actual.
\item Las propias Entidades, que le indicarán cuándo mueren o son destruidas, evento que puede ocurrir entre interaciones del bucle principal de este controlador.
\end{itemize}

Como Observador, ContentCreationController sobreescribe el método Notified para atender las notificaciones enviadas por cada uno de estos objetos. Algunos de los parámetros opcionales que se reciben son un puntero a la propia entidad notificadora, así como varios algunos atributos indicadores, con los que los Observables puedan comunicar mensajes. 

\begin{lstlisting}[language=C++]
void ContentCreationController::Notified(callinginstance,tag,flag)
{if ((tag=="RUN WORLD")||(tag=="CLOSE WORLD"))
 {boost::mutex::scoped_lock lock(m_mutex);
  DoNotifiedBySessionController(tag);
 }else if (tag=="DYING ENTITY")
  DoNotifiedByDyingEntity(callinginstance,tag,flag);}

void ContentCreationController::DoNotifiedBySessionController(tag)
{if (tag=="RUN WORLD")
 {current_world=app->GetCurrentWorld();
  current_user=app->GetCurrentUser();
  sesion_is_prepared=true;
  RestartCurrentUserBackgroundAndFog();
 }else if (tag=="CLOSE WORLD")
 {current_user=current_world=NULL;
  sesion_is_prepared=false; }
 ResetStatisticalAccumulators();}}

void ContentCreationController::DoNotifiedByDyingEntity(callinginstance,tag, flag)
{ ExtractFromEcosystem(flag,callinginstance);}
\end{lstlisting}

Para terminar, su último cometido es el de crear nuevas entidades. Para esta instalación se creará una Entidad de tipo StandAlone que será la creadora de la base rítmica de la composición musical, creándose una entidad cada segundo, que al nacer, reproduce un audio de percusión.
\\

Por otro lado, se crearán dos tipos de enjambres de entidades de tipo Boid Entities. Tras una colisión de tipo positiva se creará un enjambre de entidades con una configuración determinada (una gran cantidad de individuos, menos dispersos y con tendencia a agruparse). Tras una colisión de tipo negativa se creará un enjambre de entidades de menor número, más veloces y de mayor dispersión.

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/etapa5_secuenciaEntidadesBoids_app.png}
\end{center}
\caption{ \label{F_SecuenciaBoids_Etapa5} Secuencias: Sobre azul un tipo de enjambre tendiendo a juntarse, sobre rojo un tipo de enjambre de movimiento más veloz y errático.}
\end{figure}

\begin{lstlisting}[language=C++]
void ContentCreationController::Update()
{//CREATE STANDALONE ENTITIES
 if (current_timestamp-CreatedEntity_timesptamp>=CREATIONTIMELAPSE)) 
  CreatePresetOfEntities1(1.0f);}
  
void ContentCreationController::EntityHadAGoodUserFeedback(...)
{//CREATE SWARM ENTITIES
 if (was_good)
 {if(!(recover_createswarm1_afterseconds-current_timestamp>0))
   {CreatePresetOfSwarm1AtCoords(spawn_point,CREATESWARM1_AFTERSECONDS);
 }else
 {if(!(recover_createswarm2_afterseconds-current_timestamp>0))
   {CreatePresetOfSwarm2AtCoords(spawn_point,CREATESWARM2_AFTERSECONDS);}}
\end{lstlisting}

\item OXStandAloneEntity: Esta clase es una implementación de Prod3dEntity (que a su vez lo es de IEntity). Implementa un tipo de entidad que se muestra por sí misma y de forma independiente. 
\\

Representa un tipo de entidad básica. Cabe destacar sus métodos OnStart y OnUpdate llamados al comienzo de cada ciclo del pipeline de producción y que permite que cada entidad sea capaz de actualizar su estado según indique su implementación. También tiene los métodos OnCollisionCall y OnUserCollisionCall, llamadas cuando se detecta una colisión de esta entidad contra otra entidad o contra la representación del usuario. Por último mencionar que conserva datos como tiempo de vida, métodos para cambiar el tono o volumen de los audios que reproduce, etc.
\\

Sus métodos más distintivos son OnStart() y OnUpdate(). En el primero se inicializan todos sus atributos, especialmente las marcas de tiempo, y se reproduce el sonido de creación. En el segundo se actualizan todos los atributos que definen su comportamiento a lo largo del tiempo, fotograma a fotograma.
\\

\begin{lstlisting}[language=C++]
void OXStandAloneEntity::OnStart()
{boost::mutex::scoped_lock lock(m_mutex);
 start_timestamp=(double)clock()/CLOCKS_PER_SEC;
 if (!already_loaded_in_scene)
 { already_loaded_in_scene = true;
   if (sound_create.sound_data)
   sound_create.sound_data->Play();}}

void OXStandAloneEntity::OnUpdate()
{{boost::mutex::scoped_lock lock(m_mutex);
  current_timestamp=(double)clock()/CLOCKS_PER_SEC;
  lived_time=current_timestamp-start_timestamp;
  delta_time=current_timestamp-latestupdate_timestamp;
  if ((lived_time>time_to_live)||((killme_afterseconds>0)&&
              (killme_afterseconds-current_timestamp<0.0f)))
  {KillMyself();}
  latestupdate_timestamp = current_timestamp;
  //Animate if touched
  float rotation_speed_touchingfactor=1.0;
  bool animate_touching=recovercollisions_afterseconds-current_timestamp>0.0;
  if (animate_touching)
   rotation_speed_touchingfactor=1.75*(recovercollisions_afterseconds-current_timestamp);
  //Update entity
  this->GetPositionOrientationScale(x, y, z,
                                    h, p, r, 
                                    scale);
  scale=(killme_afterseconds-current_timestamp>0.0) ?
        scale*0.5*(killme_afterseconds-current_timestamp):scale;
  pdu.position.y-=delta_time*3.0f;
  this->SetPositionOrientationScale(
                  pdu.position.x, pdu.position.y, pdu.position.z,
                  h+delta_time*12.0f*rotation_speed_touchingfactor,
                  p+delta_time*60.0f*rotation_speed_touchingfactor,
                  r+delta_time*120.0f*rotation_speed_touchingfactor,
                  scale+0.0001f);
  this->SetPositionVelocityAcceleration(...);
}
\end{lstlisting}


\item OXBoidsEntity: Igualmente es una implementación de Prod3DEntity (que a su vez lo es de IEntity). 
\\

Implementa un tipo de entidad que pertenece a una especie gregaria y cuyo comportamiento se define por su conocimiento del entorno y del de los integrantes de su propia especie. Para conseguir esto, hereda de CommonSwarmIndividual, una implementación básica de una IA gregaria que se ofrece en el módulo iCog del SDK.
\\

La primera modificación notable está en el constructor, donde podemos pasar por parámetro todos los atributos que definen el comportamiento de la entidad como miembro de una especie.

\begin{lstlisting}[language=C++]
OXBoidsEntity::OXBoidsEntity(core::IEntityPersistence* ent,
                             const int &type_,
                             const float &max_acceleration_,
                             const float &max_velocity_,
                             const float &perception_distance_,
                             const float &separation_distance_,
                             const float &avoidance_distance_,
                             const float &randomness_factor_,
                             const float &separation_factor_,
                             const float &alignment_factor_,
                             const float &cohesion_factor_,
                             const float &attraction_factor_,
                             const float &avoidance_factor_,
                             const float &worldlimits_factor_,
                             const float &pitch, 
                             const float &amplitude)
{...}
\end{lstlisting}

La segunda se encuentra en la redefinición del método OnUpdate(), donde en esta ocasión ejecutaremos el método Think() de la IA, estudiado anteriormente en el componente 'CommonSwarmIndividual', visto en la sección \ref{Section_IACommonSwarmIndividual_Etapa5}, en la página  \pageref{Section_IACommonSwarmIndividual_Etapa5}. 
\\

Think() evaluará el estado actual de la entidad y el de su entorno, para aplicar los cambios en correspondencia con la definición de su comportamiento como individuo de una especie. 
\\

Mencionar que como medida de mejora de rendimiento, la IA no es evaluada en cada fotograma. Para los comportamientos que queremos ofrecer no es necesaria una precisión tan elevada sobre la evaluación de su entorno. Establecemos pues una frecuencia para esta actualización, de forma que sólo se volverá a evaluar la IA si ha pasado más de una cantidad de tiempo usada como umbral. Para el resto de iteraciones se interpolará con los últimos datos obtenidos.
\\

En el siguiente cuadro se reflejan las principales diferencias en la redefinición del método OnUpdate().
\\

\begin{lstlisting}[language=C++]
void OXBoidsEntity::OnUpdate()
{
 bool interpolate_pdu=true;
 if (think_time-current_timestamp<0.0f ) 
 { Think();
   interpolate_pdu=false;}
  ...
  //Update entity
  if (interpolate_pdu)
  {csi_pdu.velocity+=csi_pdu.acceleration;
   csi_pdu.acceleration=0.0f;
   csi_pdu.position+=csi_pdu.velocity;
   pdu=csi_pdu;}
  this->SetPositionOrientationScale(...);
  this->SetPositionVelocityAcceleration(...);
\end{lstlisting}
\end{itemize}

\section{Validación y Publicidad}
\subsection{Validación}
\begin{itemize}
\item Comprobación de uso de recursos de la máquina mediante las herramientas del sistema. En la máquina en la que se desarrolla la aplicación muestra consumir una media de un 21\% de CPU, con 27 subprocesos asociados y una media de 350Mb de memoria con el escenario por defecto de prueba. En esta ocasión se aprecian variaciones al cambiar los contenidos multimedia, lo que se nota especialmente con la carga de audios más pesados, lo cual es esperado. Como a lo largo del desarrollo del proyecto, se aprecia que la carga de trabajo se mantiene balanceada entre las CPUs del equipo. 

\item Respecto a a la comprobación de la ejecución y cierre correctos, así como pérdidas de memoria, se detectaron a lo largo del desarrollo de esta fase varios bugs de importancia. 
\\

El más grave de ellos se trataba de varias pérdidas de memoria que oscilaban en torno a los 10Kb y los 100Kb, pero que al producirse en un bucle a muy alta frecuencia provocaban un consumo muy elevado de memoria. En apenas 5 minutos consumía 1.5Gb. El segundo conjunto de bugs importantes eran cierres inesperados durante el arranque y cierre de sesión.\\
\\

En el momento actual se han corregido las pérdidas de memoria durante la ejecución que se han detectado. Se sigue detectando una acumulación de memoria, localizada en una sección de código donde no debería ocurrir asignaciones dinámicas de memoria. Se sospecha que la estructura de datos utilizada acumula memoria con el tiempo. En cualquier caso la acumulación de memoria debería permitir sesiones de uso de más de un día de ejecución, por lo que se considera asumible y no se da prioridad a su solución. Para resolverlo se propone estudiar en mayor profundidad la estructura de datos ofrecida por la librería Boost, y en su caso: corregir su uso, corregir la librería, sustituirla por otra o hacer una implementación propia.
\\

En relación al segundo conjunto de bugs, los cierres inesperados durante el arranque y el cierre de la aplicación estaban debidos a un problema de coordinación de hilos, de fácil solución. Permanece un cierre inesperado que sucede inmediatamente tras ejecutar. Se sospecha que puede estar relacionado con el controlador de audio que en ocasiones anteriores provocaba fallos similares. No se ha conseguido resolver, pero su frecuencia de aparición parece baja (1/15 arranques), sucede antes de que se muestre ninguna interfaz de la aplicación, e inmediatamente después es posible volver a ejecutar. 

\item Uso de herramientas para medición de fotogramas por segundo para comprobar el rendimiento de la ventana de visualización. No se aprecian cambios en la tasa de fotogramas por segundo que se mantiene a 60fps para dos ventanas de visualización. En ocasiones donde aparece una cantidad elevada de entidades, sí han podido apreciarse picos puntuales que bajan a 50 o 45 fotogramas por segundo. Por lo general, no suelen ser apreciables.

\end{itemize}
\vspace{5 mm}

\subsection{Publicidad}

Se plantea a dos compañeros y amigos la creación de prototipo de instalación para la grabación de un vídeo de muestra. 
\\

Para ello se cuenta con la ayuda de Óscar Rodríguez Vila, Director Creativo en Oquio, escultor y con experiencia en el montaje de instalaciones, y con Gloria Godínez, profesora de filosofía, bailarina, coreógrafa y también con experiencia en el montaje escénico.
\\

El propósito de esta acción es:
\begin{itemize}
\item Contrastar ideas y ver la mejor forma de montar un espacio físico que pudiera exponerse en un futuro.
\item Conocer mejor la escena artística desde el punto de vista de personas que han creado y expuesto trabajos.
\item Aunque la instalación está pensada para ser visitada por público de calle, ponerla en manos de una profesional de la escena artística y conocer su opinión sobre el potencial para crear instalaciones orientadas a performances.
\item Hacer una grabación a modo de ejemplo que permita mostrar la aplicación.
\end{itemize}


\chapter{Resultados y conclusiones}

Ha llegado el momento de echar un vistazo atrás y ver el trabajo realizado. La creación de este proyecto ha dado la oportunidad de explorar y aprender conceptos y técnicas que no quería dejar pasar y que para mí representan el eje central de mi pasión por la informática: la resolución de problemas complejos buscando soluciones creativas. Uniendo, además, dos campos que son el motor de mi vida, Ciencia y Arte. Es de esta forma como deseaba dar el cierre a esta etapa de formación académica como Ingeniero Informático, estudiando problemas complejos, evaluando herramientas y construyendo una solución.
\\

Puedo decir que el desarrollo de este trabajo ha tenido un gran impacto en la calidad del trabajo que realizo y que me ha aportado la capacidad de desarrollar herramientas de gran valor tanto a nivel personal como profesional.
\\

A modo de resumen se listan algunos de los hitos más representativos:

\begin{itemize}
\item Se ha creado un kit de desarrollo software con el que se pueden crear instalaciones artísticas basadas en técnicas de percepción e incluir componentes de vida artificial, con el que además se ha creado una propuesta de instalación a modo de ejemplo.

\item Este kit se ha desarrollado de forma que la implementación de sus componentes es desconocida para el resto de componentes, que se comunican mediante una interfaz que otorga independencia y la capacidad de, llegado el momento, cambiar la tecnología con la que ha implementado uno de ellos, sin que afecte a los demás.

\item Como resultado tenemos siete proyectos. Uno principal que genera un ejecutable, y otros seis que aportan interfaces o generan librerías que pueden ser consumidas entre sí o por terceros, por ejemplo, la aplicación principal.

\item Al comienzo de este proyecto era extraño ver computadores de más de dos núcleos, pero era evidente que la tendencia indicaría que tenderían a popularizarse. Ésta era una de las capacidades que más querían explotarse. Hasta el momento no había desarrollado aplicaciones que explotaran, en esta medida, esta capacidad. A modo de anécdota, una de las primeras sorpresas fue ver que preparar una aplicación como multihilo no implicaba que se usaran todas las CPUs físicas, por defecto todos los hilos creados se ejecutaban en la misma CPU física. 

\item Tanto el kit como la aplicación pretende explotar en la medida de lo posible la concurrencia de procesos. La cantidad de hilos en ejecución dependerá del número de componentes activos. A modo de ejemplo, cada detector sobre cada fuente, es ejecutado y procesado en un hilo independiente. A modo ilustrativo, durante una ejecución podemos tener alrededor de 20 hilos de ejecución en paralelo.

\item Aunque se han acortado las intenciones iniciales, se ha podido estudiar diferentes técnicas de proceso de imagen y visión por computador y los conceptos sobre los que se basan: sustracción de fondo, entrenamiento y uso de clasificadores, detectores y reconocedores, cálculo de información enriquecida a partir de una imagen como centro de masas, área, orientación o excentricidad, etc.

\item Aunque es un campo en el que me habría gustado profundizar más, ha sido muy interesante evaluar y desarrollar un planteamiento sobre la composición de contenidos, usando elementos gráficos y musicales.

\item Otro de los apartados que ha resultado de gran interés ha sido la búsqueda, análisis y estudio de herramientas, siendo un elemento curioso ver cómo algunas de esas herramientas han comenzando a popularizarse recientemente. Otras en cambio, que parecían estar a la vanguardia, han caído o empeorado de forma notable.

\item Sin duda, una de las tareas que más he disfrutado ha sido analizar los problemas e idear soluciones, aspecto que me resulta especialmente atractivo en la informática.

\item Otro punto que me gustaría resaltar es la ausencia de dispositivos con capacidad de ser usados de forma abierta por desarrolladores como la actual Kinect de Microsoft, conocida primero como Project Natal \cite{KINECT09}, para el momento de este desarrollo. La Kinect fue anunciada por primera vez en 2009 y comenzó a poder usarse en PCs oficialmente en 2011. Cuando fue anunciada, en el proyecto ya se había desarrollado por completo todo el módulo de percepción (salvo los añadidos de la Etapa 5). De todas formas, OpenCV ha sido igualmente interesante e instructivo. En cualquier caso, sería  interesante como trabajo futuro evaluar la capacidades de la Kinect para sustituir el actual módulo de percepción por una reimplementación que la utilice.

\item Una de las mayores dificultades ha sido la integración de tecnologías muy diferentes, especialmente cuando imponían restricciones o hacían uso de librerías de sistema o de terceros que eran incompatibles. En algunas ocasiones, se han hecho correcciones y modificaciones en esas mismas librerías, algunas enviadas y admitidas en revisión.

\item Otro punto a mencionar es sobre la planificación y seguimiento del proyecto. Se ha realizado una planificación a largo plazo debido a que además de alumno, también me encontraba trabajando, con poca experiencia, pero con una dedicación completa. Debido a ello, se inició una planificación original de dos años usando exclusivamente los tiempos libres. Esta planificación consiguió cumplirse y mantenerse correctamente, haciendo ajustes necesarios durante su ejecución, pero manteniéndose razonablemente en plazos. Sin embargo, por motivos personales y profesionales el proyecto ha sufrido una pausa importante en su etapa final. Al retomar el trabajo, se volvió a planificar para completar el desarrollo de esta última etapa en tres meses, y así se ha hecho. 
\item Este proyecto se entrega bajo licencia Creative Commons BY 1.0 \cite{CCBY10}. No pretende ser una solución universal y, sin duda, puede ser mejorada y extendida. Sin embargo, se tiene el convencimiento de haber creado una librería lo bastante completa y flexible que puede ser una base interesante para otros desarrolladores que quieran crear sus propias instalaciones, teniendo una gran parte de trabajo realizado, y pudiéndose centrar más en la parte creativa. Para ello, el proyecto ha sido subido en Github y puede accederse desde esta url: https://github.com/Adrasl/OX/. Hay que tener en cuenta, que por motivos de espacio y guía de uso de Github, no se han subido las librerías precompiladas de terceros. En el Apéndice B de esta memoria se detallan qué librerías se han usado y se pueden encontrar las referencias oficiales a las instrucciones para su compilación.
\item Se ha creado una página web para dar una presencia pública y facilitar la visibilidad. Para ello se ha creado una página gratuita en WIX a partir de una plantilla simple que permita mostrar como elemento principal un vídeo de presentación, algunas imágenes y enlaces. Se añaden algunos elementos de SEO pero por el momento no se plantea establecer un dominio propio ni eliminar la promoción propia de WIX. La página puede accederse desde la dirección https://garionsl.wix.com/projectOX/.
%\item Para acompañar esta presentación se ha creado un vídeo de demostración donde se graba una performance mostrando su uso durante una sesión. Puede encontrarse igualmente en la página web del proyecto en https://garionsl.wix.com/projectOX/.
\end{itemize}

\chapter{Trabajo Futuro}

Aunque el trabajo ha sido bastante extenso y se ha procurado ofrecer una librería lo más completa posible, no hay duda de que hay lugar para realizar mejoras y para extender sus capacidades. Especialmente por ello, se ha querido ofrecer la librería bajo la licencia Creative Commons BY 1.0 \cite{CCBY10}. En este apartado se hace una relación de las posibles líneas de trabajo futuro que pueden resultar de interés, a partir del ya realizado.

\begin{itemize}
\item OX Instalación:
\begin{itemize}
\item Crear una instalación física que pueda ser montada y expuesta.
\item Mejorar la propuesta de instalación enriqueciendo sus contenidos.
\end{itemize}
\item OX SDK: 
\begin{itemize}
\item Promoción y visibilidad: Dar a conocer la librería mediante promoción online y referencias desde sitios de terceros. Participar en eventos de sectores relacionados. Contactar con museos, universidades, salas de exposición, etc.
\item Puesta al día: Revisión de todos los componentes y librerías de terceros utilizados para actualizar a las versiones más recientes, en la medida de lo posible. Preparar el proyecto igualmente para IDEs más recientes.
\item Ports: Aunque el código desarrollado es C++, se han usado librerías disponibles para multiplataforma (al menos Windows y Linux), y se ha procurado implementar las soluciones de forma independiente al sistema operativo, el proyecto puede contener referencias explícitas al SO. Interesaría realizar una revisión del proyecto y preparar un port testeado para Linux y Mac con un IDE open source como Eclipse.
\item Persistencia: 
\begin{itemize} 
\item Crear una implementación de persistencia que conecte con un webservice en lugar de conectar directamente con una base de datos.
\item Estudiar el uso de Bases de Datos Orientadas a Objetos y Bases de Datos  para Tiempo Real.
\end{itemize}
\item Conectividad: Crear un esquema multiusuario online, que permita que varias instalaciones en diferentes localizaciones del mundo puedan estar conectadas entre sí, pudiendo incluso en ocasiones, entrar en los mismos espacios de otros usuarios y poder ver sus respectivas representaciones e interacciones. Añadir un esquema de permisos que permita personalizar las capacidades de interacción.
\item Percepción: 
\begin{itemize}
\item Estudiar alternativas y mejoras en los algoritmos y técnicas empleadas.
\item Estudiar las posibilidades de añadir el dispositivo XBox Kinect de Microsoft.
\item Incluir reconocimiento postural, de género, sexo y edad.
\item Incluir reconocimiento de expresiones faciales.
\item Estudiar la posibilidad de conseguir un sistema parallax que a partir del seguimiento de la cabeza y del estudio del punto de visión consiga una navegación más cercana a la navegación de realidad virtual.
\item Incluir análisis de señales de audio, para lo cual se recomiendo revisar Marsyas, una librería open source en C++ orientada al análisis y síntesis de audio \cite{MARSYAS}.
\item Mejorar las capacidades para combinar múltiples fuentes de vídeo y audio.
\end{itemize}
\item Producción:
\begin{itemize}
\item Sustituir el motor gráfico Panda3D por Ogre3D. En su momento, dentro de los motores gráficos Panda3D era un motor de vanguardia con un buen diseño de API. Con el tiempo ha ido decayendo y no tiene un buen mantenimiento. Revisitando los posibles motores gráficos se propone Ogre3D como mejor alternativa.
\item Añadir más capacidades de personalización de contenidos: luces, tipos de objetos, creación procedimental de objetos y texturas, texturas dinámicas y volumétricas, sistemas de partículas, luces, personalización del tipo de cámara o proyección, efectos de postproceso y shaders.
\item Mejorar el esquema de interacción usuario-escena. En esta línea una de los posibles trabajos sería la creación de un esquema de colisiones proyectando la representación del usuario en correspondencia con la proyección de la cámara.
\item Continuar el estudio e implementar una solución para creación procedimental de composiciones musicales, mediante la creación de una gramática. Para lo que se recomienda profundizar en conceptos de composición musical. Igual que para el análisis, podría usarse Marsyas \cite{MARSYAS} como herramienta para el desarrollo de este apartado. Por si el lector sintiera especial interés en esta línea, me gustaría hacer referencia a algunas lecturas como 'A Practical Guide to Musical Composition' de Alan Belkin \cite{BELKIN99}, 'AI and Music: From Composition to Expressive Performance' de Ramón López et al. \cite{RLOPEZ02} y otros \cite{MCCORMACK}, \cite{CURTISR}, \cite{LOYABBOT}.
\item Mejorar las capacidades para combinar múltiples destinos de vídeo y audio. 
\item Sería especialmente interesante plantear una combinación de técnicas de percepción para el estudio de un sujeto junto con Realidad Virtual o Aumentada. En esta línea es interesante revisar las siguientes referencias a Oculus Rift \cite{OCULUSR} y vrASE \cite{VRASE}.
\end{itemize}
\end{itemize}
\end{itemize}


% EMPIEZAN LOS APENDICES DEL PROYECTO
\appendix

\chapter{Manuales de usuario}

La instalación está pensada para ser un espacio que se comunica con la persona que se encuentra en ella. El sujeto que entra dentro de la instalación no ve en ningún momento ninguna interfaz y no existe ninguna guía de uso. Será su comportamiento dentro del espacio el que definirá el contenido que se cree.
\\

Desde el punto de vista del desarrollador que desee utilizar la aplicación desarrollada o la librería, se necesita tener en cuenta varios aspectos que son comentados en el Apéndice B que se encuentra a continuación, al final de esta misma memoria. Hay que tener en cuenta que junto con los fuentes del proyecto no se distribuyen los binarios de las librerías de terceros. 
\\

Se recomienda usar versiones recientes de las mismas, pero es importante tener en cuenta que este tipo de actividad puede generar problemas de compatibilidad. Si se diera el caso, se recomienda usar la misma versión comentada en el anexo. Igualmente se recuerda al desarrollador que pueden surgir problemas de compatilidad durante la ejecución o durante la vinculación si se usan librerías ya precompiladas con compiladores diferentes o con opciones de compilación diferentes al usado para compilar el proyecto. 
\\

También se recuerda al desarrollador que el código de este proyecto se ofrece 'como es' (As-is) sin ninguna garantía explícita o implícita. El proyecto ha sido desarrollado con propósito académico y se ofrece bajo licencia Creative Commons BY 1.0 \cite{CCBY10}. Sin embargo, los fuentes que pueden descargarse del repositorio pueden contener elementos de terceros que se encuentran bajo una licencia diferente. Se recomienda al desarrollador que revise la documentación de los mismos. El desarrollador es responsable del uso adecuado de dicho contenido.

\chapter[Detalles Implementación]{Detalles técnicos sobre la implementación del proyecto}

En este apéndice se detallan algunos aspectos técnicos relacionados con la implementación del proyecto. También se describen las modificaciones realizadas sobre las librerías de terceros, y cómo preparar la solución del proyecto en Visual Studio 2008 Express Edition; aunque igualmente puede servir como guía para portar el proyecto a otro entorno de desarrollo. 
\\

\section{Incompatibilidades}\label{detallesImp:Incompatibilidades}
Uno de los aspectos a tener en cuenta es que al usar una cantidad considerable de librerías externas aparecen con frecuencia incompatibilidades que deben ser resueltas. Algunas de estas incompatibilidades pueden afectar sólo a los proyectos de los que dependa un módulo concreto o a la totalidad del proyecto.
\\
\subsection{winsock}
Uno de los problemas más comunes fue la incompatibilidad entre librerías relacionado con librerías del sistema operativo, como fue el caso de winsock. Varias de las librerías usan módulos relacionados incompatibles; por ejemplo, Panda3D utiliza winsock mientras OpenCV usa ws2def, ambas incompatibles entre sí. Para resolver estos problemas se añaden al inicio de la cabecera de la aplicación las siguientes líneas:
\begin{lstlisting}[language=C++]
#ifdef _WINDOWS
#include <winsock2.h>
#endif
\end{lstlisting}
Estos problemas están ligados a las librerías del sistema operativo que utilizan las librerías externas, y la inclusión sólo es necesaria para el sistema operativo Windows. Se recomienda al desarrollador que desee añadir librerías o reimplementar este módulo que tenga este tipo de incompatibilidades en cuenta.
\\
\subsection{CLR}
Otra de las incompatibilidades más comunes fue la necesidad de desactivar el CLR, Common language runtime support. Debe ser puesto a  No Common language runtime support.

\subsection{NDEBUG}
Panda3D presenta graves problemas si se utiliza la macro \_NDEBUG en Release, por lo que debe quitarse de todo el proyecto.

\section{Third Parties}\label{detallesImp:ThirdParties}


\subsection{wxWidgets}\label{detallesImp:wxWidgets}
Las instrucciones para descargar y compilar wxWidgets pueden encontrarse en la web oficial \cite{wx09}. La distribución contiene un proyecto preparado para compilarla con Visual Studio 2008. Sólo es necesario descargar la distribución, abrir la solución y compilarlo en Debug y Release para obtener las librerías, sin necesidad de ningún ajuste en la compilación. Para este proyecto se ha usado la versión 2.9.0.

\subsection{Boost}\label{detallesImp:Boost}

La integración de Boost en el proyecto dio algunos problemas. Se obtuvo la distribución a partir del svn y se siguieron los pasos descritos en la documentación. Sin embargo, para poder añadir correctamente la librería a la solución son necesarias algunas opciones extras al compilarla. Estas opciones son la especificación de multi-threading y el uso de librerías estáticas. La versión utilizada fue la 1.41. El comando final para compilar la librería se muestra a continuación:

\begin{listing}[style=consola, numbers=none]
>.\bjam link=static threading=multi threading=single variant=release variant=debug runtime-link=static
\end{listing}

Hay que tener en cuenta que es necesario desactivar el CLR para evitar incompatibilidades.

\subsection{SFML}\label{detallesImp:SFML}

Puede obtenerse desde svn o descargando algunos de los paquetes que existen, con proyectos preparados para distintos IDEs. La compilación es sencilla y directa. Sin embargo, es importante destacar que la documentación acerca del paquete de sonido es confusa y en ocasiones errónea. Sin embargo, es útil para conocer las herramientas que dispone. La versión utilizada fue la 1.6. Se describen los detalles a tener en cuenta, especialmente en relación a la localización del espacio 3D. En concreto:

\begin{itemize}
\item El eje X es creciente hacia la derecha.
\item El eje Y es creciente hacia arriba.
\item El eje Z es creciente y apunta hacia afuera de la pantalla, hacia el usuario.
\item Método SetTarget(): Para orientar el oyente, se usa un vector normalizado relativo al oyente, no al objeto al que escucha, como puede malinterpretarse en la documentación.
\item Si usáramos la librería tal como se descarga, no tendremos una correcta espacialización del oyente en un entorno 3D. Esto es debido a que enmascara la creación del oyente de OpenAL, ignorando una componente crucial: el vector de verticalidad. SFML Establece este vector siempre como vertical. Como puede apreciarse en la figura \ref{F_SFML_listener_orientation}, y donde 'to' representa el vector de vecticalidad' y 'at' el vector de dirección, esto impide la correcta orientación del oyente e impide que se pueda distinguir la orientación derecha-izquierda de la 'cabeza'. Sin embargo, sí es válido para posicionamiento 2D o 3D donde el vector de dirección del usuario siempre se mueva en el plano horizontal y la verticalidad del oyente no cambie.
\end{itemize}

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/SFML_listener_orientation.png}
\end{center}
\caption{ \label{F_SFML_listener_orientation} Necesidad del vector de verticalidad.}
\end{figure}

Esto puede verse claramente en la Definición del método de la clase Listener:

\begin{lstlisting}[language=C++]
/// Change the orientation of the listener (the point he must look at) (take 3 values)
void Listener::SetTarget(float X, float Y, float Z)
{   float Orientation[] = {X, Y, Z, 0.f, 1.f, 0.f};
    ALCheck(alListenerfv(AL_ORIENTATION, Orientation)); }
\end{lstlisting}
Sin embargo, el arreglo es sencillo. Basta con añadir a la clase Listener la siguiente función:
\begin{lstlisting}[language=C++]
void Listener::SetTarget(float X, float Y, float Z, float u, float v, float p)
{   float Orientation[] = {X, Y, Z, u, v, p};
    ALCheck(alListenerfv(AL_ORIENTATION, Orientation)); }
\end{lstlisting}

Tras recompilar se obtiene una librería que, ahora sí, es capaz de posicionar correctamente sonido y oyente en 3D.

\subsection{OpenCV, Open Source Computer Vision}\label{detallesImp:OpenCV}

Para este proyecto se ha usado la versión 2.1.0. Las instrucciones para descargar y compilar OpenCV pueden encontrarse en la web oficial \cite{OPENCV}. OpenCV no distribuye versiones precompiladas para Visual Studio. Para desarrollar en este entorno fue necesario hacer uso de CMake para generar un proyecto que pudiera ser compilado, siguiendo las instrucciones descritas en la guía oficial \cite{OPENCVCMAKE}. 
\\

Se cita el resumen del procedimiento:

\begin{lstlisting}[language=C++]
Here is the procedure at glance:
--------------------------------
1. Install CMake from www.cmake.org/cmake/resources/software.html

2. Run CMake GUI tool and configure OpenCV there:
   2.1. select C:/OpenCV2.0 (or the installation directory you chose)
        as the source directory;
   2.2. choose some other directory name for the generated project 
        files, e.g. C:/OpenCV2
   2.3. press Configure button, select your build environment
   2.4. adjust any options at your choice
   2.5. press Configure again, then press Generate.
   
3a. In the case of Visual Studio or any other IDE, open the generated
    solution/workspace/project, e.g. C:/OpenCV2.0/vs2008/OpenCV.sln,
    build it in Release and Debug configurations.
3b. In the case of command-line Makefiles, enter the destination 
    directory and type "make" (or "nmake" etc.)   
       
4. Add the output directories to the system path, e.g.:
   C:/OpenCV2.0/vs2008/bin/Debug;C:/OpenCV2.0/vs2008/bin/Release:
   \%PATH\% It is safe to add both directories, since the Debug
   OpenCV DLLs have the "d" suffix, which the Release DLLs do not have.
      
5. Optionally, add C:/OpenCV2.0/include/opencv to the list of
   include directories in your IDE settings,
   and the output library directories
   (e.g. C:/OpenCV2.0/vs2008/lib/(Debug,Release))
   to the list of library paths.

It is important to build both release and debug configurations, and link
you code with the proper libraries in each configuration,
otherwise various compile-time or run-time errors are possible.
\end{lstlisting}

Hay que tener en cuenta que Visual Studio 2008 no tiene soporte para OPENMP, seleccionado por defecto, así que es necesario desactivarlo en CMake. También se desactiva la opción PYTHON SUPPORT.


%Dependencias cv200d.lib cxcore200d.lib highgui200d.lib
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Panda3D}\label{detallesImp:Panda3D}

Para el desarrollo de este proyecto se comenzó utilizando la versión 1.6, en la que ya se habían detectado problemas de seguridad en la ejecución de hilos en algunos componentes del motor. Debido a ello, durante la última etapa del desarrollo se actualiza con la versión estable 1.8.1, que mejora algunos de estos problemas. Es importante recordar que, Panda3D usa winsock mientras que OpenCV usa ws2def, que son incompatibles y entran en conflicto durante la etapa de vinculación. Para resolverlo, se añade winsock2.h al comienzo de Application.h, antes de que ninguna otra librería incluya ws2def.
\\

Durante el uso de la versión 1.6, fue necesario modificar los propios fuentes de Panda3D para resolver conflictos de vínculo durante la compilación del proyecto. Para resolverlo bastaba con exportar los símbolos Geom::CDataCache y GeomVertexData::CDataCache. Este cambio fue subido y aceptado en revisión, por lo que si es usada una versión posterior a la 1.6, no será necesario aplicarlo:

\begin{lstlisting}[language=C++]
File: src/gobj/geom.h
---------------------------------
class EXPCL\_PANDA\_GOBJ CDataCache : public CycleData {
public:
 INLINE CDataCache();
 INLINE CDataCache(const CDataCache \&copy);
 virtual ~CDataCache();
 ALLOC\_DELETED\_CHAIN(CDataCache);
 virtual CycleData *make\_copy() const;
 virtual TypeHandle get\_parent\_type() const {
 return Geom::get\_class\_type();
}

File: src/gobj/geomvertexdata
---------------------------------
class EXPCL\_PANDA\_GOBJ CDataCache : public CycleData {
public:
 INLINE CDataCache();
 INLINE CDataCache(const CDataCache \&copy);
 ALLOC\_DELETED\_CHAIN(CDataCache);
 virtual CycleData *make\_copy() const;
 virtual TypeHandle get\_parent\_type() const {
 return GeomVertexData::get\_class\_type();
}
\end{lstlisting}
 
Para utilizar esta librería basta con seguir las instrucciones descritas en la guía oficial. En ambas se describe cómo descargar y utilizar un paquete precompilado \cite{PANDA3DINSTALL}, y también cómo compilarlo por nuestra cuenta \cite{PANDA3DCOMPILE}. Para evitar problemas y complicaciones, se recomienda utilizar la versión 1.8.1 o posterior, y si (si resultaran compatibles) el pack precompilado. 
\\

Se debe mencionar que el desarrollo de Panda3D en la actualidad es bastante deficiente. Algunas funcionalidades avanzadas dan problemas si son ejecutadas en modo Debug, sólo funcionando en modo Release. Esto ha llevado al punto en el que, en la situación final del desarrollo, no se usa y no se recomienda usar el proyecto en modo Debug.


\subsection{PostgreSQL}\label{detallesImp:PostgreSQL}
Se describen los pasos necesarios para preparar la Base de Datos PostgreSQL.

\begin{itemize}
\item Descargar \cite{POSTGRESQL} e instalar la versión de 32bits para Windows. 
\item Se recomienda usar un directorio de instalación al mismo nivel que el proyecto, dentro de una carpeta llamada BBDD, dado que el proyecto usará ésta ruta relativa por defecto. En cualquier caso, es posible crear nuestra propia variable de entorno o añadir al PATH del sistema la ruta relativa al directorio de instalación que deseemos. 
\item Durante la instalación, establecer el puerto por defecto a 5432. 
\item Una vez instalada, ejecutar la aplicación pgAdmin y crear la base de datos con esta configuración:
\begin{itemize}
\item Nombre: voxDB
\item Codificacion: UTF8
\item Tablespace: pg\_default
\item Colación: spanish\_spain.1252. 
\end{itemize}
\item Finalmente, creamos un nuevo rol de login con nombre 'vox', contraseña 'vox' y lo asignamos como usuario de la base de datos.
\end{itemize} 

\subsection{Debea}\label{detallesImp:Debea}

Se comentan los pasos necesarios para compilar y preparar el entorno. Para este proyecto se ha usado la versión 1.4:

\begin{itemize}
\item Descargar la librería y seguir las instrucciones que se comentan en la guía\cite{DEBEAINSTALL}. En este caso el comando que hemos usado para compilar ha sido el siguiente:

\begin{lstlisting}[language=C++]
nmake -f makefile.vc DEBUG=0 DEBUG=1 PGSQL=1 install
\end{lstlisting}
\item Añadir al PATH del sistema los directorio extern, debea, vc\_release\_static
\item Añadir la variable de entorno POSTGRESQL apuntando al directorio de instalación de Postgre, donde están los subdirectorios 'lib' y 'bin'.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Interfaces core}\label{detallesImp:InterfacesCore}
Para preparar el módulo ha sido necesario configurar el proyecto igui de la siguiente forma:
\begin{itemize}
\item Additional Include Directories:
\begin{lstlisting}[language=C++]
.
..\..\src
..\..\extern\include
\end{lstlisting}
\item Additional Library Directories:
\begin{lstlisting}[language=C++]
\"\$(OutDir)\"
\end{lstlisting}
\item Preprocessor Definitions (Debug): WIN32; \_DEBUG; \_WINDOWS; 
\item Preprocessor Definitions (Release): WIN32; \_WINDOWS;
\end{itemize}

Además por asuntos de compatibilidad es necesario asegurar las siguientes opciones:
\begin{itemize}
\item No usar la macro \_NDEBUG en release.
\item Configuration Type: Static Library (.lib). (actualemnte dll cambiar)
\item Use of ATL: Not Using ATL.
\item Common Language Runtime Support: No Common Language Runtime Support.
\item Runtime Library: Multi-threaded [Debug] DLL ($\backslash$MTD $\backslash$MTDd).
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Módulo de GUI}\label{detallesImp:ModuloGUI}
Para preparar el módulo ha sido necesario configurar el proyecto igui de la siguiente forma:
\begin{itemize}
\item Additional Include Directories:
\begin{lstlisting}[language=C++]
.
..\..\src
..\..\extern\include
..\..\extern\include\wxwidgets
..\..\extern\include\wxwidgets\msvc
..\..\extern\boost
\end{lstlisting}
\item Additional Library Directories:
\begin{lstlisting}[language=C++]
..\..\bin
..\..\extern\lib
..\..\extern\boost\lib
\"\$(OutDir)\"
\end{lstlisting}
\item Preprocessor Definitions (Debug): WIN32; \_DEBUG; \_WINDOWS; \_USRDLL; \_MSVC; \_IGUIEXPORT\_; \_\_WXMSW\_\_; \_\_WXDEBUG\_\_; \_UNICODE; NOPCH
\item Preprocessor Definitions (Release): WIN32; \_WINDOWS; \_USRDLL; \_MSVC;\_IGUIEXPORT\_; \_\_WXMSW\_\_; \_UNICODE; NOPCH
\item Additional Dependencies (Debug): 
wxmsw29ud\_core.lib wxbase29ud.lib wxtiffd.lib wxjpegd.lib wxpngd.lib wxzlibd.lib wxregexud.lib wxexpatd.lib kernel32.lib user32.lib gdi32.lib comdlg32.lib
winspool.lib winmm.lib shell32.lib comctl32.lib ole32.lib oleaut32.lib uuid.lib rpcrt4.lib advapi32.lib wsock32.lib wininet.lib 
\item Additional Dependencies (Release): wxmsw29u\_core.lib wxbase29u.lib wxtiff.lib wxjpeg.lib wxpng.lib wxzlib.lib wxregexu.lib wxexpat.lib kernel32.lib user32.lib gdi32.lib comdlg32.lib winspool.lib winmm.lib shell32.lib comctl32.lib ole32.lib oleaut32.lib rpcrt4.lib advapi32.lib wsock32.lib wininet.lib
\end{itemize}

Además por asuntos de compatibilidad es necesario asegurar las siguientes opciones:
\begin{itemize}
\item No usar la macro \_NDEBUG en release.
\item Configuration Type: Static Library (.lib).
\item Use of ATL: Not Using ATL.
\item Common Language Runtime Support: No Common Language Runtime Support.
\item Runtime Library: Multi-threaded [Debug] DLL ($\backslash$MTD $\backslash$MTDd).
\end{itemize}

 
\section{Módulo de Percepción}\label{detallesImp:ModuloPercepcion}

Para implementar el módulo IPercept se ha configurado el proyecto de la siguiente forma:

\begin{itemize}
\item Additional Include Directories:
\begin{lstlisting}[language=C++]
.
..\..\src
..\..\extern\include
..\..\extern\include\opencv
..\..\extern\boost
..\..\extern\include\wxwidgets\msvc
..\..\extern\include\wxwidgets
..\..\extern\sfml\include
\end{lstlisting}
\item Additional Library Directories:
\begin{lstlisting}[language=C++]
..\..\bin
..\..\extern\lib
..\..\extern\lib\opencv
..\..\extern\boost\lib
..\..\extern\sfml\lib\vc2008
\"\$(OutDir)\"
\end{lstlisting}
\item Preprocessor Definitions (Debug): WIN32;\_DEBUG
\item Preprocessor Definitions (Release): WIN32;\_WINDOWS;\_MSVC;
\item Additional Dependencies (Debug): cv200d.lib cxcore200d.lib highgui200d.lib sfml-audio-s-d.lib sfml-system-s-d.lib
\item Additional Dependencies (Release): cv200.lib cxcore200.lib highgui200.lib sfml-audio-s.lib sfml-system-s.lib
\end{itemize}

Además por asuntos de compatibilidad es necesario asegurar las siguientes opciones:
\begin{itemize}
\item No usar la macro \_NDEBUG en release.
\item Configuration Type: Static Library (.lib).
\item Use of ATL: Not Using ATL.
\item Common Language Runtime Support: No Common Language Runtime Support.
\item Runtime Library: Multi-threaded [Debug] DLL ($\backslash$MTD $\backslash$MTDd).
\end{itemize}

\section{Módulo de Producción}\label{detallesImp:ModuloProduccion}

Para implementar el módulo IProd se ha configurado el proyecto de la siguiente forma:

\begin{itemize}
\item Additional Include Directories:
\begin{lstlisting}[language=C++]
.
..\..\src
..\..\extern\include
..\..\extern\panda3d\built\include
..\..\extern\panda3d\built\python\include
..\..\extern\boost
..\..\extern\include\wxwidgets\msvc
..\..\extern\include\wxwidgets
..\..\extern\sfml\include
\end{lstlisting}
\item Additional Library Directories (Debug):
\begin{lstlisting}[language=C++]
..\..\bin
..\..\extern\lib
..\..\extern\panda3d\debug\lib
..\..\extern\panda3d\debug\python
..\..\extern\panda3d\debug\python\Lib
..\..\extern\panda3d\debug\python\libs
..\..\extern\boost\lib
..\..\extern\sfml\lib\vc2008
\$(OutDir)
\end{lstlisting}
\item Additional Library Directories (Release):
\begin{lstlisting}[language=C++]
..\..\bin
..\..\extern\lib
..\..\extern\panda3d\built\lib
..\..\extern\panda3d\built\python
..\..\extern\panda3d\built\python\Lib
..\..\extern\panda3d\built\python\libs
..\..\extern\boost\lib
..\..\extern\sfml\lib\vc2008
\$(OutDir)
\end{lstlisting}
\item Preprocessor Definitions (Debug): WIN32;\_DEBUG
\item Preprocessor Definitions (Release): WIN32;
\item Additional Dependencies (Debug): libpandaexpress.lib libp3framework.lib libpanda.lib libpandafx.lib libp3dtool.lib libp3dtoolconfig.lib libp3pystub.lib libp3direct.lib sfml-audio-s-d.lib sfml-system-s-d.lib
\item Additional Dependencies (Release): libpandaexpress.lib libp3framework.lib libpanda.lib libpandafx.lib libp3dtool.lib libp3dtoolconfig.lib libp3pystub.lib libp3direct.lib sfml-audio-s.lib sfml-system-s.lib
\end{itemize}

Además por asuntos de compatibilidad es necesario asegurar las siguientes opciones:
\begin{itemize}
\item No usar la macro \_NDEBUG en release.
\item Configuration Type: Static Library (.lib).
\item Use of ATL: Not Using ATL.
\item Common Language Runtime Support: No Common Language Runtime Support.
\item Runtime Library: Multi-threaded [Debug] DLL ($\backslash$MTD $\backslash$MTDd).
\end{itemize}


\section{Módulo de Persistencia}\label{detallesImp:ModuloPersistencia}

Para implementar el módulo IPersistence se ha configurado el proyecto de la siguiente forma:

\begin{itemize}
\item Additional Include Directories:
\begin{lstlisting}[language=C++]
.
..\..\src
..\..\extern\include
..\..\extern\debea\include
..\..\extern\boost
\end{lstlisting}
\item Additional Library Directories:
\begin{lstlisting}[language=C++]
..\..\bin
..\..\extern\lib
..\..\extern\debea\vc_release_static
\end{lstlisting}
\item Preprocessor Definitions (Debug): WIN32;\_DEBUG
\item Preprocessor Definitions (Release): WIN32;\_WINDOWS;\_MSVC;
\item Additional Dependencies (Debug): dbad.lib dbapgsql-static.lib
\item Additional Dependencies (Release): dbad.lib dbapgsql-static.lib
\end{itemize}

Además por asuntos de compatibilidad es necesario asegurar las siguientes opciones:
\begin{itemize}
\item No usar la macro \_NDEBUG en release.
\item Configuration Type: Static Library (.lib).
\item Use of ATL: Not Using ATL.
\item Common Language Runtime Support: No Common Language Runtime Support.
\item Runtime Library: Multi-threaded [Debug] DLL ($\backslash$MTD $\backslash$MTDd).
\end{itemize}

\section{Módulo de Inteligencia Artificial}\label{detallesImp:ModuloIA}

Para implementar el módulo IPercept se ha configurado el proyecto de la siguiente forma:

\begin{itemize}
\item Additional Include Directories:
\begin{lstlisting}[language=C++]
.
..\..\src
..\..\extern\include
..\..\extern\boost
..\..\extern\utils
\end{lstlisting}
\item Additional Library Directories:
\begin{lstlisting}[language=C++]
..\..\bin
..\..\extern\lib
\"\$(OutDir)\"
\end{lstlisting}
\item Preprocessor Definitions (Debug): WIN32;\_DEBUG
\item Preprocessor Definitions (Release): WIN32;\_WINDOWS;\_MSVC;
\end{itemize}

Además por asuntos de compatibilidad es necesario asegurar las siguientes opciones:

\begin{itemize}
\item No usar la macro \_NDEBUG en release.
\item Configuration Type: Static Library (.lib).
\item Use of ATL: Not Using ATL.
\item Common Language Runtime Support: No Common Language Runtime Support.
\item Runtime Library: Multi-threaded [Debug] DLL ($\backslash$MTD $\backslash$MTDd).
\end{itemize}

\section{Aplicación Principal}\label{detallesImp:Application}

El proyecto que conforma la aplicación principal se ha configurado de la siguiente forma:

\begin{itemize}
\item Additional Include Directories:
\begin{lstlisting}[language=C++]
.
..\..\src
..\..\extern\include
..\..\extern\panda3d\built\include
..\..\extern\panda3d\built\python\include
..\..\extern\include\opencv
..\..\extern\include\wxwidgets
..\..\extern\include\wxwidgets\msvc
..\..\extern\boost
..\..\extern\sfml\include
\end{lstlisting}
\item Additional Library Directories (Debug):
\begin{lstlisting}[language=C++]
..\..\bin
..\..\extern\lib
..\..\extern\bin
..\..\extern\panda3d\debug\python\libs
..\..\extern\boost\lib
\$(OutDir)
\end{lstlisting}
\item Additional Library Directories (Release):
\begin{lstlisting}[language=C++]
..\..\bin
..\..\extern\lib
..\..\extern\bin
..\..\extern\panda3d\built\python\libs
..\..\extern\boost\lib
\$(OutDir)
\end{lstlisting}
\item Preprocessor Definitions (Debug): WIN32;\_DEBUG;\_WINDOWS;\_MSVC;
\item Preprocessor Definitions (Release): WIN32;\_WINDOWS;\_MSVC;
\item Additional Dependencies (Debug): icogd.lib iguid.lib iperceptd.lib ipersistenced.lib iprodd.lib
\item Additional Dependencies (Release): icog.lib igui.lib ipercept.lib ipersistence.lib iprod.lib
\item Environment (Debug): 
\begin{listing}[style=consola, numbers=none]
PATH=%PATH%;..\..\extern\bin;..\..\extern\bin\opencv;..\..\extern\panda3d\debug\bin;..\..\extern\panda3d\debug\python;..\..\extern\panda3d\debug\python\DLLs;..\..\..\BBDD\PostgreSQL\bin;..\..\extern\debea\vc_release_static;..\..\extern\sfml\extlibs\bin;
\end{listing}
\item Environment (Release): 
\begin{listing}[style=consola, numbers=none]
PATH=..\..\extern\opencv\bin;%PATH%;..\..\extern\bin;..\..\extern\bin\opencv;..\..\extern\panda3d\lib;..\..\extern\panda3d\built\bin;..\..\extern\panda3d\built\python;..\..\extern\panda3d\built\python\libs;..\..\..\BBDD\PostgreSQL\bin;..\..\extern\debea\vc_release_static;..\..\extern\sfml\extlibs\bin;
\end{listing}
\end{itemize}

Además por asuntos de compatibilidad es necesario asegurar las siguientes opciones:
\begin{itemize}
\item No usar la macro \_NDEBUG en release.
\item Configuration Type: Application (.exe).
\item Use of ATL: Not Using ATL.
\item Common Language Runtime Support: No Common Language Runtime Support.
\item Runtime Library: Multi-threaded [Debug] DLL ($\backslash$MD $\backslash$MDd).
\end{itemize}

\section{Diagrama Completo}
%En la Figura \ref{F_DiagramaCompleto} se muestra a modo de ilustración el conjunto de todos los bloques del proyecto. 
Se muestra a modo de ilustración el conjunto de todos los bloques del proyecto. 

%\begin{figure} [H]
%\begin{center}
%\includegraphics[width=\textwidth]{recursos_memoria/blocks_full.jpg}\label{F_DiagramaCompleto}
%\end{center}
%\caption{ \label{F_DiagramaCompleto} Diagrama Completo.}
%\end{figure}

\begin{figure} [H]
\begin{center}
\includegraphics[width=\textwidth]{recursos_memoria/blocks_full2.jpg}\label{F_DiagramaCompleto2}
\end{center}
%\caption{ \label{F_DiagramaCompleto} Diagrama Completo.}
\end{figure}

% Aqui va la Bibliografía utilizada por el proyecto.

\begin{thebibliography}{1}

\bibitem{La86} Leslie Lamport {\em LaTex : A document Preparation System}. Addison"=Wesley, 1986.

\bibitem{Ro93} Christian Rolland {\em LaTex guide pratique}. Addison"=Wesley, 1993.

\bibitem{Castrillon05} M. Castrillón, O. Déniz, C. Guerra, M. Hernández {\em ENCARA2: Real-time Detection of Multiple Faces at Different Resolutions in Video Streams } Journal of Visual Communication and Image Representation, ISSN 1047-3203, vol 18, issue 2, pp. 130-140, April 2007.

\bibitem{CastrillonSchmidt} M. Castrillón Santana, Joachim Schmidt {\em AUTOMATIC INITIALIZATION FOR BODY TRACKING: Using Appearance to Learn a Model for Tracking Human Upper Body Motions}


\bibitem{Chellappa95} R. Chellappa et al. {\em Human and machine recognition of faces: A survey.} Proceedings IEEE, vol. 83(5), 705~-740, 1995.

\bibitem{Cielniak03} G. Cielniak, M. Miladinovic, D. Hammarin, L. Göransson, A. Lilienthal and T. Duckett {\em Appearance-based Tracking of Persons with an Omnidirectional Vision Sensor} Proceedings of the Fourth IEEE Workshop on Omnidirectional Vision (Omnivis 2003)", Madison, Wisconsin", 2003

\bibitem{Deniz04} O. Déniz, A. Falcón, J. Méndez, M. Castrillón {\em Useful Computer Vision Techniques for Human-Robot Interaction.} International Conference on Image Analysis and Recognition, September 2004, Porto, Portugal.

\bibitem{Hjelmas01a} E. Hjelmas y B. K. Low {\em Face Detection: A Survey.} Computer Vision and Image Understanding, vol. 83(3), 2001.

\bibitem{Iges99} José Iges {\em El espacio. El tiempo en la mirada del sonido.} Catálogo de exposición. Kulturanea. España, 1999.

\bibitem{Krueger85} Myron W. Krueger, Thomas Gionfriddo y Katrin Hinrichsen {\em VIDEOPLACE: An Artificial Reality} Proceedings of the SIGCHI conference on Human factors in computing systems, 35~-40, 1985.

\bibitem{Levin04} Golan Levin y Zachary Lieberman {\em In-Situ Speech Visualization in Real-Time Interactive Installation and Performance.} The 3rd International Symposium on Non-Photorealistic Animation and Rendering (NPAR) June 7~-9 2004, Annecy, France

\bibitem{Samal92} A. Samal and P. A. Iyengar {\em Automatic Recognition and Analysis of Human Faces and Facial Expressions: A Survey.} Pattern Recognition, vol. 25(1), 1992.

\bibitem{Spalter99} Anne Morgan Spalter {\em The Computer in The Visual Arts.} Addison-Wesley Professional. 1st edition, 1999.

\bibitem{Viola01cvpr} P. Viola and M. J. Jones {\em Rapid Object Detection using a Boosted Cascade of Simple Features.} In Computer Vision and Pattern Recognition, 2001a.

\bibitem{LIENHART} Rainer Lienhart {\em Empirical Analysis of Detection Cascades of Boosted Classifiers for Rapid Object Detection} MRL Technical Report, May 2002, revised December 2002

\bibitem{KBERGGREN} Karl Berggren {\em Camera focus controlled by face detection on GPU} 2008.

\bibitem{AHARVEY} Adam Harvey {\em Adam Harvey Explains Viola-Jones Face Detection} http://www.makematics.com/research/viola-jones/

\bibitem{Yang02} M. H. Yang et al. {\em Detecting Faces in Images: A Survey.} Transactions on Pattern Analysis and Machine Intelligence, vol. 24(1), 34~-58, 2002.

\bibitem{OpenCV09} Willow Garage: S. Hassan, S. Cousins, B. Gerkey et al. {\em OpenCV, Open Source Computer Vision} Official Page and Documentation: http://opencv.willowgarage.com/documentation/index.html 2009.

\bibitem{TURKPENTLAND} Matthew Turk, Alex Pentland {\em Eigenfaces for Recognition} Vision and Modelling Group, The Media Laboratory, MIT

\bibitem{DReynolds} Douglas Reynolds {\em Gaussian Mixture Models} MIT Lincoln Laboratory, 244 Wood St., Lexington, MA 02140, USA

\bibitem{OLIVERGRAU} Oliver Grau {\em Virtual Art: From Illusion to Immerison} 2003

\bibitem{WILSON02} Stephen Wilson {\em Information Arts: intersections of art, science, and technology} 2002 

\bibitem{TYPEMOONDA} TypeMOON, Domus Aurea {\em Interpretación de la sala principal del Domus Aurea por TYPEMOON, compañía de videojuegos y novelas gráficas} http://typemoon.wikia.com/wiki/Aestus\_Domus\_Aurea?file=Aestus.jpg

\bibitem{BARKERXIX} Robert Barker, Panorama Rotunda {\em A Section of the Rotunda in Leicester Square, in which is exhibited The Panorama. Aquatint from Robert Mitchell Plans and Views in Perspective, with Descriptions, of Buildings Erected in England and Scotland (London: Wilson and Co., 1801).} The British Library.

\bibitem{IMAXDOMET} Domo IMAX en el centro cultura de Tijuana, fotografía de El Randy http://en.wikipedia.org/wiki/File:Cecut.jpg

\bibitem{IMAXDOMEHSW} Sección esquemática de un Domo IMAX por HowStuffWorks http://entertainment.howstuffworks.com/imax1.htm


\bibitem{CASTROM09} Federico Castro Morales, CAAM - Centro Atlántico de Arte Moderno {\em Iniciación al Arte Contemporáneo a través de la Colección del CAAM} 2009

\bibitem{VASARELY} Víctor Vasarely http://www.vasarely.com/site/site.htm

\bibitem{MDUCHAMP} Marcel Duchamp, {\em Marcel Duchamp World Community} http://www.marcelduchamp.net/

\bibitem{ACALDER} Alexander Calder {\em Calder Foundation} http://www.calder.org/

\bibitem{CMANRIQUE} César Manrique {\em Fundación César Manrique} http://www.fcmanrique.org/

\bibitem{NWIENER} Norvert Wiener at the Wikipedia http://en.wikiquote.org/wiki/Norbert\_Wiener

\bibitem{LAPOSKY} Ben F. Laposky, Artículo sobre Ben Laposky en Proyecto IDIS http://proyectoidis.org/ben-laposky/

\bibitem{JCAGEMKN} John Cage, Fontana Mix y más referencias en Medien Kusnt Net http://www.medienkunstnetz.de/works/fontana-mix/audio/1/ http://www.medienkunstnetz.de/search/?qt=john+cage

\bibitem{DEMOSCENE} Demoscene https://www.scene.org/

\bibitem{CDAKQO} cdak - Quite and Orange http://www.youtube.com/watch?v=cjSJc2eCetE

\bibitem{EBANDROMEDA} Electric Bullet - a n d r o m e d a http://www.youtube.com/watch?v=g3TRZamP85c

\bibitem{SOFTCRACKING} Software cracking at the Wikipedia http://en.wikipedia.org/wiki/Software\_cracking 

\bibitem{CHAOSTC} Chaos Theory - Conspiracy http://www.youtube.com/watch?v=ZfuierUvx1A 2009

\bibitem{OKRISTENSEN} Ole Kristensen  http://ole.kristensen.name/

\bibitem{FLONG} Golan Levin and Colaborators http://www.flong.com/

\bibitem{FWEISS} Frieder Weiss http://www.frieder-weiss.de/
 
\bibitem{TMIZUGICHI} Tetsuya Mizugichi http://www.mzgc.net/?lang=en

\bibitem{MAKTEN} Memo Akten http://www.memo.tv/

\bibitem{MANIFESTOA} Manifiesto por el Desarrollo Ágil de Software http://agilemanifesto.org/iso/es/

\bibitem{SCRUMMANAGER} Certificación Scrum Manager http://www.scrummanager.net/certificacion

\bibitem{TEXMAKER} TexMaker, The Universal LaTex Editor http://www.xm1math.net/texmaker/

\bibitem{ESSWORK} EssWorks, Working with Software Practices http://www.ivarjacobson.com/EssWork/

\bibitem{DIA} Dia, GTK+ based diagram creation program https://wiki.gnome.org/Apps/Dia

\bibitem{WEBPP} WebProjectPlan, Planificación del PFC utilizando la aplicación WebProjectPlan http://www2.dis.ulpgc.es/~lalvarez/teaching/pi/Informacion\_Uso\_WebProjectPlan.html

\bibitem{MOOVIA} MOOVIA Social Collaboration https://site.moovia.com/

\bibitem{VS2008} Microsoft Visual Studio 2008 Express Edition http://msdn.microsoft.com/es-es/express/aa975050.aspx

\bibitem{BLENDER} Blender, A a Free and Open Source 3D Animation Suite http://www.blender.org/

\bibitem{GIMP} GIMP, The GNU Image Manipulation Software http://www.gimp.org/

\bibitem{AUDACITY} Audacity, Editor y grabador de Audio libre http://audacity.sourceforge.net/

\bibitem{FREESOUND} FreeSound, Collaborative database of Creative Commons Licensed sounds http://www.freesound.org/

\bibitem{POSTGRESQL} PostgreSQL, The world's most advanced open source database http://www.postgresql.org/

\bibitem{Chuck09} G. Wang, P. Cook et al. {\em Chuck, Strongly-timed, Concurrent, and On-the-fly Audio Programming Language} http://chuck.cs.princeton.edu/ 2009.

\bibitem{DME09} Devmasters.net {\em 3D Engines Data Base} http://www.devmaster.net/engines/ 2009

\bibitem{UDK09} Epic Games {\em Unreal Development Kit} http://www.udk.com/ 2009

\bibitem{UDKS09} Epic Games {\em Unreal Script Language Reference, Example Program Structure}  http://udn.epicgames.com/Three/UnrealScriptReference.html/ 2009

\bibitem{UDKFL09} Epic Games {\em Unreal Engine 3: Rendering Feature List}  http://www.unrealtechnology.com/features.php?ref=rendering 2009

\bibitem{CryE309} CryTek {\em CryENGINE 3 Educational SDK License} http://mycryengine.com/index.php?conid=42 2009

\bibitem{CryE3FL09} CryTek {\em CryENGINE 3 - Specifications}  http://www.crytek.com/technology/cryengine-3/specifications/ 2009

\bibitem{CryMod09} Crytek {\em CryTek Official Modding Portal} Educational community area for the CryENGINE 3 Software Development Kit: http://www.crymod.com/ 2009

\bibitem{wx09} wxWidgets {\em wxWidgets, Cross~platform GUI library} http://www.wxwidgets.org/ 2009

\bibitem{wxFL09} wxWidgets {\em wxWidgets Features} http://www.wxwidgets.org/about/feature2.htm 2009

\bibitem{UDK} Unreal 3 DEvelopment Kit http://www.unrealengine.com/en/udk/

\bibitem{CRYENGINEE} CRYENGINE 3 Educational SDK http://www.crytek.com/news/free-educational-license-for-cryengine--3

\bibitem{CRYSTALSPACE} Crystal Space http://www.crystalspace3d.org/main/Main\_Page

\bibitem{OGRE} OGRE, Object-Oriented Graphics Rendering Engine http://www.ogre3d.org/

\bibitem{OSG} OSG, Open Scene Graph http://www.openscenegraph.org/

\bibitem{IRRLICHT} Irrlicht, Lightning Fast Realtime 3D Engine http://irrlicht.sourceforge.net/

\bibitem{IRRLICHTLICENSE} Irrlicht license http://irrlicht.sourceforge.net/license/

\bibitem{PANDA3D} Panda3D, Platform Agnostic Networked Display Architecture http://www.panda3d.org/

\bibitem{PANDA3DLICENSE} Panda3D license http://www.panda3d.org/license.php

\bibitem{PANDA3DINSTALL} Panda3D Manual, Installing Panda3D in Windows \\
http://www.panda3d.org/manual/index.php/Installing\_Panda3D\_in\_Windows

\bibitem{PANDA3DCOMPILE} Panda3D Manual, Tutorial: Compiling the Panda3D Source on Windows www.panda3d.org/wiki/index.php/Tutorial:\_Compiling\_the\_Panda\_3D\_Source\_on\_Windows

\bibitem{DOXYGEN} Doxygen, generate documentation from source code http://www.stack.nl/~dimitri/doxygen/

\bibitem{BOOST} Boost, C++ libraries http://www.boost.org/

\bibitem{QT} Qt, Cross-platform application and UI development framework http://qt.digia.com/

\bibitem{WXWIDGETS} wxWidgets, A cross-platform GUI and tools library for GTK, MS Windows, and MacOS http://www.wxwidgets.org/

\bibitem{OPENCV} OpenCV, Open Source Computer Vision http://opencv.org/

\bibitem{OPENCVCMAKE} OpenCV, Installation Guide http://opencv.willowgarage.com/wiki/InstallGuide.

\bibitem{ENCARA2} Encara2, Real-time face detection http://new-mozart.dis.ulpgc.es/encara2

\bibitem{FMOD} Fmod, Music and Sound Effects System http://www.fmod.org/

\bibitem{OPENAL} OpenAL, Cross-platform 3D Audio API http://www.openal.org/

\bibitem{SDLMIXER} SDL Mixer, Simple Direct Media Layer Mixer http://www.libsdl.org/projects/SDL\_mixer/

\bibitem{CLUNK} Clunk, Open Source 3D-Sound library http://www.ohloh.net/p/clunk

\bibitem{IRRKLANG} Irrklang, High level 3D audio engine http://www.ambiera.com/irrklang/

\bibitem{SFML} SFML, Simple and Fast Multimedia Library http://www.sfml-dev.org/

\bibitem{SFMLLICENSE} SFML License http://www.sfml-dev.org/license.php

\bibitem{CHUCK} Chuck, Strongly-timed Concurrent and On-the.fly Audio Programming Language http://chuck.cs.princeton.edu/

\bibitem{DEBEA} Debea, Debea Database Library http://debea.net/trac

\bibitem{DEBEALICENSE} Debea License http://debea.net/trac/wiki/DebeaLicense

\bibitem{DEBEAINSTALL} Debea, Installation instructions http://debea.net/trac/wiki/CompilePackage

\bibitem{MARSYAS} Marsyas: Music Analysis, Retrieval and Synthesis for Audio Signals http://marsyas.info/

\bibitem{PLANESHIFT} PlaneShift, A 3D Fantasy MMORPG http://www.planeshift.it/

\bibitem{HIERROVIRTUAL} CAVACAN, El Hierro Virtual http://serdis.dis.ulpgc.es/~cavacan/

\bibitem{YOFRANKIE} Yo Frankie!, Apricot Open Game Project http://www.yofrankie.org/

\bibitem{SIANI} SIANI, Instituto Universitario de Sistemas Inteligentes y Aplicaciones Numéricas en Ingeniería http://www.siani.es/

\bibitem{KenB01} K. Beck, J. Sutherland et al. {\em Manifesto for Agile Software Development} http://www.agilemanifesto.org 2001

\bibitem{RCol09} R. Colusso {\em Desarrollo ágil de software} http://knol.google.com/k/desarrollo-ágil-de-software 2009

\bibitem{IJacobson09} I. Jacobson {Introducing the Essential Unified Process} 2009

\bibitem{BOOST10} D. Abrahams et al. {Boost C++ Liraries} http://www.boost.org/ 2010

\bibitem{SFML10} L. Gomila {Simple and Fast Multimedia Library} http://www.sfml-dev.org 2010

\bibitem{PGSQL10} PostGreSQL {\em PostGreSQL, The world most advanced open source database} http://www.postgresql.org/about/ 2010

\bibitem{DEBEA10} Debea {\em Debea Database Access Library} http://www.debea.net/ 2010

\bibitem{THOMASB00} Thomas B. y Erick Granum {\em Application Areas, Surveys and Taxonomies } A Survey of Computer Vision-Based Human Motion Capture 2000

\bibitem{LIYUAN03} Liyuan Li, Weimin Huang et al. {\em Foreground Object Segmentation } Foreground Object Detection from Videos Containing Complex Background 2003

\bibitem{HOMOGF} Homography at the Wikipedia http://en.wikipedia.org/wiki/Homography\_(computer\_vision)

\bibitem{CALIBCV} OpenCV Documentation {\em Camera Calibration and 3D Reconstruction } http://opencv.willowgarage.com/documentation/camera\_calibration\_and\_3d\_reconstruction.html/ 2010

\bibitem{DAVIS97} Davis, J.W. and Bobick, A.F. {\em The Representation and Recognition of Action Using Temporal Templates} CVPR97, 1997

\bibitem{DBRADSKI00} Davis, J.W. and Bradski, G.R. {\em Motion Segmentation and Pose Recognition with Motion History Gradients} WACV00, 2000


\bibitem{JKAGG} J.K. Aggarwal, Q. Cai {\em Motion Analysis of Human Body Parts } Human Motion Analysis: A Review 

\bibitem{GAVRILA98} D.M. Gavrila {\em Action Recognition } The Visual Analysis of Human Movement: A Survey 1998

\bibitem{GREGMXIAOF} Greg Mori, Xiaofeng Ren, et al. {\em Finding Body Parts } Recovering Human Body Configurations: Combining Segmentation and Recognition

\bibitem{YASERY97} Yaser Yacoob {\em Parameterized modeling and recognition of activities } Parameterized Modeling and Recognition of Activities 1997


\bibitem{IVANAM} Ivana Mikic, Edward Hunter, Pamela Cosman. {\em  } Articulated Body Posture Estimation from Multi-Camera Voxel Data 

\bibitem{GREGMJM} Greg Mori, Jitendra Malik {\em  } Estimating Human Body configurations using Shape Context Matching 

\bibitem{JUERGEN} Juergen Gall, Carsten Stoll et al {\em Motion Capture Using Joint Skeleton Tracking and Surface Estimation}

\bibitem{CRAIGREYN} Craig Reynolds, {\em Boids (Flocks, Herds and Schools: A Distributed Behavioural Model)} http://www.red3d.com/cwr/boids/

\bibitem{JOHANNESK} Johannes Kilian, {\em Simple Image Analysis By Moments} March 15, 2001

\bibitem{LUKASK81} Bruce D. Lukas, Takeo KAnade,  {\em An Iterative Image Registration Technique with an Application to Stereo Vision} 1981

\bibitem{RTREE} R-Tree description at the Wikipedia http://en.wikipedia.org/wiki/R-tree

\bibitem{DEADRECKONING} Gamasutra - Dead Reckoning: Latency Hiding for
Networked Games http://www.gamasutra.com/view/feature/3230/dead\_reckoning\_latency\_hiding\_for\_.php


\bibitem{STEVES00} Steve Seitz, {\em From Images to Voxels} SIGGRAPH 2000 Course on 3D Photography

\bibitem{CHUCKD} Chuck Dyer, {\em Volumetric Scene Reconstruction from Multiple Views}

\bibitem{PAULB94} Paul Bourke, {\em Polygonising a scalar field: Also known as 3D Contouring, Marching Cubes, Surface Reconstruction} 1994

\bibitem{MCUBESW} Marching Cubes at the Wikipedia http://en.wikipedia.org/wiki/Marching\_cubes

\bibitem{MINECRAFT} Minecraft, Notch Development AB 2011

\bibitem{KINECT09} Artículo sobre la historia de Kinect en la Wikipedia http://es.wikipedia.org/wiki/Kinect

\bibitem{BELKIN99} Alan Belkin, {\em A Practical Guide to Musical Composition} 1995-1999

\bibitem{RLOPEZ02} Ramón López de Mantaras, Josep Lluis Arcos, {\em AI and Music: From Composition to Expressive Performance} 2002

\bibitem{MCCORMACK} Jon McCormack, {\em Grammar Based Music Composition}

\bibitem{CURTISR} Curtis Roads, {\em Research in Music and Artificial Intelligence}

\bibitem{LOYABBOT} Gareth Loy, Curtis Abbot {\em Programming Languages for Computer Music Synthesis, Performance, and Composition}

\bibitem{OCULUSR} Oculus Rift {\em Virtual Reality Headset for 3D Gaming}

\bibitem{VRASE} vrASE {\em The Smartphone Virtual Reality Case}

\bibitem{GPL2} GNU General Public License v2 http://www.gnu.org/licenses/gpl-2.0.html

\bibitem{LGPL} GNU Lesser General Public License https://www.gnu.org/licenses/lgpl.html

\bibitem{MITL} MIT License http://opensource.org/licenses/mit-license.php 

\bibitem{CCBY10} Creative Commons BY 1.0 http://creativecommons.org/licenses/by/1.0/

\bibitem{CMAKE} CMake - Cross Platform Make http://www.cmake.org/


\end{thebibliography}


% Termina el documento
\end{document}
